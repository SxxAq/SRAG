{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d85c32",
   "metadata": {},
   "source": [
    "### RAG Pipeline - from data ingestion to Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169e4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feb619aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4 PDF files to process\n",
      "\n",
      "processing: embeddings.pdf\n",
      "‚òëÔ∏è loaded 27 pages.\n",
      "\n",
      "processing: NNDT.pdf\n",
      "‚òëÔ∏è loaded 8 pages.\n",
      "\n",
      "processing: attention.pdf\n",
      "‚òëÔ∏è loaded 11 pages.\n",
      "\n",
      "processing: objectdetection.pdf\n",
      "‚òëÔ∏è loaded 21 pages.\n",
      "\n",
      " Total docs loaded : 67\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdfs inside the directory\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "  all_docs=[]\n",
    "  pdf_dir=Path(pdf_directory)\n",
    "  \n",
    "  #find all pdf files recursively\n",
    "  pdf_files=list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "  print(f\"found {len(pdf_files)} PDF files to process\")\n",
    "  \n",
    "  for pdf in pdf_files:\n",
    "    print(f\"\\nprocessing: {pdf.name}\")\n",
    "    try:\n",
    "      loader=PyMuPDFLoader(str(pdf))\n",
    "      documents=loader.load()\n",
    "      \n",
    "      # add source info to metadata\n",
    "      for doc in documents:\n",
    "        doc.metadata['source_file']=pdf.name\n",
    "        doc.metadata['file_type']='pdf'\n",
    "        \n",
    "      all_docs.extend(documents)\n",
    "      print(f\"‚òëÔ∏è loaded {len(documents)} pages.\")\n",
    "    except Exception as e:\n",
    "      print(f\"‚ùå Error : {e}\")\n",
    "  print(f\"\\n Total docs loaded : {len(all_docs)}\")\n",
    "  return all_docs\n",
    "\n",
    "all_pdfs=process_all_pdfs(\"../data\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51dad42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 0, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Speech and Language Processing.\\nDaniel Jurafsky & James H. Martin.\\nCopyright ¬© 2025.\\nAll\\nrights reserved.\\nDraft of August 24, 2025.\\nCHAPTER\\n5\\nEmbeddings\\nËçÉËÄÖÊâÄ‰ª•Âú®È±ºÔºåÂæóÈ±ºËÄåÂøòËçÉNets are for Ô¨Åsh;\\nOnce you get the Ô¨Åsh, you can forget the net.\\nË®ÄËÄÖÊâÄ‰ª•Âú®ÊÑèÔºåÂæóÊÑèËÄåÂøòË®ÄWords are for meaning;\\nOnce you get the meaning, you can forget the words\\nÂ∫ÑÂ≠ê(Zhuangzi), Chapter 26\\nThe asphalt that Los Angeles is famous for occurs mainly on its freeways. But\\nin the middle of the city is another patch of asphalt, the La Brea tar pits, and this\\nasphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleis-\\ntocene Epoch. One of these fossils is the Smilodon, or saber-toothed tiger, instantly\\nrecognizable by its long canines. Five million years ago or so, a completely different\\nsaber-tooth tiger called Thylacosmilus lived\\nin Argentina and other parts of South Amer-\\nica. Thylacosmilus was a marsupial whereas\\nSmilodon was a placental mammal, but Thy-\\nlacosmilus had the same long upper canines\\nand, like Smilodon, had a protective bone\\nÔ¨Çange on the lower jaw.\\nThe similarity of\\nthese two mammals is one of many examples\\nof parallel or convergent evolution, in which particular contexts or environments\\nlead to the evolution of very similar structures in different species (Gould, 1980).\\nThe role of context is also important in the similarity of a less biological kind\\nof organism: the word. Words that occur in similar contexts tend to have similar\\nmeanings. This link between similarity in how words are distributed and similarity\\nin what they mean is called the distributional hypothesis. The hypothesis was\\ndistributional\\nhypothesis\\nÔ¨Årst formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth\\n(1957), who noticed that words which are synonyms (like oculist and eye-doctor)\\ntended to occur in the same environment (e.g., near words like eye or examined)\\nwith the amount of meaning difference between two words ‚Äúcorresponding roughly\\nto the amount of difference in their environments‚Äù (Harris, 1954, p. 157).\\nIn this chapter we introduce embeddings, vector representations of the meaning\\nembeddings\\nof words that are learned directly from word distributions in texts. Embeddings lie\\nat the heart of large language models and other modern applications. The static em-\\nbeddings we introduce here underlie the more powerful dynamic or contextualized\\nembeddings like BERT that we will see in Chapter 10 and Chapter 8.\\nThe linguistic Ô¨Åeld that studies embeddings and their meanings is called vector\\nsemantics. Embeddings are also the Ô¨Årst example in this book of representation\\nvector\\nsemantics\\nlearning, automatically learning useful representations of the input text. Finding\\nrepresentation\\nlearning\\nsuch self-supervised ways to learn representations of language, instead of creat-\\ning representations by hand via feature engineering, is an important principle of\\nmodern NLP (Bengio et al., 2013).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 1, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='2\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\n5.1\\nLexical Semantics\\nLet‚Äôs begin by introducing some basic principles of word meaning. How should\\nwe represent the meaning of a word? In the n-gram models of Chapter 3, and in\\nclassical NLP applications, our only representation of a word is as a string of letters,\\nor an index in a vocabulary list. This representation is not that different from a\\ntradition in philosophy, perhaps you‚Äôve seen it in introductory logic classes, in which\\nthe meaning of words is represented by just spelling the word with small capital\\nletters; representing the meaning of ‚Äúdog‚Äù as DOG, and ‚Äúcat‚Äù as CAT, or by using an\\napostrophe (DOG‚Äô).\\nRepresenting the meaning of a word by capitalizing it is a pretty unsatisfactory\\nmodel. You might have seen a version of a joke due originally to semanticist Barbara\\nPartee (Carlson, 1977):\\nQ: What‚Äôs the meaning of life?\\nA: LIFE‚Äô\\nSurely we can do better than this! After all, we‚Äôll want a model of word meaning\\nto do all sorts of things for us. It should tell us that some words have similar mean-\\nings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some\\nhave positive connotations (happy) while others have negative connotations (sad). It\\nshould represent the fact that the meanings of buy, sell, and pay offer differing per-\\nspectives on the same underlying purchasing event. (If I buy something from you,\\nyou‚Äôve probably sold it to me, and I likely paid you.) More generally, a model of\\nword meaning should allow us to draw inferences to address meaning-related tasks\\nlike question-answering or dialogue.\\nIn this section we summarize some of these desiderata, drawing on results in the\\nlinguistic study of word meaning, which is called lexical semantics; we‚Äôll return to\\nlexical\\nsemantics\\nand expand on this list in Appendix G and Chapter 21.\\nLemmas and Senses\\nLet‚Äôs start by looking at how one word (we‚Äôll choose mouse)\\nmight be deÔ¨Åned in a dictionary (simpliÔ¨Åed from the online dictionary WordNet):\\nmouse (N)\\n1.\\nany of numerous small rodents...\\n2.\\na hand-operated device that controls a cursor...\\nHere the form mouse is the lemma, also called the citation form. The form\\nlemma\\ncitation form\\nmouse would also be the lemma for the word mice; dictionaries don‚Äôt have separate\\ndeÔ¨Ånitions for inÔ¨Çected forms like mice. Similarly sing is the lemma for sing, sang,\\nsung. In many languages the inÔ¨Ånitive form is used as the lemma for the verb, so\\nSpanish dormir ‚Äúto sleep‚Äù is the lemma for duermes ‚Äúyou sleep‚Äù. The speciÔ¨Åc forms\\nsung or carpets or sing or duermes are called wordforms.\\nwordform\\nAs the example above shows, each lemma can have multiple meanings; the\\nlemma mouse can refer to the rodent or the cursor control device. We call each\\nof these aspects of the meaning of mouse a word sense. The fact that lemmas can\\nbe polysemous (have multiple senses) can make interpretation difÔ¨Åcult (is some-\\none who searches for ‚Äúmouse info‚Äù looking for a pet or a widget?). Chapter 10\\nand Appendix G will discuss the problem of polysemy, and introduce word sense\\ndisambiguation, the task of determining which sense of a word is being used in a\\nparticular context.\\nSynonymy\\nOne important component of word meaning is the relationship be-\\ntween word senses. For example when one word has a sense whose meaning is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 2, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='5.1\\n‚Ä¢\\nLEXICAL SEMANTICS\\n3\\nidentical to a sense of another word, or nearly identical, we say the two senses of\\nthose two words are synonyms. Synonyms include such pairs as\\nsynonym\\ncouch/sofa vomit/throw up Ô¨Ålbert/hazelnut car/automobile\\nA more formal deÔ¨Ånition of synonymy (between words rather than senses) is that\\ntwo words are synonymous if they are substitutable for one another in any sentence\\nwithout changing the truth conditions of the sentence, the situations in which the\\nsentence would be true.\\nWhile substitutions between some pairs of words like car / automobile or wa-\\nter / H2O are truth preserving, the words are still not identical in meaning. Indeed,\\nprobably no two words are absolutely identical in meaning. One of the fundamental\\ntenets of semantics, called the principle of contrast (Girard 1718, Br¬¥eal 1897, Clark\\nprinciple of\\ncontrast\\n1987), states that a difference in linguistic form is always associated with some dif-\\nference in meaning. For example, the word H2O is used in scientiÔ¨Åc contexts and\\nwould be inappropriate in a hiking guide‚Äîwater would be more appropriate‚Äî and\\nthis genre difference is part of the meaning of the word. In practice, the word syn-\\nonym is therefore used to describe a relationship of approximate or rough synonymy.\\nWord Similarity\\nWhile words don‚Äôt have many synonyms, most words do have\\nlots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly\\nsimilar words. In moving from synonymy to similarity, it will be useful to shift from\\ntalking about relations between word senses (like synonymy) to relations between\\nwords (like similarity). Dealing with words avoids having to commit to a particular\\nrepresentation of word senses, which will turn out to simplify our task.\\nThe notion of word similarity is very useful in larger semantic tasks. Knowing\\nsimilarity\\nhow similar two words are can help in computing how similar the meaning of two\\nphrases or sentences are, a very important component of tasks like question answer-\\ning, paraphrasing, and summarization. One way of getting values for word similarity\\nis to ask humans to judge how similar one word is to another. A number of datasets\\nhave resulted from such experiments. For example the SimLex-999 dataset (Hill\\net al., 2015) gives values on a scale from 0 to 10, like the examples below, which\\nrange from near-synonyms (vanish, disappear) to pairs that scarcely seem to have\\nanything in common (hole, agreement):\\nvanish\\ndisappear\\n9.8\\nbelief\\nimpression 5.95\\nmuscle bone\\n3.65\\nmodest Ô¨Çexible\\n0.98\\nhole\\nagreement\\n0.3\\nWord Relatedness\\nThe meaning of two words can be related in ways other than\\nsimilarity. One such class of connections is called word relatedness (Budanitsky\\nrelatedness\\nand Hirst, 2006), also traditionally called word association in psychology.\\nassociation\\nConsider the meanings of the words coffee and cup. Coffee is not similar to cup;\\nthey share practically no features (coffee is a plant or a beverage, while a cup is a\\nmanufactured object with a particular shape). But coffee and cup are clearly related;\\nthey are associated by co-participating in an everyday event (the event of drinking\\ncoffee out of a cup). Similarly scalpel and surgeon are not similar but are related\\neventively (a surgeon tends to make use of a scalpel).\\nOne common kind of relatedness between words is if they belong to the same\\nsemantic Ô¨Åeld. A semantic Ô¨Åeld is a set of words which cover a particular semantic\\nsemantic Ô¨Åeld\\ndomain and bear structured relations with each other. For example, words might be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 3, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='4\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\nrelated by being in the semantic Ô¨Åeld of hospitals (surgeon, scalpel, nurse, anes-\\nthetic, hospital), restaurants (waiter, menu, plate, food, chef), or houses (door, roof,\\nkitchen, family, bed). Semantic Ô¨Åelds are also related to topic models, like Latent\\ntopic models\\nDirichlet Allocation, LDA, which apply unsupervised learning on large sets of texts\\nto induce sets of associated words from text. Semantic Ô¨Åelds and topic models are\\nvery useful tools for discovering topical structure in documents.\\nIn Appendix G we‚Äôll introduce more relations between senses like hypernymy\\nor IS-A, antonymy (opposites) and meronymy (part-whole relations).\\nConnotation\\nFinally, words have affective meanings or connotations. The word\\nconnotations\\nconnotation has different meanings in different Ô¨Åelds, but here we use it to mean the\\naspects of a word‚Äôs meaning that are related to a writer or reader‚Äôs emotions, senti-\\nment, opinions, or evaluations. For example some words have positive connotations\\n(wonderful) while others have negative connotations (dreary). Even words whose\\nmeanings are similar in other ways can vary in connotation; consider the difference\\nin connotations between fake, knockoff, forgery, on the one hand, and copy, replica,\\nreproduction on the other, or innocent (positive connotation) and naive (negative\\nconnotation). Some words describe positive evaluation (great, love) and others neg-\\native evaluation (terrible, hate). Positive or negative evaluation language is called\\nsentiment, as we saw in Appendix K, and word sentiment plays a role in impor-\\nsentiment\\ntant tasks like sentiment analysis, stance detection, and applications of NLP to the\\nlanguage of politics and consumer reviews.\\nEarly work on affective meaning (Osgood et al., 1957) found that words varied\\nalong three important dimensions of affective meaning:\\nvalence: the pleasantness of the stimulus\\narousal: the intensity of emotion provoked by the stimulus\\ndominance: the degree of control exerted by the stimulus\\nThus words like happy or satisÔ¨Åed are high on valence, while unhappy or an-\\nnoyed are low on valence. Excited is high on arousal, while calm is low on arousal.\\nControlling is high on dominance, while awed or inÔ¨Çuenced are low on dominance.\\nEach word is thus represented by three numbers, corresponding to its value on each\\nof the three dimensions:\\nValence Arousal Dominance\\ncourageous 8.0\\n5.5\\n7.4\\nmusic\\n7.7\\n5.6\\n6.5\\nheartbreak\\n2.5\\n5.7\\n3.6\\ncub\\n6.7\\n4.0\\n4.2\\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\\nmeaning of a word, the model was representing each word as a point in a three-\\ndimensional space, a vector whose three dimensions corresponded to the word‚Äôs\\nrating on the three scales. This revolutionary idea that word meaning could be rep-\\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\\nrepresented as the point [2.5,5.7,3.6]) was the Ô¨Årst expression of the vector seman-\\ntics models that we introduce next.\\n5.2\\nVector Semantics: The Intuition\\nVector semantics is the standard way to represent word meaning in NLP, helping\\nvector\\nsemantics'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 4, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='5.2\\n‚Ä¢\\nVECTOR SEMANTICS: THE INTUITION\\n5\\nus model many of the aspects of word meaning we saw in the previous section. The\\nroots of the model lie in the 1950s when two big ideas converged: Osgood‚Äôs 1957\\nidea mentioned above to use a point in three-dimensional space to represent the\\nconnotation of a word, and the proposal by linguists like Joos (1950), Harris (1954),\\nand Firth (1957) to deÔ¨Åne the meaning of a word by its distribution in language\\nuse, meaning its neighboring words or grammatical environments. Their idea was\\nthat two words that occur in very similar distributions (whose neighboring words are\\nsimilar) have similar meanings.\\nFor example, suppose you didn‚Äôt know the meaning of the word ongchoi (a re-\\ncent borrowing from Cantonese) but you see it in the following contexts:\\n(5.1) Ongchoi is delicious sauteed with garlic.\\n(5.2) Ongchoi is superb over rice.\\n(5.3) ...ongchoi leaves with salty sauces...\\nAnd suppose that you had seen many of these context words in other contexts:\\n(5.4) ...spinach sauteed with garlic over rice...\\n(5.5) ...chard stems and leaves are delicious...\\n(5.6) ...collard greens and other salty leafy greens\\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\\nsalty, as do words like spinach, chard, and collard greens might suggest that ongchoi\\nis a leafy green similar to these other leafy greens.1 We can implement the same\\nintuition computationally by just counting words in the context of ongchoi.\\nFigure 5.1\\nA two-dimensional (t-SNE) visualization of 200-dimensional word2vec em-\\nbeddings for some words close to the word sweet, showing that words with similar mean-\\nings are nearby in space. Visualization created using the TensorBoard Embedding Projector\\nhttps://projector.tensorflow.org/.\\nThe idea of vector semantics is to represent a word as a point in a multidimen-\\nsional semantic space that is derived (in different ways we‚Äôll see) from the distri-\\nbutions of word neighbors. Vectors for representing words are called embeddings.\\nembeddings\\nThe word ‚Äúembedding‚Äù derives historically from its mathematical sense as a map-\\nping from one space or structure to another, although the meaning has shifted; see\\nthe end of the chapter.\\nFig. 5.1 shows a visualization of embeddings learned by the word2vec algorithm,\\nshowing the location of selected words (neighbors of ‚Äúsweet‚Äù) projected down from\\n1\\nIt‚Äôs in fact Ipomoea aquatica, a relative of morning glory sometimes called water spinach in English.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 5, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\n200-dimensional space into a 2-dimensional space. Note that the nearest neighbors\\nof sweet are semantically related words like honey, candy, juice, chocolate. This idea\\nthat similar words are near each other in high-dimensional space is an important\\nthat offers enormous power to language models and other NLP applications. For\\nexample the sentiment classiÔ¨Åers of Chapter 4 depend on the same words appearing\\nin the training and test sets. But by representing words as embeddings, a classiÔ¨Åer\\ncan assign sentiment as long as it sees some words with similar meanings. And as\\nwe‚Äôll see, vector semantic models like the ones showed in Fig. 5.1 can be learned\\nautomatically from text without supervision.\\nIn this chapter we‚Äôll begin with a simple pedagogical model of embeddings in\\nwhich the meaning of a word is deÔ¨Åned by a vector with the counts of nearby words.\\nWe introduce this model as a helpful way to understand the concept of vectors and\\nwhat it means for a vector to be a representation of word meaning, but more sophis-\\nticated variants like the tf-idf model we will introduce in Chapter 11 are important\\nmethods you should understand. We will see that this method results in very long\\nvectors that are sparse, i.e. mostly zeros (since most words simply never occur in the\\ncontext of others). We‚Äôll then introduce the word2vec model family for constructing\\nshort, dense vectors that have even more useful semantic properties.\\nWe‚Äôll also introduce the cosine, the standard way to use embeddings to com-\\npute semantic similarity, between two words, two sentences, or two documents, an\\nimportant tool in practical applications.\\n5.3\\nSimple count-based embeddings\\n‚ÄúThe most important attributes of a vector in 3-space are {Location, Location, Location}‚Äù\\nRandall Munroe, the hover from https://xkcd.com/2358/\\nLet‚Äôs now introduce the Ô¨Årst way to compute word vector embeddings. This sim-\\nplest vector model of meaning is based on the co-occurrence matrix, a way of rep-\\nresenting how often words co-occur. We‚Äôll deÔ¨Åne a particular kind of co-occurrence\\nmatrix, the word-context matrix, in which each row in the matrix represents a word\\nword-context\\nmatrix\\nin the vocabulary and each column represents how often each other word in the vo-\\ncabulary appears nearby. This matrix is thus of dimensionality |V| √ó |V| and each\\ncell records the number of times the row (target) word and the column (context)\\nword co-occur nearby in some training corpus.\\nWhat do we mean by ‚Äònearby‚Äô? We could implement various methods, but let‚Äôs\\nstart with a very simple one: a context window around the word, let‚Äôs say of 4 words\\nto the left and 4 words to the right. If we do that, each cell will represents the\\nnumber of times (in some training corpus) the column word occurs in such a ¬±4\\nword window around the row word.\\nLet‚Äôs see how this works for 4 words: cherry, strawberry, digital, and informa-\\ntion. For each word we took a single instance from a corpus, and we show the ¬±4\\nword window from that instance:\\nis traditionally followed by cherry\\npie, a traditional dessert\\noften mixed, such as strawberry\\nrhubarb pie. Apple pie\\ncomputer peripherals and personal digital\\nassistants. These devices usually\\na computer. This includes information available on the internet\\nIf we then take every occurrence of each word in a large corpus and count the\\ncontext words around it, we get a word-context co-occurrence matrix. The full word-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 6, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='5.3\\n‚Ä¢\\nSIMPLE COUNT-BASED EMBEDDINGS\\n7\\ncontext co-occurrence matrix is very large, because for each word in the vocabulary\\n(since |V|) we have to count how often it occurs with every other word in the vo-\\ncabulary, hence dimensionality |V|√ó|V|. Let‚Äôs therefore instead sketch the process\\non a smaller scale. Imagine that we are going to look at only the 4 words, and only\\nconsider the following 3 context words: a, computer, and pie. Furthermore let‚Äôs\\nassume we only count occurrences in the mini-corpus above.\\nSo before looking at Fig. 5.2, compute by hand the counts for these 3 context\\nwords for the four words cherry, strawberry, digital, and information.\\na\\ncomputer\\npie\\ncherry\\n1\\n0\\n1\\nstrawberry\\n0\\n0\\n2\\ndigital\\n0\\n1\\n0\\ninformation\\n1\\n1\\n0\\nFigure 5.2\\nCo-occurrence vectors for four words with counts from the 4 windows above,\\nshowing just 3 of the potential context word dimensions. The vector for cherry is outlined in\\nred. Note that a real vector would have vastly more dimensions and thus be even sparser.\\nHopefully your count matches what is shown in Fig. 5.2, so that each cell repre-\\nsents the number of times a particular word (deÔ¨Åned by the row) occurs in a partic-\\nular context (deÔ¨Åned by the word column).\\nEach row, then, is a vector representing a word. To review some basic linear\\nalgebra, a vector is, at heart, just a list or array of numbers. So cherry is represented\\nvector\\nas the list [1,0,1] (the Ô¨Årst row vector in Fig. 5.2) and information is represented as\\nthe list [1,1,0] (the fourth row vector).\\nA vector space is a collection of vectors, and is characterized by its dimension.\\nvector space\\ndimension\\nVectors in a 3-dimensional vector space have an element for each dimension of the\\nspace. We will loosely refer to a vector in a 3-dimensional space as a 3-dimensional\\nvector, with one element along each dimension. In the example in Fig. 5.2, we‚Äôve\\nchosen to make the document vectors of dimension 3, just so they Ô¨Åt on the page; in\\nreal term-document matrices, the document vectors would have dimensionality |V|,\\nthe vocabulary size.\\nThe ordering of the numbers in a vector space indicates the different dimensions\\non which documents vary. The third dimension for all these vectors corresponds\\nto the number of times pie occurs in the context. The second dimension for all of\\nthem corresponds to the number of times the word computer occurs. Notice that\\nthe vectors for information and digital have the same value (1) for this ‚Äúcomputer‚Äù\\ndimension.\\nIn reality, we don‚Äôt compute word vectors on a single context window. Instead,\\nwe compute them over an entire corpus. Let‚Äôs see what some real counts look like.\\nLet‚Äôs look at some vectors computed in this way. Fig. 5.3 shows a subset of the\\nword-word co-occurrence matrix for these four words, where, again because it‚Äôs\\nimpossible to visualize all |V| possible context words on the page of this textbook,\\nwe show a subset of 6 of the dimensions, with counts computed from the Wikipedia\\ncorpus (Davies, 2015).\\nNote in Fig. 5.3 that the two words cherry and strawberry are more similar to\\neach other (both pie and sugar tend to occur in their window) than they are to other\\nwords like digital; conversely, digital and information are more similar to each other\\nthan, say, to strawberry.\\nWe can think of the vector for a document as a point in |V|-dimensional space;\\nthus the documents in Fig. 5.3 are points in 3-dimensional space. Fig. 5.4 shows a\\nspatial visualization.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 7, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='8\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\naardvark\\n...\\ncomputer\\ndata\\nresult\\npie\\nsugar\\n...\\ncherry\\n0\\n...\\n2\\n8\\n9\\n442\\n25\\n...\\nstrawberry\\n0\\n...\\n0\\n0\\n1\\n60\\n19\\n...\\ndigital\\n0\\n...\\n1670\\n1683\\n85\\n5\\n4\\n...\\ninformation\\n0\\n...\\n3325\\n3982\\n378\\n5\\n13\\n...\\nFigure 5.3\\nCo-occurrence vectors for four words in the Wikipedia corpus, showing six of\\nthe dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in\\nred. Note that a real vector would have vastly more dimensions and thus be much sparser, i.e.\\nwould have zero values in most dimensions.\\n1000 2000 3000 4000\\n1000\\n2000\\ndigital\\n [1683,1670]\\ncomputer\\n data\\ninformation\\n [3982,3325] \\n3000\\n4000\\nFigure 5.4\\nA spatial visualization of word vectors for digital and information, showing just\\ntwo of the dimensions, corresponding to the words data and computer.\\nNote that |V|, the dimensionality of the vector, is generally the size of the vo-\\ncabulary, often between 10,000 and 50,000 words (using the most frequent words\\nin the training corpus; keeping words after about the most frequent 50,000 or so is\\ngenerally not helpful). Since most of these numbers are zero these are sparse vector\\nrepresentations; there are efÔ¨Åcient algorithms for storing and computing with sparse\\nmatrices.\\nIt‚Äôs also possible to applying various kinds of weighting functions to the counts\\nin these cells. The most popular such weighting is tf-idf, which we‚Äôll introduce in\\nChapter 11, but there have historically been a wide variety of other weightings.\\nNow that we have some intuitions, let‚Äôs move on to examine the details of com-\\nputing word similarity.\\n5.4\\nCosine for measuring similarity\\nTo measure similarity between two target words v and w, we need a metric that\\ntakes two vectors (of the same dimensionality, either both with words as dimensions,\\nhence of length |V|, or both with documents as dimensions, of length |D|) and gives\\na measure of their similarity. By far the most common similarity metric is the cosine\\nof the angle between the vectors.\\nThe cosine‚Äîlike most measures for vector similarity used in NLP‚Äîis based on\\nthe dot product operator from linear algebra, also called the inner product:\\ndot product\\ninner product\\ndot product(v,w) = v ¬∑w =\\nN\\nX\\ni=1\\nviwi = v1w1 +v2w2 +...+vNwN\\n(5.7)\\nThe dot product acts as a similarity metric because it will tend to be high just when\\nthe two vectors have large values in the same dimensions. Alternatively, vectors that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 8, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='5.4\\n‚Ä¢\\nCOSINE FOR MEASURING SIMILARITY\\n9\\nhave zeros in different dimensions‚Äîorthogonal vectors‚Äîwill have a dot product of\\n0, representing their strong dissimilarity.\\nThis raw dot product, however, has a problem as a similarity metric: it favors\\nlong vectors. The vector length is deÔ¨Åned as\\nvector length\\n|v| =\\nv\\nu\\nu\\nt\\nN\\nX\\ni=1\\nv2\\ni\\n(5.8)\\nThe dot product is higher if a vector is longer, with higher values in each dimension.\\nMore frequent words have longer vectors, since they tend to co-occur with more\\nwords and have higher co-occurrence values with each of them. The raw dot product\\nthus will be higher for frequent words. But this is a problem; we‚Äôd like a similarity\\nmetric that tells us how similar two words are regardless of their frequency.\\nWe modify the dot product to normalize for the vector length by dividing the\\ndot product by the lengths of each of the two vectors. This normalized dot product\\nturns out to be the same as the cosine of the angle between the two vectors, following\\nfrom the deÔ¨Ånition of the dot product between two vectors a and b:\\na¬∑b = |a||b|cosŒ∏\\na¬∑b\\n|a||b| = cosŒ∏\\n(5.9)\\nThe cosine similarity metric between two vectors v and w thus can be computed as:\\ncosine\\ncosine(v,w) = v ¬∑w\\n|v||w| =\\nN\\nX\\ni=1\\nviwi\\nv\\nu\\nu\\nt\\nN\\nX\\ni=1\\nv2\\ni\\nv\\nu\\nu\\nt\\nN\\nX\\ni=1\\nw2\\ni\\n(5.10)\\nFor some applications we pre-normalize each vector, by dividing it by its length,\\ncreating a unit vector of length 1. Thus we could compute a unit vector from a by\\nunit vector\\ndividing it by |a|. For unit vectors, the dot product is the same as the cosine.\\nThe cosine value ranges from 1 for vectors pointing in the same direction, through\\n0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since\\nraw frequency values are non-negative, the cosine for these vectors ranges from 0‚Äì1.\\nLet‚Äôs see how the cosine computes which of the words cherry or digital is closer\\nin meaning to information, just using raw counts from the following shortened table:\\npie\\ndata computer\\ncherry\\n442\\n8\\n2\\ndigital\\n5\\n1683\\n1670\\ninformation\\n5\\n3982\\n3325\\ncos(cherry,information) =\\n442‚àó5+8‚àó3982+2‚àó3325\\n‚àö\\n4422 +82 +22‚àö\\n52 +39822 +33252 = .018\\ncos(digital,information) =\\n5‚àó5+1683‚àó3982+1670‚àó3325\\n‚àö\\n52 +16832 +16702‚àö\\n52 +39822 +33252 = .996\\nThe model decides that information is way closer to digital than it is to cherry, a\\nresult that seems sensible. Fig. 5.5 shows a visualization.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 9, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='10\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n500\\ndigital\\ncherry\\ninformation\\nDimension 1: ‚Äòpie‚Äô\\nDimension 2: ‚Äòcomputer‚Äô\\nFigure 5.5\\nA (rough) graphical demonstration of cosine similarity, showing vectors for\\nthree words (cherry, digital, and information) in the two dimensional space deÔ¨Åned by counts\\nof the words computer and pie nearby. The Ô¨Ågure doesn‚Äôt show the cosine, but it highlights the\\nangles; note that the angle between digital and information is smaller than the angle between\\ncherry and information. When two vectors are more similar, the cosine is larger but the angle\\nis smaller; the cosine has its maximum (1) when the angle between two vectors is smallest\\n(0‚ó¶); the cosine of all other angles is less than 1.\\ncan be used to compute word similarity, for tasks like Ô¨Ånding word paraphrases,\\ntracking changes in word meaning, or automatically discovering meanings of words\\nin different corpora. For example, we can Ô¨Ånd the 10 most similar words to any\\ntarget word w by computing the cosines between w and each of the |V| ‚àí1 other\\nwords, sorting, and looking at the top 10.\\n5.5\\nWord2vec\\nIn the previous sections we saw how to represent a word as a sparse, long vector with\\ndimensions corresponding to words in the vocabulary. We now introduce a more\\npowerful word representation: embeddings, short dense vectors. Unlike the vectors\\nwe‚Äôve seen so far, embeddings are short, with number of dimensions d ranging from\\n50-1000, rather than the much larger vocabulary size |V|.These d dimensions don‚Äôt\\nhave a clear interpretation. And the vectors are dense: instead of vector entries\\nbeing sparse, mostly-zero counts or functions of counts, the values will be real-\\nvalued numbers that can be negative.\\nIt turns out that dense vectors work better in every NLP task than sparse vectors.\\nWhile we don‚Äôt completely understand all the reasons for this, we have some intu-\\nitions. Representing words as 300-dimensional dense vectors requires our classiÔ¨Åers\\nto learn far fewer weights than if we represented words as 50,000-dimensional vec-\\ntors, and the smaller parameter space possibly helps with generalization and avoid-\\ning overÔ¨Åtting. Dense vectors may also do a better job of capturing synonymy.\\nFor example, in a sparse vector representation, dimensions for synonyms like car\\nand automobile dimension are distinct and unrelated; sparse vectors may thus fail\\nto capture the similarity between a word with car as a neighbor and a word with\\nautomobile as a neighbor.\\nIn this section we introduce one method for computing embeddings: skip-gram\\nskip-gram\\nwith negative sampling, sometimes called SGNS. The skip-gram algorithm is one\\nSGNS\\nof two algorithms in a software package called word2vec, and so sometimes the\\nword2vec\\nalgorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al.\\n2013b). The word2vec methods are fast, efÔ¨Åcient to train, and easily available on-\\nline with code and pretrained embeddings. Word2vec embeddings are static em-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 10, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='5.5\\n‚Ä¢\\nWORD2VEC\\n11\\nbeddings, meaning that the method learns one Ô¨Åxed embedding for each word in the\\nstatic\\nembeddings\\nvocabulary. In Chapter 10 we‚Äôll introduce methods for learning dynamic contextual\\nembeddings like the popular family of BERT representations, in which the vector\\nfor each word is different in different contexts.\\nThe intuition of word2vec is that instead of counting how often each word w oc-\\ncurs near, say, apricot, we‚Äôll instead train a classiÔ¨Åer on a binary prediction task: ‚ÄúIs\\nword w likely to show up near apricot?‚Äù We don‚Äôt actually care about this prediction\\ntask; instead we‚Äôll take the learned classiÔ¨Åer weights as the word embeddings.\\nThe revolutionary intuition here is that we can just use running text as implicitly\\nsupervised training data for such a classiÔ¨Åer; a word c that occurs near the target\\nword apricot acts as gold ‚Äòcorrect answer‚Äô to the question ‚ÄúIs word c likely to show\\nup near apricot?‚Äù This method, often called self-supervision, avoids the need for\\nself-supervision\\nany sort of hand-labeled supervision signal. This idea was Ô¨Årst proposed in the task\\nof neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011)\\nshowed that a neural language model (a neural network that learned to predict the\\nnext word from prior words) could just use the next word in running text as its\\nsupervision signal, and could be used to learn an embedding representation for each\\nword as part of doing this prediction task.\\nWe‚Äôll see how to do neural networks in the next chapter, but word2vec is a\\nmuch simpler model than the neural network language model, in two ways. First,\\nword2vec simpliÔ¨Åes the task (making it binary classiÔ¨Åcation instead of word pre-\\ndiction). Second, word2vec simpliÔ¨Åes the architecture (training a logistic regression\\nclassiÔ¨Åer instead of a multi-layer neural network with hidden layers that demand\\nmore sophisticated training algorithms). The intuition of skip-gram is:\\n1. Treat the target word and a neighboring context word as positive examples.\\n2. Randomly sample other words in the lexicon to get negative samples.\\n3. Use logistic regression to train a classiÔ¨Åer to distinguish those two cases.\\n4. Use the learned weights as the embeddings.\\n5.5.1\\nThe classiÔ¨Åer\\nLet‚Äôs start by thinking about the classiÔ¨Åcation task, and then turn to how to train.\\nImagine a sentence like the following, with a target word apricot, and assume we‚Äôre\\nusing a window of ¬±2 context words:\\n... lemon,\\na [tablespoon of apricot jam,\\na] pinch ...\\nc1\\nc2\\nw\\nc3\\nc4\\nOur goal is to train a classiÔ¨Åer such that, given a tuple (w,c) of a target word\\nw paired with a candidate context word c (for example (apricot, jam), or perhaps\\n(apricot, aardvark)) it will return the probability that c is a real context word (true\\nfor jam, false for aardvark):\\nP(+|w,c)\\n(5.11)\\nThe probability that word c is not a real context word for w is just 1 minus\\nEq. 5.11:\\nP(‚àí|w,c) = 1‚àíP(+|w,c)\\n(5.12)\\nHow does the classiÔ¨Åer compute the probability P? The intuition of the skip-\\ngram model is to base this probability on embedding similarity: a word is likely to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 11, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='12\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\noccur near the target if its embedding vector is similar to the target embedding. To\\ncompute similarity between these dense embeddings, we rely on the intuition that\\ntwo vectors are similar if they have a high dot product (after all, cosine is just a\\nnormalized dot product). In other words:\\nSimilarity(w,c) ‚âàc¬∑w\\n(5.13)\\nThe dot product c ¬∑ w is not a probability, it‚Äôs just a number ranging from ‚àí‚àûto ‚àû\\n(since the elements in word2vec embeddings can be negative, the dot product can be\\nnegative). To turn the dot product into a probability, we‚Äôll use the logistic or sigmoid\\nfunction œÉ(x), the fundamental core of logistic regression:\\nœÉ(x) =\\n1\\n1+exp(‚àíx)\\n(5.14)\\nWe model the probability that word c is a real context word for target word w as:\\nP(+|w,c) = œÉ(c¬∑w) =\\n1\\n1+exp(‚àíc¬∑w)\\n(5.15)\\nThe sigmoid function returns a number between 0 and 1, but to make it a probability\\nwe‚Äôll also need the total probability of the two possible events (c is a context word,\\nand c isn‚Äôt a context word) to sum to 1. We thus estimate the probability that word c\\nis not a real context word for w as:\\nP(‚àí|w,c) = 1‚àíP(+|w,c)\\n= œÉ(‚àíc¬∑w) =\\n1\\n1+exp(c¬∑w)\\n(5.16)\\nEquation 5.15 gives us the probability for one word, but there are many context\\nwords in the window. Skip-gram makes the simplifying assumption that all context\\nwords are independent, allowing us to just multiply their probabilities:\\nP(+|w,c1:L) =\\nL\\nY\\ni=1\\nœÉ(ci ¬∑w)\\n(5.17)\\nlogP(+|w,c1:L) =\\nL\\nX\\ni=1\\nlogœÉ(ci ¬∑w)\\n(5.18)\\nIn summary, skip-gram trains a probabilistic classiÔ¨Åer that, given a test target word\\nw and its context window of L words c1:L, assigns a probability based on how similar\\nthis context window is to the target word. The probability is based on applying the\\nlogistic (sigmoid) function to the dot product of the embeddings of the target word\\nwith each context word. To compute this probability, we just need embeddings for\\neach target word and context word in the vocabulary.\\nFig. 5.6 shows the intuition of the parameters we‚Äôll need. Skip-gram actually\\nstores two embeddings for each word, one for the word as a target, and one for the\\nword considered as context. Thus the parameters we need to learn are two matrices\\nW and C, each containing an embedding for every one of the |V| words in the\\nvocabulary V.2 Let‚Äôs now turn to learning these embeddings (which is the real goal\\nof training this classiÔ¨Åer in the Ô¨Årst place).\\n2\\nIn principle the target matrix and the context matrix could use different vocabularies, but we‚Äôll simplify\\nby assuming one shared vocabulary V.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 12, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='5.5\\n‚Ä¢\\nWORD2VEC\\n13\\n1\\nW\\nC\\naardvark\\nzebra\\nzebra\\naardvark\\napricot\\napricot\\n|V|\\n|V|+1\\n2|V|\\nùúΩ =\\ntarget words\\ncontext & noise\\nwords\\n‚Ä¶\\n‚Ä¶\\n1..d\\n‚Ä¶\\n‚Ä¶\\nFigure 5.6\\nThe embeddings learned by the skipgram model. The algorithm stores two em-\\nbeddings for each word, the target embedding (sometimes called the input embedding) and\\nthe context embedding (sometimes called the output embedding). The parameter Œ∏ that the al-\\ngorithm learns is thus a matrix of 2|V| vectors, each of dimension d, formed by concatenating\\ntwo matrices, the target embeddings W and the context+noise embeddings C.\\n5.5.2\\nLearning skip-gram embeddings\\nThe learning algorithm for skip-gram embeddings takes as input a corpus of text,\\nand a chosen vocabulary size N. It begins by assigning a random embedding vector\\nfor each of the N vocabulary words, and then proceeds to iteratively shift the em-\\nbedding of each word w to be more like the embeddings of words that occur nearby\\nin texts, and less like the embeddings of words that don‚Äôt occur nearby. Let‚Äôs start\\nby considering a single piece of training data:\\n... lemon,\\na [tablespoon of apricot jam,\\na] pinch ...\\nc1\\nc2\\nw\\nc3\\nc4\\nThis example has a target word w (apricot), and 4 context words in the L = ¬±2\\nwindow, resulting in 4 positive training instances (on the left below):\\npositive examples +\\nw\\ncpos\\napricot tablespoon\\napricot of\\napricot jam\\napricot a\\nnegative examples -\\nw\\ncneg\\nw\\ncneg\\napricot aardvark apricot seven\\napricot my\\napricot forever\\napricot where\\napricot dear\\napricot coaxial\\napricot if\\nFor training a binary classiÔ¨Åer we also need negative examples. In fact skip-\\ngram with negative sampling (SGNS) uses more negative examples than positive\\nexamples (with the ratio between them set by a parameter k). So for each of these\\n(w,cpos) training instances we‚Äôll create k negative samples, each consisting of the\\ntarget w plus a ‚Äònoise word‚Äô cneg. A noise word is a random word from the lexicon,\\nconstrained not to be the target word w. The table right above shows the setting\\nwhere k = 2, so we‚Äôll have 2 negative examples in the negative training set ‚àífor\\neach positive example w,cpos.\\nThe noise words are chosen according to their weighted unigram probability\\npŒ±(w), where Œ± is a weight. If we were sampling according to unweighted proba-\\nbility P(w), it would mean that with unigram probability P(‚Äúthe‚Äù) we would choose\\nthe word the as a noise word, with unigram probability P(‚Äúaardvark‚Äù) we would\\nchoose aardvark, and so on. But in practice it is common to set Œ± = 0.75, i.e. use'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 13, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='14\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\nthe weighting P3\\n4 (w):\\nPŒ±(w) =\\ncount(w)Œ±\\nP\\nw‚Ä≤ count(w‚Ä≤)Œ±\\n(5.19)\\nSetting Œ± = .75 gives better performance because it gives rare noise words slightly\\nhigher probability: for rare words, PŒ±(w) > P(w). To illustrate this intuition, it\\nmight help to work out the probabilities for an example with Œ± = .75 and two events,\\nP(a) = 0.99 and P(b) = 0.01:\\nPŒ±(a) =\\n.99.75\\n.99.75 +.01.75 = 0.97\\nPŒ±(b) =\\n.01.75\\n.99.75 +.01.75 = 0.03\\n(5.20)\\nThus using Œ± = .75 increases the probability of the rare event b from 0.01 to 0.03.\\nGiven the set of positive and negative training instances, and an initial set of\\nembeddings, the goal of the learning algorithm is to adjust those embeddings to\\n‚Ä¢ Maximize the similarity of the target word, context word pairs (w,cpos) drawn\\nfrom the positive examples\\n‚Ä¢ Minimize the similarity of the (w,cneg) pairs from the negative examples.\\nIf we consider one word/context pair (w,cpos) with its k noise words cneg1...cnegk,\\nwe can express these two goals as the following loss function L to be minimized\\n(hence the ‚àí); here the Ô¨Årst term expresses that we want the classiÔ¨Åer to assign the\\nreal context word cpos a high probability of being a neighbor, and the second term\\nexpresses that we want to assign each of the noise words cnegi a high probability of\\nbeing a non-neighbor, all multiplied because we assume independence:\\nL = ‚àílog\\n\"\\nP(+|w,cpos)\\nkY\\ni=1\\nP(‚àí|w,cnegi)\\n#\\n= ‚àí\\n\"\\nlogP(+|w,cpos)+\\nk\\nX\\ni=1\\nlogP(‚àí|w,cnegi)\\n#\\n= ‚àí\\n\"\\nlogP(+|w,cpos)+\\nk\\nX\\ni=1\\nlog\\n\\x001‚àíP(+|w,cnegi)\\n\\x01\\n#\\n= ‚àí\\n\"\\nlogœÉ(cpos ¬∑w)+\\nk\\nX\\ni=1\\nlogœÉ(‚àícnegi ¬∑w)\\n#\\n(5.21)\\nThat is, we want to maximize the dot product of the word with the actual context\\nwords, and minimize the dot products of the word with the k negative sampled non-\\nneighbor words.\\nWe minimize this loss function using stochastic gradient descent. Fig. 5.7 shows\\nthe intuition of one step of learning.\\nTo get the gradient, we need to take the derivative of Eq. 5.21 with respect to\\nthe different embeddings. It turns out the derivatives are the following (we leave the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 14, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='5.5\\n‚Ä¢\\nWORD2VEC\\n15\\nW\\nC\\nmove apricot and jam closer,\\nincreasing cpos z w\\naardvark\\nmove apricot and matrix apart\\ndecreasing cneg1 z w\\n‚Äú‚Ä¶apricot jam‚Ä¶‚Äù\\nw\\nzebra\\nzebra\\naardvark\\njam\\napricot\\ncpos\\nmatrix\\nTolstoy\\nmove apricot and Tolstoy apart\\ndecreasing cneg2 z w\\n!\\ncneg1\\ncneg2\\nk=2\\nFigure 5.7\\nIntuition of one step of gradient descent. The skip-gram model tries to shift em-\\nbeddings so the target embeddings (here for apricot) are closer to (have a higher dot product\\nwith) context embeddings for nearby words (here jam) and further from (lower dot product\\nwith) context embeddings for noise words that don‚Äôt occur nearby (here Tolstoy and matrix).\\nproof as an exercise at the end of the chapter):\\n‚àÇL\\n‚àÇcpos\\n= [œÉ(cpos ¬∑w)‚àí1]w\\n(5.22)\\n‚àÇL\\n‚àÇcneg\\n= [œÉ(cneg ¬∑w)]w\\n(5.23)\\n‚àÇL\\n‚àÇw = [œÉ(cpos ¬∑w)‚àí1]cpos +\\nk\\nX\\ni=1\\n[œÉ(cnegi ¬∑w)]cnegi\\n(5.24)\\nThe update equations going from time step t to t + 1 in stochastic gradient descent\\nare thus:\\nct+1\\npos\\n= ct\\npos ‚àíŒ∑[œÉ(ct\\npos ¬∑wt)‚àí1]wt\\n(5.25)\\nct+1\\nneg = ct\\nneg ‚àíŒ∑[œÉ(ct\\nneg ¬∑wt)]wt\\n(5.26)\\nwt+1 = wt ‚àíŒ∑\\n\"\\n[œÉ(ct\\npos ¬∑wt)‚àí1]ct\\npos +\\nk\\nX\\ni=1\\n[œÉ(ct\\nnegi ¬∑wt)]ct\\nnegi\\n#\\n(5.27)\\nJust as in logistic regression, then, the learning algorithm starts with randomly ini-\\ntialized W and C matrices, and then walks through the training corpus using gradient\\ndescent to move W and C so as to minimize the loss in Eq. 5.21 by making the up-\\ndates in (Eq. 5.25)-(Eq. 5.27).\\nRecall that the skip-gram model learns two separate embeddings for each word i:\\nthe target embedding wi and the context embedding ci, stored in two matrices, the\\ntarget\\nembedding\\ncontext\\nembedding\\ntarget matrix W and the context matrix C. It‚Äôs common to just add them together,\\nrepresenting word i with the vector wi +ci. Alternatively we can throw away the C\\nmatrix and just represent each word i by the vector wi.\\nAs with the simple count-based methods like tf-idf, the context window size L\\naffects the performance of skip-gram embeddings, and experiments often tune the\\nparameter L on a devset.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 15, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='16\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\n5.5.3\\nOther kinds of static embeddings\\nThere are many kinds of static embeddings. An extension of word2vec, fasttext\\nfasttext\\n(Bojanowski et al., 2017), addresses a problem with word2vec as we have presented\\nit so far: it has no good way to deal with unknown words‚Äîwords that appear in\\na test corpus but were unseen in the training corpus. A related problem is word\\nsparsity, such as in languages with rich morphology, where some of the many forms\\nfor each noun and verb may only occur rarely. Fasttext deals with these problems\\nby using subword models, representing each word as itself plus a bag of constituent\\nn-grams, with special boundary symbols < and > added to each word. For example,\\nwith n = 3 the word where would be represented by the sequence <where> plus the\\ncharacter n-grams:\\n<wh, whe, her, ere, re>\\nThen a skipgram embedding is learned for each constituent n-gram, and the word\\nwhere is represented by the sum of all of the embeddings of its constituent n-grams.\\nUnknown words can then be presented only by the sum of the constituent n-grams.\\nA fasttext open-source library, including pretrained embeddings for 157 languages,\\nis available at https://fasttext.cc.\\nAnother very widely used static embedding model is GloVe (Pennington et al.,\\n2014), short for Global Vectors, because the model is based on capturing global\\ncorpus statistics. GloVe is based on ratios of probabilities from the word-word co-\\noccurrence matrix.\\nIt turns out that dense embeddings like word2vec actually have an elegant math-\\nematical relationship with count-based embeddings, in which word2vec can be seen\\nas implicitly optimizing a function of a count matrix with a particular (PPMI) weight-\\ning (Levy and Goldberg, 2014c).\\n5.6\\nVisualizing Embeddings\\n‚ÄúI see well in many dimensions as long as the dimensions are around two.‚Äù\\nThe late economist Martin Shubik\\nVisualizing embeddings is an important goal in helping understand, apply, and\\nimprove these models of word meaning. But how can we visualize a (for example)\\n100-dimensional vector?\\nWRIST\\nANKLE\\nSHOULDER\\nARM\\nLEG\\nHAND\\nFOOT\\nHEAD\\nNOSE\\nFINGER\\nTOE\\nFACE\\nEAR\\nEYE\\nTOOTH\\nDOG\\nCAT\\nPUPPY\\nKITTEN\\nCOW\\nMOUSE\\nTURTLE\\nOYSTER\\nLION\\nBULL\\nCHICAGO\\nATLANTA\\nMONTREAL\\nNASHVILLE\\nTOKYO\\nCHINA\\nRUSSIA\\nAFRICA\\nASIA\\nEUROPE\\nAMERICA\\nBRAZIL\\nMOSCOW\\nFRANCE\\nHAWAII\\nThe simplest way to visualize the meaning of a word\\nw embedded in a space is to list the most similar words to\\nw by sorting the vectors for all words in the vocabulary by\\ntheir cosine with the vector for w. For example the 7 closest\\nwords to frog using a particular embeddings computed with\\nthe GloVe algorithm are: frogs, toad, litoria, leptodactyli-\\ndae, rana, lizard, and eleutherodactylus (Pennington et al.,\\n2014).\\nYet another visualization method is to use a clustering\\nalgorithm to show a hierarchical representation of which\\nwords are similar to others in the embedding space. The\\nuncaptioned Ô¨Ågure on the left uses hierarchical clustering\\nof some embedding vectors for nouns as a visualization\\nmethod (Rohde et al., 2006).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 16, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='5.7\\n‚Ä¢\\nSEMANTIC PROPERTIES OF EMBEDDINGS\\n17\\nProbably the most common visualization method, how-\\never, is to project the 100 dimensions of a word down into 2\\ndimensions. Fig. 5.1 showed one such visualization, as does\\nFig. 5.9, using a projection method called t-SNE (van der\\nMaaten and Hinton, 2008).\\n5.7\\nSemantic properties of embeddings\\nIn this section we brieÔ¨Çy summarize some of the semantic properties of embeddings\\nthat have been studied.\\nDifferent types of similarity or association:\\nOne parameter of vector semantic\\nmodels that is relevant to both sparse PPMI vectors and dense word2vec vectors is\\nthe size of the context window used to collect counts. This is generally between 1\\nand 10 words on each side of the target word (for a total context of 2-20 words).\\nThe choice depends on the goals of the representation. Shorter context windows\\ntend to lead to representations that are a bit more syntactic, since the information is\\ncoming from immediately nearby words. When the vectors are computed from short\\ncontext windows, the most similar words to a target word w tend to be semantically\\nsimilar words with the same parts of speech. When vectors are computed from long\\ncontext windows, the highest cosine words to a target word w tend to be words that\\nare topically related but not similar.\\nFor example Levy and Goldberg (2014a) showed that using skip-gram with a\\nwindow of ¬±2, the most similar words to the word Hogwarts (from the Harry Potter\\nseries) were names of other Ô¨Åctional schools: Sunnydale (from Buffy the Vampire\\nSlayer) or Evernight (from a vampire series). With a window of ¬±5, the most similar\\nwords to Hogwarts were other words topically related to the Harry Potter series:\\nDumbledore, Malfoy, and half-blood.\\nIt‚Äôs also often useful to distinguish two kinds of similarity or association between\\nwords (Sch¬®utze and Pedersen, 1993). Two words have Ô¨Årst-order co-occurrence\\nÔ¨Årst-order\\nco-occurrence\\n(sometimes called syntagmatic association) if they are typically nearby each other.\\nThus wrote is a Ô¨Årst-order associate of book or poem. Two words have second-order\\nco-occurrence (sometimes called paradigmatic association) if they have similar\\nsecond-order\\nco-occurrence\\nneighbors. Thus wrote is a second-order associate of words like said or remarked.\\nAnalogy/Relational Similarity:\\nAnother semantic property of embeddings is their\\nability to capture relational meanings. In an important early vector space model of\\ncognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model\\nparallelogram\\nmodel\\nfor solving simple analogy problems of the form a is to b as a* is to what?. In such\\nproblems, a system is given a problem like apple:tree::grape:?, i.e., apple is to tree\\nas grape is to\\n, and must Ô¨Åll in the word vine. In the parallelogram model, il-\\nlustrated in Fig. 5.8, the vector from the word apple to the word tree (= #   ¬ª\\ntree‚àí#       ¬ª\\napple)\\nis added to the vector for grape (#        ¬ª\\ngrape); the nearest word to that point is returned.\\nIn early work with sparse embeddings, scholars showed that sparse vector mod-\\nels of meaning could solve such analogy problems (Turney and Littman, 2005),\\nbut the parallelogram method received more modern attention because of its suc-\\ncess with word2vec or GloVe vectors (Mikolov et al. 2013c, Levy and Goldberg\\n2014b, Pennington et al. 2014). For example, the result of the expression #     ¬ª\\nking ‚àí\\n#     ¬ª\\nman + #            ¬ª\\nwoman is a vector close to #         ¬ª\\nqueen. Similarly, #      ¬ª\\nParis ‚àí#           ¬ª\\nFrance + #     ¬ª\\nItaly results\\nin a vector that is close to #         ¬ª\\nRome. The embedding model thus seems to be extract-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 17, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='18\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\ntree\\napple\\ngrape\\nvine\\nFigure 5.8\\nThe parallelogram model for analogy problems (Rumelhart and Abrahamson,\\n1973): the location of #     ¬ª\\nvine can be found by subtracting #       ¬ª\\napple from #   ¬ª\\ntree and adding #       ¬ª\\ngrape.\\ning representations of relations like MALE-FEMALE, or CAPITAL-CITY-OF, or even\\nCOMPARATIVE/SUPERLATIVE, as shown in Fig. 5.9 from GloVe.\\n(a)\\n(b)\\nFigure 5.9\\nRelational properties of the GloVe vector space, shown by projecting vectors onto two dimensions.\\n(a) #     ¬ª\\nking‚àí#     ¬ª\\nman+ #            ¬ª\\nwoman is close to #        ¬ª\\nqueen. (b) offsets seem to capture comparative and superlative morphology\\n(Pennington et al., 2014).\\nFor a a : b :: a‚àó: b‚àóproblem, meaning the algorithm is given vectors a, b, and\\na‚àóand must Ô¨Ånd b‚àó, the parallelogram method is thus:\\nÀÜb‚àó= argmin\\nx\\ndistance(x,b‚àía+a‚àó)\\n(5.28)\\nwith some distance function, such as Euclidean distance.\\nThere are some caveats. For example, the closest value returned by the paral-\\nlelogram algorithm in word2vec or GloVe embedding spaces is usually not in fact\\nb* but one of the 3 input words or their morphological variants (i.e., cherry:red ::\\npotato:x returns potato or potatoes instead of brown), so these must be explicitly\\nexcluded. Furthermore while embedding spaces perform well if the task involves\\nfrequent words, small distances, and certain relations (like relating countries with\\ntheir capitals or verbs/nouns with their inÔ¨Çected forms), the parallelogram method\\nwith embeddings doesn‚Äôt work as well for other relations (Linzen 2016, Gladkova\\net al. 2016, Schluter 2018, Ethayarajh et al. 2019a), and indeed Peterson et al. (2020)\\nargue that the parallelogram method is in general too simple to model the human\\ncognitive process of forming analogies of this kind.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 18, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='5.8\\n‚Ä¢\\nBIAS AND EMBEDDINGS\\n19\\n5.7.1\\nEmbeddings and Historical Semantics\\nEmbeddings can also be a useful tool for studying how meaning changes over time,\\nby computing multiple embedding spaces, each from texts written in a particular\\ntime period. For example Fig. 5.10 shows a visualization of changes in meaning in\\nEnglish words over the last two centuries, computed by building separate embedding\\nspaces for each decade from historical corpora like Google n-grams (Lin et al., 2012)\\nand the Corpus of Historical American English (Davies, 2012).\\nFigure 5.10\\nA t-SNE visualization of the semantic change of 3 words in English using\\nword2vec vectors. The modern sense of each word, and the grey context words, are com-\\nputed from the most recent (modern) time-point embedding space. Earlier points are com-\\nputed from earlier historical embedding spaces. The visualizations show the changes in the\\nword gay from meanings related to ‚Äúcheerful‚Äù or ‚Äúfrolicsome‚Äù to referring to homosexuality,\\nthe development of the modern ‚Äútransmission‚Äù sense of broadcast from its original sense of\\nsowing seeds, and the pejoration of the word awful as it shifted from meaning ‚Äúfull of awe‚Äù\\nto meaning ‚Äúterrible or appalling‚Äù (Hamilton et al., 2016).\\n5.8\\nBias and Embeddings\\nIn addition to their ability to learn word meaning from text, embeddings, alas,\\nalso reproduce the implicit biases and stereotypes that were latent in the text. As\\nthe prior section just showed, embeddings can roughly model relational similar-\\nity: ‚Äòqueen‚Äô as the closest word to ‚Äòking‚Äô - ‚Äòman‚Äô + ‚Äòwoman‚Äô implies the analogy\\nman:woman::king:queen. But these same embedding analogies also exhibit gender\\nstereotypes. For example Bolukbasi et al. (2016) Ô¨Ånd that the closest occupation\\nto ‚Äòcomputer programmer‚Äô - ‚Äòman‚Äô + ‚Äòwoman‚Äô in word2vec embeddings trained on\\nnews text is ‚Äòhomemaker‚Äô, and that the embeddings similarly suggest the analogy\\n‚Äòfather‚Äô is to ‚Äòdoctor‚Äô as ‚Äòmother‚Äô is to ‚Äònurse‚Äô. This could result in what Crawford\\n(2017) and Blodgett et al. (2020) call an allocational harm, when a system allo-\\nallocational\\nharm\\ncates resources (jobs or credit) unfairly to different groups. For example algorithms\\nthat use embeddings as part of a search for hiring potential programmers or doctors\\nmight thus incorrectly downweight documents with women‚Äôs names.\\nIt turns out that embeddings don‚Äôt just reÔ¨Çect the statistics of their input, but also\\namplify bias; gendered terms become more gendered in embedding space than they\\nbias\\nampliÔ¨Åcation\\nwere in the input text statistics (Zhao et al. 2017, Ethayarajh et al. 2019b, Jia et al.\\n2020), and biases are more exaggerated than in actual labor employment statistics\\n(Garg et al., 2018).\\nEmbeddings also encode the implicit associations that are a property of human\\nreasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 19, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='20\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\nple‚Äôs associations between concepts (like ‚ÄòÔ¨Çowers‚Äô or ‚Äòinsects‚Äô) and attributes (like\\n‚Äòpleasantness‚Äô and ‚Äòunpleasantness‚Äô) by measuring differences in the latency with\\nwhich they label words in the various categories.3 Using such methods, people\\nin the United States have been shown to associate African-American names with\\nunpleasant words (more than European-American names), male names more with\\nmathematics and female names with the arts, and old people‚Äôs names with unpleas-\\nant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan\\net al. (2017) replicated all these Ô¨Åndings of implicit associations using GloVe vectors\\nand cosine similarity instead of human latencies. For example African-American\\nnames like ‚ÄòLeroy‚Äô and ‚ÄòShaniqua‚Äô had a higher GloVe cosine with unpleasant words\\nwhile European-American names (‚ÄòBrad‚Äô, ‚ÄòGreg‚Äô, ‚ÄòCourtney‚Äô) had a higher cosine\\nwith pleasant words. These problems with embeddings are an example of a repre-\\nsentational harm (Crawford 2017, Blodgett et al. 2020), which is a harm caused by\\nrepresentational\\nharm\\na system demeaning or even ignoring some social groups. Any embedding-aware al-\\ngorithm that made use of word sentiment could thus exacerbate bias against African\\nAmericans.\\nRecent research focuses on ways to try to remove these kinds of biases, for\\nexample by developing a transformation of the embedding space that removes gen-\\nder stereotypes but preserves deÔ¨Ånitional gender (Bolukbasi et al. 2016, Zhao et al.\\n2017) or changing the training procedure (Zhao et al., 2018). However, although\\nthese sorts of debiasing may reduce bias in embeddings, they do not eliminate it\\ndebiasing\\n(Gonen and Goldberg, 2019), and this remains an open problem.\\nHistorical embeddings are also being used to measure biases in the past. Garg\\net al. (2018) used embeddings from historical texts to measure the association be-\\ntween embeddings for occupations and embeddings for names of various ethnici-\\nties or genders (for example the relative cosine similarity of women‚Äôs names versus\\nmen‚Äôs to occupation words like ‚Äòlibrarian‚Äô or ‚Äòcarpenter‚Äô) across the 20th century.\\nThey found that the cosines correlate with the empirical historical percentages of\\nwomen or ethnic groups in those occupations. Historical embeddings also repli-\\ncated old surveys of ethnic stereotypes; the tendency of experimental participants in\\n1933 to associate adjectives like ‚Äòindustrious‚Äô or ‚Äòsuperstitious‚Äô with, e.g., Chinese\\nethnicity, correlates with the cosine between Chinese last names and those adjectives\\nusing embeddings trained on 1930s text. They also were able to document historical\\ngender biases, such as the fact that embeddings for adjectives related to competence\\n(‚Äòsmart‚Äô, ‚Äòwise‚Äô, ‚Äòthoughtful‚Äô, ‚Äòresourceful‚Äô) had a higher cosine with male than fe-\\nmale words, and showed that this bias has been slowly decreasing since 1960. We\\nreturn in later chapters to this question about the role of bias in natural language\\nprocessing.\\n5.9\\nEvaluating Vector Models\\nThe most important evaluation metric for vector models is extrinsic evaluation on\\ntasks, i.e., using vectors in an NLP task and seeing whether this improves perfor-\\nmance over some other model.\\n3\\nRoughly speaking, if humans associate ‚ÄòÔ¨Çowers‚Äô with ‚Äòpleasantness‚Äô and ‚Äòinsects‚Äô with ‚Äòunpleasant-\\nness‚Äô, when they are instructed to push a green button for ‚ÄòÔ¨Çowers‚Äô (daisy, iris, lilac) and ‚Äòpleasant words‚Äô\\n(love, laughter, pleasure) and a red button for ‚Äòinsects‚Äô (Ô¨Çea, spider, mosquito) and ‚Äòunpleasant words‚Äô\\n(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for\\n‚ÄòÔ¨Çowers‚Äô and ‚Äòunpleasant words‚Äô and a green button for ‚Äòinsects‚Äô and ‚Äòpleasant words‚Äô.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 20, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='5.10\\n‚Ä¢\\nSUMMARY\\n21\\nNonetheless it is useful to have intrinsic evaluations. The most common metric\\nis to test their performance on similarity, computing the correlation between an\\nalgorithm‚Äôs word similarity scores and word similarity ratings assigned by humans.\\nWordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0\\nto 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77.\\nSimLex-999 (Hill et al., 2015) is a more complex dataset that quantiÔ¨Åes similarity\\n(cup, mug) rather than relatedness (cup, coffee), and includes concrete and abstract\\nadjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each\\nconsisting of a target word with 4 additional word choices; the task is to choose\\nwhich is the correct synonym, as in the example: Levied is closest in meaning to:\\nimposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these\\ndatasets present words without context.\\nSlightly more realistic are intrinsic similarity tasks that include context. The\\nStanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the\\nWord-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer\\nevaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in\\ntheir sentential context, while WiC gives target words in two sentential contexts that\\nare either in the same or different senses; see Appendix G. The semantic textual\\nsimilarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of\\nsentence-level similarity algorithms, consisting of a set of pairs of sentences, each\\npair with human-labeled similarity scores.\\nAnother task used for evaluation is the analogy task, discussed on page 17, where\\nthe system has to solve problems of the form a is to b as a* is to b*, given a, b, and a*\\nand having to Ô¨Ånd b* (Turney and Littman, 2005). A number of sets of tuples have\\nbeen created for this task (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova\\net al. 2016), covering morphology (city:cities::child:children), lexicographic rela-\\ntions (leg:table::spout:teapot) and encyclopedia relations (Beijing:China::Dublin:Ireland),\\nsome drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jur-\\ngens et al., 2012).\\nAll embedding algorithms suffer from inherent variability. For example because\\nof randomness in the initialization and the random negative sampling, algorithms\\nlike word2vec may produce different results even from the same dataset, and in-\\ndividual documents in a collection may strongly impact the resulting embeddings\\n(Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When em-\\nbeddings are used to study word associations in particular corpora, therefore, it is\\nbest practice to train multiple embeddings with bootstrap sampling over documents\\nand average the results (Antoniak and Mimno, 2018).\\n5.10\\nSummary\\n‚Ä¢ In vector semantics, a word is modeled as a vector‚Äîa point in high-dimensional\\nspace, also called an embedding. In this chapter we focus on static embed-\\ndings, where each word is mapped to a Ô¨Åxed embedding.\\n‚Ä¢ Vector semantic models fall into two classes: sparse and dense. In sparse\\nmodels each dimension corresponds to a word in the vocabulary V and cells\\nare functions of co-occurrence counts. The word-context or term-term ma-\\ntrix has a row for each (target) word in the vocabulary and a column for each\\ncontext term in the vocabulary.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 21, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='22\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\n‚Ä¢ Dense vector models typically have dimensionality 50‚Äì1000. Word2vec al-\\ngorithms like skip-gram are a popular way to compute dense embeddings.\\nSkip-gram trains a logistic regression classiÔ¨Åer to compute the probability that\\ntwo words are ‚Äòlikely to occur nearby in text‚Äô. This probability is computed\\nfrom the dot product between the embeddings for the two words.\\n‚Ä¢ Skip-gram uses stochastic gradient descent to train the classiÔ¨Åer, by learning\\nembeddings that have a high dot product with embeddings of words that occur\\nnearby and a low dot product with noise words.\\n‚Ä¢ Other important embedding algorithms include GloVe, a method based on\\nratios of word co-occurrence probabilities.\\n‚Ä¢ Whether using sparse or dense vectors, word and document similarities are\\ncomputed by some function of the dot product between vectors. The cosine\\nof two vectors‚Äîa normalized dot product‚Äîis the most popular such metric.\\nHistorical Notes\\nThe idea of vector semantics arose out of research in the 1950s in three distinct\\nÔ¨Åelds: linguistics, psychology, and computer science, each of which contributed a\\nfundamental aspect of the model.\\nThe idea that meaning is related to the distribution of words in context was\\nwidespread in linguistic theory of the 1950s, among distributionalists like Zellig\\nHarris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos\\n(1950) put it,\\nthe linguist‚Äôs ‚Äúmeaning‚Äù of a morpheme. . . is by deÔ¨Ånition the set of conditional\\nprobabilities of its occurrence in context with all other morphemes.\\nThe idea that the meaning of a word might be modeled as a point in a multi-\\ndimensional semantic space came from psychologists like Charles E. Osgood, who\\nhad been studying how people responded to the meaning of words by assigning val-\\nues along scales like happy/sad or hard/soft. Osgood et al. (1957) proposed that the\\nmeaning of a word in general could be modeled as a point in a multidimensional\\nEuclidean space, and that the similarity of meaning between two words could be\\nmodeled as the distance between these points in the space.\\nA Ô¨Ånal intellectual source in the 1950s and early 1960s was the Ô¨Åeld then called\\nmechanical indexing, now known as information retrieval. In what became known\\nmechanical\\nindexing\\nas the vector space model for information retrieval (Salton 1971, Sparck Jones\\n1986), researchers demonstrated new ways to deÔ¨Åne the meaning of words in terms\\nof vectors (Switzer, 1965), and reÔ¨Åned methods for word similarity based on mea-\\nsures of statistical association between words like mutual information (Giuliano,\\n1965) and idf (Sparck Jones, 1972), and showed that the meaning of documents\\ncould be represented in the same vector spaces used for words. Around the same\\ntime, (Cordier, 1965) showed that factor analysis of word association probabilities\\ncould be used to form dense vector representations of words.\\nSome of the philosophical underpinning of the distributional way of thinking\\ncame from the late writings of the philosopher Wittgenstein, who was skeptical of\\nthe possibility of building a completely formal theory of meaning deÔ¨Ånitions for\\neach word. Wittgenstein suggested instead that ‚Äúthe meaning of a word is its use in\\nthe language‚Äù (Wittgenstein, 1953, PI 43). That is, instead of using some logical lan-\\nguage to deÔ¨Åne each word, or drawing on denotations or truth values, Wittgenstein‚Äôs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 22, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='HISTORICAL NOTES\\n23\\nidea is that we should deÔ¨Åne a word by how it is used by people in speaking and un-\\nderstanding in their day-to-day interactions, thus preÔ¨Åguring the movement toward\\nembodied and experiential models in linguistics and NLP (Glenberg and Robertson\\n2000, Lake and Murphy 2021, Bisk et al. 2020, Bender and Koller 2020).\\nMore distantly related is the idea of deÔ¨Åning words by a vector of discrete fea-\\ntures, which has roots at least as far back as Descartes and Leibniz (Wierzbicka 1992,\\nWierzbicka 1996). By the middle of the 20th century, beginning with the work of\\nHjelmslev (Hjelmslev, 1969) (originally 1943) and Ô¨Çeshed out in early models of\\ngenerative grammar (Katz and Fodor, 1963), the idea arose of representing mean-\\ning with semantic features, symbols that represent some sort of primitive meaning.\\nsemantic\\nfeature\\nFor example words like hen, rooster, or chick, have something in common (they all\\ndescribe chickens) and something different (their age and sex), representable as:\\nhen\\n+female, +chicken, +adult\\nrooster -female, +chicken, +adult\\nchick\\n+chicken, -adult\\nThe dimensions used by vector models of meaning to deÔ¨Åne words, however, are\\nonly abstractly related to this idea of a small Ô¨Åxed number of hand-built dimensions.\\nNonetheless, there has been some attempt to show that certain dimensions of em-\\nbedding models do contribute some speciÔ¨Åc compositional aspect of meaning like\\nthese early semantic features.\\nThe use of dense vectors to model word meaning, and indeed the term embed-\\nding, grew out of the latent semantic indexing (LSI) model (Deerwester et al.,\\n1988) recast as LSA (latent semantic analysis) (Deerwester et al., 1990). In LSA\\nsingular value decomposition‚ÄîSVD‚Äî is applied to a term-document matrix (each\\nSVD\\ncell weighted by log frequency and normalized by entropy), and then the Ô¨Årst 300\\ndimensions are used as the LSA embedding. Singular Value Decomposition (SVD)\\nis a method for Ô¨Ånding the most important dimensions of a data set, those dimen-\\nsions along which the data varies the most. LSA was then quickly widely applied:\\nas a cognitive model (Landauer and Dumais, 1997), and for tasks like spell checking\\n(Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Ju-\\nrafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000,\\nSchone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky,\\n2001a), and essay grading (Rehder et al., 1998). Related models were simultane-\\nously developed and applied to word sense disambiguation by Sch¬®utze (1992). LSA\\nalso led to the earliest use of embeddings to represent words in a probabilistic clas-\\nsiÔ¨Åer, in the logistic regression document router of Sch¬®utze et al. (1995). The idea of\\nSVD on the term-term matrix (rather than the term-document matrix) as a model of\\nmeaning for NLP was proposed soon after LSA by Sch¬®utze (1992). Sch¬®utze applied\\nthe low-rank (97-dimensional) embeddings produced by SVD to the task of word\\nsense disambiguation, analyzed the resulting semantic space, and also suggested\\npossible techniques like dropping high-order dimensions. See Sch¬®utze (1997).\\nA number of alternative matrix models followed on from the early SVD work,\\nincluding Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent\\nDirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factoriza-\\ntion (NMF) (Lee and Seung, 1999).\\nThe LSA community seems to have Ô¨Årst used the word ‚Äúembedding‚Äù in Landauer\\net al. (1997), in a variant of its mathematical meaning as a mapping from one space\\nor mathematical structure to another. In LSA, the word embedding seems to have\\ndescribed the mapping from the space of sparse count vectors to the latent space of\\nSVD dense vectors. Although the word thus originally meant the mapping from one'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 23, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='24\\nCHAPTER 5\\n‚Ä¢\\nEMBEDDINGS\\nspace to another, it has metonymically shifted to mean the resulting dense vector in\\nthe latent space, and it is in this sense that we currently use the word.\\nBy the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that\\nneural language models could also be used to develop embeddings as part of the task\\nof word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and\\nCollobert et al. (2011) then demonstrated that embeddings could be used to represent\\nword meanings for a number of NLP tasks. Turian et al. (2010) compared the value\\nof different kinds of embeddings for different NLP tasks. Mikolov et al. (2011)\\nshowed that recurrent neural nets could be used as language models. The idea of\\nsimplifying the hidden layer of these neural net language models to create the skip-\\ngram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The\\nnegative sampling training algorithm was proposed in Mikolov et al. (2013b). There\\nare numerous surveys of static embeddings and their parameterizations (Bullinaria\\nand Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark\\n2014, Levy et al. 2015).\\nSee Manning et al. (2008) and Chapter 11 for a deeper understanding of the role\\nof vectors in information retrieval, including how to compare queries with docu-\\nments, more details on tf-idf, and issues of scaling to very large datasets. See Kim\\n(2019) for a clear and comprehensive tutorial on word2vec. Cruse (2004) is a useful\\nintroductory linguistic text on lexical semantics.\\nExercises'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 24, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Exercises\\n25\\nAgirre, E., C. Banea, C. Cardie, D. Cer, M. Diab,\\nA. Gonzalez-Agirre, W. Guo, I. Lopez-Gazpio, M. Mar-\\nitxalar, R. Mihalcea, G. Rigau, L. Uria, and J. Wiebe.\\n2015. SemEval-2015 task 2: Semantic textual similarity,\\nEnglish, Spanish and pilot on interpretability. SemEval-\\n15.\\nAgirre, E., M. Diab, D. Cer, and A. Gonzalez-Agirre. 2012.\\nSemEval-2012 task 6: A pilot on semantic textual simi-\\nlarity. SemEval-12.\\nAntoniak, M. and D. Mimno. 2018. Evaluating the stability\\nof embedding-based word similarities. TACL, 6:107‚Äì119.\\nBellegarda, J. R. 1997. A latent semantic analysis framework\\nfor large-span language modeling. EUROSPEECH.\\nBellegarda, J. R. 2000. Exploiting latent semantic informa-\\ntion in statistical language modeling. Proceedings of the\\nIEEE, 89(8):1279‚Äì1296.\\nBender, E. M. and A. Koller. 2020. Climbing towards NLU:\\nOn meaning, form, and understanding in the age of data.\\nACL.\\nBengio, Y., A. Courville, and P. Vincent. 2013. Represen-\\ntation learning: A review and new perspectives. IEEE\\nTransactions on Pattern Analysis and Machine Intelli-\\ngence, 35(8):1798‚Äì1828.\\nBengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003.\\nA neural probabilistic language model. JMLR, 3:1137‚Äì\\n1155.\\nBengio, Y., H. Schwenk, J.-S. Sen¬¥ecal, F. Morin, and J.-L.\\nGauvain. 2006. Neural probabilistic language models. In\\nInnovations in Machine Learning, 137‚Äì186. Springer.\\nBisk, Y., A. Holtzman, J. Thomason, J. Andreas, Y. Bengio,\\nJ. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich,\\nN. Pinto, and J. Turian. 2020. Experience grounds lan-\\nguage. EMNLP.\\nBlei, D. M., A. Y. Ng, and M. I. Jordan. 2003. Latent Dirich-\\nlet allocation. JMLR, 3(5):993‚Äì1022.\\nBlodgett, S. L., S. Barocas, H. Daum¬¥e III, and H. Wallach.\\n2020. Language (technology) is power: A critical survey\\nof ‚Äúbias‚Äù in NLP. ACL.\\nBojanowski, P., E. Grave, A. Joulin, and T. Mikolov. 2017.\\nEnriching word vectors with subword information. TACL,\\n5:135‚Äì146.\\nBolukbasi, T., K.-W. Chang, J. Zou, V. Saligrama, and A. T.\\nKalai. 2016. Man is to computer programmer as woman\\nis to homemaker? Debiasing word embeddings. NeurIPS.\\nBr¬¥eal, M. 1897. Essai de S¬¥emantique: Science des signiÔ¨Åca-\\ntions. Hachette.\\nBudanitsky, A. and G. Hirst. 2006.\\nEvaluating WordNet-\\nbased measures of lexical semantic relatedness. Compu-\\ntational Linguistics, 32(1):13‚Äì47.\\nBullinaria, J. A. and J. P. Levy. 2007. Extracting seman-\\ntic representations from word co-occurrence statistics:\\nA computational study.\\nBehavior research methods,\\n39(3):510‚Äì526.\\nBullinaria, J. A. and J. P. Levy. 2012. Extracting semantic\\nrepresentations from word co-occurrence statistics: stop-\\nlists, stemming, and SVD. Behavior research methods,\\n44(3):890‚Äì907.\\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman-\\ntics derived automatically from language corpora contain\\nhuman-like biases. Science, 356(6334):183‚Äì186.\\nCarlson, G. N. 1977. Reference to kinds in English. Ph.D.\\nthesis, University of Massachusetts, Amherst. Forward.\\nClark, E. 1987. The principle of contrast: A constraint on\\nlanguage acquisition. In B. MacWhinney, ed., Mecha-\\nnisms of language acquisition, 1‚Äì33. LEA.\\nCoccaro, N. and D. Jurafsky. 1998. Towards better integra-\\ntion of semantic predictors in statistical language model-\\ning. ICSLP.\\nCollobert, R. and J. Weston. 2007. Fast semantic extraction\\nusing a novel neural network architecture. ACL.\\nCollobert, R. and J. Weston. 2008. A uniÔ¨Åed architecture for\\nnatural language processing: Deep neural networks with\\nmultitask learning. ICML.\\nCollobert,\\nR.,\\nJ.\\nWeston,\\nL.\\nBottou,\\nM.\\nKarlen,\\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural language\\nprocessing (almost) from scratch. JMLR, 12:2493‚Äì2537.\\nCordier, B. 1965. Factor-analysis of correspondences. COL-\\nING 1965.\\nCrawford, K. 2017.\\nThe trouble with bias.\\nKeynote at\\nNeurIPS.\\nCruse, D. A. 2004. Meaning in Language: an Introduction\\nto Semantics and Pragmatics. Oxford University Press.\\nSecond edition.\\nDavies, M. 2012.\\nExpanding horizons in historical lin-\\nguistics with the 400-million word Corpus of Historical\\nAmerican English. Corpora, 7(2):121‚Äì157.\\nDavies, M. 2015. The Wikipedia Corpus: 4.6 million arti-\\ncles, 1.9 billion words. Adapted from Wikipedia. https:\\n//www.english-corpora.org/wiki/.\\nDeerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harsh-\\nman, T. K. Landauer, K. E. Lochbaum, and L. Streeter.\\n1988. Computer information retrieval using latent seman-\\ntic structure: US Patent 4,839,853.\\nDeerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Fur-\\nnas, and R. A. Harshman. 1990. Indexing by latent se-\\nmantics analysis. JASIS, 41(6):391‚Äì407.\\nEthayarajh, K., D. Duvenaud, and G. Hirst. 2019a. Towards\\nunderstanding linear word analogies. ACL.\\nEthayarajh, K., D. Duvenaud, and G. Hirst. 2019b. Under-\\nstanding undesirable word embedding associations. ACL.\\nFinkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin,\\nZ. Solan, G. Wolfman, and E. Ruppin. 2002.\\nPlacing\\nsearch in context: The concept revisited. ACM Trans-\\nactions on Information Systems, 20(1):116‚Äî-131.\\nFirth, J. R. 1957.\\nA synopsis of linguistic theory 1930‚Äì\\n1955. In Studies in Linguistic Analysis. Philological So-\\nciety. Reprinted in Palmer, F. (ed.) 1968. Selected Papers\\nof J. R. Firth. Longman, Harlow.\\nGarg, N., L. Schiebinger, D. Jurafsky, and J. Zou. 2018.\\nWord embeddings quantify 100 years of gender and eth-\\nnic stereotypes. Proceedings of the National Academy of\\nSciences, 115(16):E3635‚ÄìE3644.\\nGirard, G. 1718. La justesse de la langue franc¬∏oise: ou les\\ndiff¬¥erentes signiÔ¨Åcations des mots qui passent pour syn-\\nonimes. Laurent d‚ÄôHoury, Paris.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 25, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='26\\nChapter 5\\n‚Ä¢\\nEmbeddings\\nGiuliano,\\nV. E. 1965.\\nThe interpretation of word\\nassociations.\\nStatistical Association Methods For\\nMechanized\\nDocumentation.\\nSymposium\\nProceed-\\nings.\\nWashington,\\nD.C.,\\nUSA,\\nMarch\\n17,\\n1964.\\nhttps://nvlpubs.nist.gov/nistpubs/Legacy/\\nMP/nbsmiscellaneouspub269.pdf.\\nGladkova, A., A. Drozd, and S. Matsuoka. 2016. Analogy-\\nbased detection of morphological and semantic relations\\nwith word embeddings: what works and what doesn‚Äôt.\\nNAACL Student Research Workshop.\\nGlenberg, A. M. and D. A. Robertson. 2000. Symbol ground-\\ning and meaning: A comparison of high-dimensional and\\nembodied theories of meaning. Journal of memory and\\nlanguage, 43(3):379‚Äì401.\\nGonen, H. and Y. Goldberg. 2019. Lipstick on a pig: Debi-\\nasing methods cover up systematic gender biases in word\\nembeddings but do not remove them. NAACL HLT.\\nGould, S. J. 1980. The Panda‚Äôs Thumb. Penguin Group.\\nGreenwald, A. G., D. E. McGhee, and J. L. K. Schwartz.\\n1998. Measuring individual differences in implicit cogni-\\ntion: the implicit association test. Journal of personality\\nand social psychology, 74(6):1464‚Äì1480.\\nHamilton, W. L., J. Leskovec, and D. Jurafsky. 2016. Di-\\nachronic word embeddings reveal statistical laws of se-\\nmantic change. ACL.\\nHarris, Z. S. 1954. Distributional structure. Word, 10:146‚Äì\\n162.\\nHellrich,\\nJ. and U. Hahn. 2016.\\nBad company‚Äî\\nNeighborhoods in neural embedding spaces considered\\nharmful. COLING.\\nHill, F., R. Reichart, and A. Korhonen. 2015. Simlex-999:\\nEvaluating semantic models with (genuine) similarity es-\\ntimation. Computational Linguistics, 41(4):665‚Äì695.\\nHjelmslev, L. 1969. Prologomena to a Theory of Language.\\nUniversity of Wisconsin Press. Translated by Francis J.\\nWhitÔ¨Åeld; original Danish edition 1943.\\nHofmann, T. 1999. Probabilistic latent semantic indexing.\\nSIGIR-99.\\nHuang, E. H., R. Socher, C. D. Manning, and A. Y. Ng. 2012.\\nImproving word representations via global context and\\nmultiple word prototypes. ACL.\\nJia, S., T. Meng, J. Zhao, and K.-W. Chang. 2020. Mitigat-\\ning gender bias ampliÔ¨Åcation in distribution by posterior\\nregularization. ACL.\\nJones, M. P. and J. H. Martin. 1997. Contextual spelling cor-\\nrection using latent semantic analysis. ANLP.\\nJoos, M. 1950.\\nDescription of language design.\\nJASA,\\n22:701‚Äì708.\\nJurgens, D., S. M. Mohammad, P. Turney, and K. Holyoak.\\n2012. SemEval-2012 task 2: Measuring degrees of rela-\\ntional similarity. *SEM 2012.\\nKatz, J. J. and J. A. Fodor. 1963. The structure of a semantic\\ntheory. Language, 39:170‚Äì210.\\nKiela, D. and S. Clark. 2014. A systematic study of semantic\\nvector space model parameters. EACL 2nd Workshop on\\nContinuous Vector Space Models and their Composition-\\nality (CVSC).\\nKim,\\nE.\\n2019.\\nOptimize\\ncomputational\\nefÔ¨Åciency\\nof skip-gram with negative sampling.\\nhttps://\\naegis4048.github.io/optimize_computational_\\nefficiency_of_skip-gram_with_negative_\\nsampling.\\nLake, B. M. and G. L. Murphy. 2021.\\nWord meaning in\\nminds and machines. Psychological Review. In press.\\nLandauer, T. K. and S. T. Dumais. 1997. A solution to Plato‚Äôs\\nproblem: The Latent Semantic Analysis theory of acqui-\\nsition, induction, and representation of knowledge. Psy-\\nchological Review, 104:211‚Äì240.\\nLandauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner.\\n1997. How well can passage meaning be derived with-\\nout using word order? A comparison of Latent Semantic\\nAnalysis and humans. COGSCI.\\nLapesa, G. and S. Evert. 2014. A large scale evaluation of\\ndistributional semantic models: Parameters, interactions\\nand model selection. TACL, 2:531‚Äì545.\\nLee, D. D. and H. S. Seung. 1999. Learning the parts of\\nobjects by non-negative matrix factorization.\\nNature,\\n401(6755):788‚Äì791.\\nLevy, O. and Y. Goldberg. 2014a. Dependency-based word\\nembeddings. ACL.\\nLevy, O. and Y. Goldberg. 2014b. Linguistic regularities in\\nsparse and explicit word representations. CoNLL.\\nLevy, O. and Y. Goldberg. 2014c. Neural word embedding\\nas implicit matrix factorization. NeurIPS.\\nLevy, O., Y. Goldberg, and I. Dagan. 2015. Improving dis-\\ntributional similarity with lessons learned from word em-\\nbeddings. TACL, 3:211‚Äì225.\\nLin, Y., J.-B. Michel, E. Lieberman Aiden, J. Orwant,\\nW. Brockman, and S. Petrov. 2012. Syntactic annotations\\nfor the Google Books NGram corpus. ACL.\\nLinzen, T. 2016. Issues in evaluating semantic spaces us-\\ning word analogies. 1st Workshop on Evaluating Vector-\\nSpace Representations for NLP.\\nManning, C. D., P. Raghavan, and H. Sch¬®utze. 2008. Intro-\\nduction to Information Retrieval. Cambridge.\\nMikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Ef-\\nÔ¨Åcient estimation of word representations in vector space.\\nICLR 2013.\\nMikolov, T., S. Kombrink, L. Burget, J. H. ÀáCernock`y, and\\nS. Khudanpur. 2011. Extensions of recurrent neural net-\\nwork language model. ICASSP.\\nMikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and\\nJ. Dean. 2013b. Distributed representations of words and\\nphrases and their compositionality. NeurIPS.\\nMikolov, T., W.-t. Yih, and G. Zweig. 2013c.\\nLinguis-\\ntic regularities in continuous space word representations.\\nNAACL HLT.\\nNosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002a.\\nHarvesting implicit group attitudes and beliefs from a\\ndemonstration web site. Group Dynamics: Theory, Re-\\nsearch, and Practice, 6(1):101.\\nNosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002b.\\nMath=male, me=female, therefore mathÃ∏= me. Journal of\\npersonality and social psychology, 83(1):44.\\nOsgood, C. E., G. J. Suci, and P. H. Tannenbaum. 1957. The\\nMeasurement of Meaning. University of Illinois Press.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 26, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Exercises\\n27\\nPennington, J., R. Socher, and C. D. Manning. 2014. GloVe:\\nGlobal vectors for word representation. EMNLP.\\nPeterson, J. C., D. Chen, and T. L. GrifÔ¨Åths. 2020. Parallelo-\\ngrams revisited: Exploring the limitations of vector space\\nmodels for simple analogies. Cognition, 205.\\nPilehvar, M. T. and J. Camacho-Collados. 2019. WiC: the\\nword-in-context dataset for evaluating context-sensitive\\nmeaning representations. NAACL HLT.\\nRehder, B., M. E. Schreiner, M. B. W. Wolfe, D. Laham,\\nT. K. Landauer, and W. Kintsch. 1998.\\nUsing Latent\\nSemantic Analysis to assess knowledge: Some technical\\nconsiderations. Discourse Processes, 25(2-3):337‚Äì354.\\nRohde, D. L. T., L. M. Gonnerman, and D. C. Plaut. 2006.\\nAn improved model of semantic similarity based on lexi-\\ncal co-occurrence. CACM, 8:627‚Äì633.\\nRumelhart, D. E. and A. A. Abrahamson. 1973. A model for\\nanalogical reasoning. Cognitive Psychology, 5(1):1‚Äì28.\\nSalton, G. 1971. The SMART Retrieval System: Experiments\\nin Automatic Document Processing. Prentice Hall.\\nSchluter, N. 2018. The word analogy testing caveat. NAACL\\nHLT.\\nSchone, P. and D. Jurafsky. 2000. Knowlege-free induction\\nof morphology using latent semantic analysis. CoNLL.\\nSchone, P. and D. Jurafsky. 2001a. Is knowledge-free in-\\nduction of multiword unit dictionary headwords a solved\\nproblem? EMNLP.\\nSchone, P. and D. Jurafsky. 2001b. Knowledge-free induc-\\ntion of inÔ¨Çectional morphologies. NAACL.\\nSch¬®utze, H. 1992. Dimensions of meaning. Proceedings of\\nSupercomputing ‚Äô92. IEEE Press.\\nSch¬®utze, H. 1997. Ambiguity Resolution in Language Learn-\\ning ‚Äì Computational and Cognitive Models. CSLI, Stan-\\nford, CA.\\nSch¬®utze, H., D. A. Hull, and J. Pedersen. 1995. A compar-\\nison of classiÔ¨Åers and document representations for the\\nrouting problem. SIGIR-95.\\nSch¬®utze, H. and J. Pedersen. 1993. A vector model for syn-\\ntagmatic and paradigmatic relatedness. 9th Annual Con-\\nference of the UW Centre for the New OED and Text Re-\\nsearch.\\nSparck Jones, K. 1972. A statistical interpretation of term\\nspeciÔ¨Åcity and its application in retrieval. Journal of Doc-\\numentation, 28(1):11‚Äì21.\\nSparck Jones, K. 1986. Synonymy and Semantic ClassiÔ¨Åca-\\ntion. Edinburgh University Press, Edinburgh. Republica-\\ntion of 1964 PhD Thesis.\\nSwitzer, P. 1965.\\nVector images in document retrieval.\\nStatistical Association Methods For Mechanized Docu-\\nmentation. Symposium Proceedings. Washington, D.C.,\\nUSA, March 17, 1964. https://nvlpubs.nist.gov/\\nnistpubs/Legacy/MP/nbsmiscellaneouspub269.\\npdf.\\nTian, Y., V. Kulkarni, B. Perozzi, and S. Skiena. 2016. On\\nthe convergent properties of word embedding methods.\\nArXiv preprint arXiv:1605.03956.\\nTurian, J., L. Ratinov, and Y. Bengio. 2010. Word represen-\\ntations: a simple and general method for semi-supervised\\nlearning. ACL.\\nTurney, P. D. and M. L. Littman. 2005. Corpus-based learn-\\ning of analogies and semantic relations. Machine Learn-\\ning, 60(1-3):251‚Äì278.\\nvan der Maaten, L. and G. E. Hinton. 2008. Visualizing high-\\ndimensional data using t-SNE. JMLR, 9:2579‚Äì2605.\\nWierzbicka, A. 1992. Semantics, Culture, and Cognition:\\nUniversity Human Concepts in Culture-SpeciÔ¨Åc ConÔ¨Ågu-\\nrations. Oxford University Press.\\nWierzbicka, A. 1996. Semantics: Primes and Universals.\\nOxford University Press.\\nWittgenstein, L. 1953. Philosophical Investigations. (Trans-\\nlated by Anscombe, G.E.M.). Blackwell.\\nZhao, J., T. Wang, M. Yatskar, V. Ordonez, and K.-\\nW. Chang. 2017.\\nMen also like shopping: Reducing\\ngender bias ampliÔ¨Åcation using corpus-level constraints.\\nEMNLP.\\nZhao, J., Y. Zhou, Z. Li, W. Wang, and K.-W. Chang. 2018.\\nLearning gender-neutral word embeddings. EMNLP.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251127215050', 'source': '../data/pdf/NNDT.pdf', 'file_path': '../data/pdf/NNDT.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251127215050', 'page': 0, 'source_file': 'NNDT.pdf', 'file_type': 'pdf'}, page_content='Neural Networks are Decision Trees\\nCaglar Aytekin\\nAI Lead\\nAAC Technologies\\ncaglaraytekin@aactechnologies.com, cagosmail@gmail.com\\nAbstract\\nIn this manuscript, we show that any neural network\\nwith any activation function can be represented as a de-\\ncision tree. The representation is equivalence and not an\\napproximation, thus keeping the accuracy of the neural net-\\nwork exactly as is. We believe that this work provides bet-\\nter understanding of neural networks and paves the way to\\ntackle their black-box nature. We share equivalent trees\\nof some neural networks and show that besides providing\\ninterpretability, tree representation can also achieve some\\ncomputational advantages for small networks. The analysis\\nholds both for fully connected and convolutional networks,\\nwhich may or may not also include skip connections and/or\\nnormalizations.\\n1. Introduction\\nDespite the immense success of neural networks over the\\npast decade, the black-box nature of their predictions pre-\\nvent their wider and more reliable adoption in many indus-\\ntries, such as health and security. This fact led researchers to\\ninvestigate ways to explain neural network decisions. The\\nefforts in explaining neural network decisions can be cate-\\ngorized into several approaches: saliency maps, approxima-\\ntion by interpretable methods and joint models.\\nSaliency maps are ways of highlighting areas on the in-\\nput, of which a neural network make use of the most while\\nprediction. An earlier work [20] takes the gradient of the\\nneural network output with respect to the input in order to\\nvisualize an input-speciÔ¨Åc linearization of the entire net-\\nwork. Another work [26] uses a deconvnet to go back to\\nfeatures from decisions. The saliency maps obtained via\\nthese methods are often noisy and prevent a clear under-\\nstanding of the decisions made. Another track of meth-\\nods [29], [18], [4], [6] make use of the derivative of a neural\\nnetwork output with respect to an activation, usually the one\\nright before fully connected layers. This saliency maps ob-\\ntained by this track, and some other works [27], [11], [5] are\\nclearer in the sense of highlighting areas related to the pre-\\ndicted class. Although useful for purposes such as check-\\ning whether the support area for decisions are sound, these\\nmethods still lack a detailed logical reasoning of why such\\ndecision is made.\\nConversion between neural networks and interpretable\\nby-design models -such as decision trees- has been a topic\\nof interest. In [8], a method was devised to initialize neural\\nnetworks with decision trees. [9, 19, 25] also provides neu-\\nral network equivalents of decision trees. The neural net-\\nworks in these works have speciÔ¨Åc architectures, thus the\\nconversion lacks generalization to any model. In [24], neu-\\nral networks were trained in such a way that their decision\\nboundaries can be approximated by trees. This work does\\nnot provide a correspondence between neural networks and\\ndecision trees, and merely uses the latter as a regulariza-\\ntion. In [7], a neural network was used to train a decision\\ntree. Such tree distillation is an approximation of a neural\\nnetwork and not a direct conversion, thus performs poorly\\non the tasks that the neural network was trained on.\\nJoint neural network and decision tree models [12], [16],\\n[13], [14], [17], [2], [10], [22] genarally use deep learning to\\nassists some trees, or come up with a neural network struc-\\nture so it resembles a tree. A recent work [23] replaces the\\nÔ¨Ånal fully connected layer of a neural network with a deci-\\nsion tree. Since the backbone features are still that of neural\\nnetworks, the explanation is sought to be achieved via pro-\\nviding a means to humans to validate the decision as a good\\nor bad one, rather than a complete logical reasoning of the\\ndecision.\\nIn this paper, we show that any neural network having\\nany activations has a directly equivalent decision tree rep-\\nresentation. Thus, the induced tree output is exactly the\\nsame with that of the neural network and tree representation\\ndoesn‚Äôt limit or require altering of the neural architecture\\nin any way. We believe that the decision tree equivalence\\nprovides better understanding of neural networks and paves\\nthe way to tackle the black-box nature of neural networks,\\ne.g. via analyzing the category that a test sample belongs\\nto, which can be extracted by the node rules that a sample\\nis categorized. We show that the decision tree equivalent of\\narXiv:2210.05189v3  [cs.LG]  25 Oct 2022'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251127215050', 'source': '../data/pdf/NNDT.pdf', 'file_path': '../data/pdf/NNDT.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251127215050', 'page': 1, 'source_file': 'NNDT.pdf', 'file_type': 'pdf'}, page_content='a neural network can be found for either fully connected or\\nconvolutional neural networks which may include skip lay-\\ners and normalizations as well. Besides the interpretability\\naspect, we show that the induced tree is also advantageous\\nto the corresponding neural network in terms of computa-\\ntional complexity, at the expense of increased storage mem-\\nory.\\nUpon writing this paper, we have noticed the following\\nworks having overlaps with ours [28], [3], [15], [21], es-\\npecially for feedforward ReLU networks. We extend the\\nÔ¨Åndings in these works to any activation function and also\\nrecurrent neural networks.\\n2. Decision Tree Analysis of Neural Networks\\nThe derivations in this section will be Ô¨Årst made for feed-\\nforward neural networks with piece-wise linear activation\\nfunctions such as ReLU, Leaky ReLU, etc. Next, the analy-\\nsis will be extended to any neural network with any activa-\\ntion function.\\n2.1. Fully Connected Networks\\nLet Wi be the weight matrix of a network‚Äôs ith layer.\\nLet œÉ be any piece-wise linear activation function, and x0\\nbe the input to the neural network. Then, the output and an\\nintermediate feature of a feed-forward neural network can\\nbe represented as in Eq. 1.\\nNN(x0) = WT\\nn‚àí1œÉ(WT\\nn‚àí2œÉ(...WT\\n1 œÉ(WT\\n0 x0)))\\nxi = œÉ(WT\\ni‚àí1œÉ(...WT\\n1 œÉ(WT\\n0 x0)))\\n(1)\\nNote that in Eq. 1, we omit any Ô¨Ånal activation (e.g.\\nsoftmax) and we ignore the bias term as it can be simply\\nincluded by concatenating a 1 value to each xi. The acti-\\nvation function œÉ acts as an element-wise scalar multiplica-\\ntion, hence the following can be written.\\nWT\\ni œÉ(WT\\ni‚àí1xi‚àí1) = WT\\ni (ai‚àí1 ‚äô(WT\\ni‚àí1xi‚àí1))\\n(2)\\nIn Eq. 2, ai‚àí1 is a vector indicating the slopes of activa-\\ntions in the corresponding linear regions where WT\\ni‚àí1xi‚àí1\\nfall into, ‚äôdenotes element-wise multiplication. Note that,\\nai‚àí1 can directly be interpreted as a categorization result\\nsince it includes indicators (slopes) of linear regions in ac-\\ntivation function. The Eq. 2 can be re-organized as follows.\\nWT\\ni œÉ(WT\\ni‚àí1xi‚àí1) = (Wi ‚äôai‚àí1)T WT\\ni‚àí1xi‚àí1\\n(3)\\nIn Eq.\\n3, we use ‚äôas a column-wise element-wise\\nmultiplication on Wi. This corresponds to element-wise\\nmultiplication by a matrix obtained via by repeating ai‚àí1\\ncolumn-vector to match the size of Wi. Using Eq. 3, Eq. 1\\ncan be rewritten as follows.\\nNN(x0) = (Wn‚àí1 ‚äôan‚àí2)T (Wn‚àí2 ‚äôan‚àí3)T\\n...(W1 ‚äôa0)T WT\\n0 x0\\n(4)\\nFrom Eq. 4, one can deÔ¨Åne an effective weight matrix\\nÀÜW\\nT\\ni of a layer i to be applied directly on input x0 as follows:\\nCi‚àí1 ÀÜW\\nT\\ni = (Wi ‚äôai‚àí1)T ...(W1 ‚äôa0)T WT\\n0\\nCi‚àí1 ÀÜW\\nT\\ni x0 = WT\\ni xi\\n(5)\\nIn Eq. 5, the categorization vector until layer i is de-\\nÔ¨Åned as follows: ci‚àí1 = a0 ‚à•a1 ‚à•...ai‚àí1, where ‚à•is the\\nconcatenation operator.\\nOne can directly observe from Eq.\\n5 that, the effec-\\ntive matrix of layer i is only dependent on the categoriza-\\ntion vectors from previous layers. This indicates that in\\neach layer, a new efÔ¨Åcient Ô¨Ålter is selected -to be applied\\non the network input- based on the previous categoriza-\\ntions/decisions. This directly shows that a fully connected\\nneural network can be represented as a single decision tree,\\nwhere effective matrices acts as categorization rules.\\nIn\\neach layer i, response of effective matrix Ci‚àí1 ÀÜW\\nT\\ni is catego-\\nrized into ai vector, and based on this categorization result,\\nnext layer‚Äôs effective matrix Ci ÀÜW\\nT\\ni+1 is determined. A layer\\ni is thus represented as kmi-way categorization, where mi is\\nthe number Ô¨Ålters in each layer i and k is the total number\\nof linear regions in an activation. This categorization in a\\nlayer i can thus be represented by a tree of depth mi, where\\na node in any depth is branched into k categorizations.\\nIn order to better illustrate the equivalent decision tree of\\na neural network, in Algorithm 1, we rewrite Eq. 5 for the\\nentire network, as an algorithm. For the sake of simplic-\\nity and without loss of generality, we provide the algorithm\\nwith the ReLU activation function, where a ‚àà{0, 1}. It\\ncan be clearly observed that, the lines 5 ‚àí9 in Algorithm 1\\ncorresponds to a node in the decision tree, where a simple\\nyes/no decision is made.\\nThe decision tree equivalent of a neural network can thus\\nbe constructed as in Algorithm 2. Using this algorithm,\\nwe share a a tree representation obtained for a neural net-\\nwork with three layers, having 2,1 and 1 Ô¨Ålter for layer 1,\\n2 and 3 respectively. The network has ReLU activation in\\nbetween layers, and no activation after last layer. It can be\\nobserved from Algorithm 2 and Fig. 1 that the depth of a\\nNN-equivalent tree is d = Pn‚àí2\\ni=0 mi, and total number of\\ncategories in last branch is 2d. At Ô¨Årst glance, the number\\nof categories seem huge. For example, if Ô¨Årst layer of a neu-\\nral network contains 64 Ô¨Ålters, there would exist at least 264\\nbranches in a tree, which is already intractable. But, there'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251127215050', 'source': '../data/pdf/NNDT.pdf', 'file_path': '../data/pdf/NNDT.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251127215050', 'page': 2, 'source_file': 'NNDT.pdf', 'file_type': 'pdf'}, page_content='WT\\n00x0 > 0\\nWT\\n01x0 > 0\\n00 ÀÜ\\nW10\\nT x0 > 0\\n000 ÀÜ\\nW20\\nT x0\\n001 ÀÜ\\nW20\\nT x0\\n01 ÀÜ\\nW10\\nT x0 > 0\\n010 ÀÜ\\nW20\\nT x0\\n011 ÀÜ\\nW20\\nT x0\\nWT\\n01x0 > 0\\n10 ÀÜ\\nW10\\nT x0 > 0\\n100 ÀÜ\\nW20\\nT x0\\n101 ÀÜ\\nW20\\nT x0\\n11 ÀÜ\\nW10\\nT x0 > 0\\n110 ÀÜ\\nW20\\nT x0\\n111 ÀÜ\\nW20\\nT x0\\nFigure 1. Decision Tree for a 2-layer ReLU Neural Network\\nmay occur violating and redundant rules that would pro-\\nvide lossless pruning of the NN-equivalent tree. Another\\nobservation is that, it is highly likely that not all categories\\nwill be realized during training due to the possibly much\\nlarger number of categories (tree leaves) than training data.\\nThese categories can be pruned as well based on the ap-\\nplication, and the data falling into these categories can be\\nconsidered invalid, if the application permits. In the next\\nsection, we show that such redundant, violating and unre-\\nalized categories indeed exist, by analysing decision trees\\nof some neural networks. But before that, we show that the\\ntree equivalent of a neural network exists for skip connec-\\ntions, normalizations, convolutions, other activation func-\\ntions and recurrence.\\n2.1.1\\nSkip Connections\\nWe analyse a residual neural network of the following type:\\nrx0 = WT\\n0 x0\\nrxi = rxi‚àí1 + WT\\ni œÉ(rxi‚àí1)\\n(6)\\nUsing Eq. 6, via a similar analysis in Sec. 2.1, one can\\nrewrite rxi as follows.\\nrxi = ai‚àí1 ÀÜW\\nT\\ni rxi‚àí1\\nai‚àí1 ÀÜW\\nT\\ni = I + (Wi ‚äôai‚àí1)T\\n(7)\\nFinally, using ai‚àí1 ÀÜW\\nT\\ni in Eq. 7, one can deÔ¨Åne effective\\nmatrices for residual neural networks as follows.\\nrxi = r ÀÜW\\nT\\ni x0\\nr ÀÜW\\nT\\ni = ai‚àí1 ÀÜW\\nT\\ni ai‚àí2 ÀÜW\\nT\\ni‚àí1...a0 ÀÜW\\nT\\n1 WT\\n0\\n(8)\\nOne can observe from Eq. 8 that, for layer i, the residual\\neffective matrix r ÀÜW\\nT\\ni is deÔ¨Åned solely based on categoriza-\\ntions from the previous activations. Similar to the analysis\\nAlgorithm 1: Algorithm of Eq. 5 for ReLU net-\\nworks\\n1 ÀÜW = W0\\n2 for i = 0 to n ‚àí2 do\\n3\\na = []\\n4\\nfor j = 0 to mi ‚àí1 do\\n5\\nif ÀÜW\\nT\\nijx0 > 0 then\\n6\\na.append(1)\\n7\\nelse\\n8\\na.append(0)\\n9\\nend\\n10\\nend\\n11\\nÀÜW = ÀÜW(Wi+1 ‚äôa)\\n12 end\\n13 return ÀÜW\\nT x0\\nAlgorithm 2: Algorithm of converting neural net-\\nworks to decision trees\\n1 Initialize Tree: Set root.\\n2 Branch all leafs to k nodes, decision rule is Ô¨Årst\\neffective Ô¨Ålter.\\n3 Branch all nodes to k more nodes, and repeat until\\nall effective Ô¨Ålters in a layer is covered.\\n4 Calculate effective matrix for each leaf via Eq. 5.\\nRepeat 2,3.\\n5 Repeat 4 until all layers are covered.\\n6 return Tree\\nin Sec. 2.1, this enables a tree equivalent of residual neural\\nnetworks.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251127215050', 'source': '../data/pdf/NNDT.pdf', 'file_path': '../data/pdf/NNDT.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251127215050', 'page': 3, 'source_file': 'NNDT.pdf', 'file_type': 'pdf'}, page_content='2.1.2\\nNormalization Layers\\nA separate analysis is not needed for any normalization\\nlayer, as popular normalization layers are linear, and after\\ntraining, they can be embedded into the linear layer that it\\ncomes after or before, in pre-activation or post-activation\\nnormalizations respectively.\\n2.2. Convolutional Neural Networks\\nLet Ki : Ci+1 √óCi √óMi √óNi be the convolution kernel\\nfor layer i, applying on an input Fi : Ci √ó Hi √ó Wi. Note\\nthat Mi and Ni denote the spatial size of the convolutional\\nkernel, and Hi and Wi denote the spatial size of the input.\\nOne can write the output of a convolutional neural net-\\nwork CNN(F0), and an intermediate feature Fi as follows.\\nCNN(F0) = Kn‚àí1 ‚àóœÉ(Kn‚àí2 ‚àóœÉ(...œÉ(K0 ‚àóF0))\\nFi = œÉ(Ki‚àí1 ‚àóœÉ(...œÉ(K0 ‚àóF0))\\n(9)\\nSimilar to the fully connected network analysis, one can\\nwrite the following, due to element-wise scalar multiplica-\\ntion nature of the activation function.\\nKi ‚àóœÉ(Ki‚àí1 ‚àóFi‚àí1) = (Ki ‚äôai‚àí1) ‚àó(Ki‚àí1 ‚àóFi‚àí1) (10)\\nIn Eq. 10, ai‚àí1 is of same spatial size as Ki and consists\\nof the slopes of activation function in corresponding regions\\nin the previous feature Fi‚àí1. Note that the above only holds\\nfor a speciÔ¨Åc spatial region, and there exists a separate ai‚àí1\\nfor each spatial region that the convolution Ki‚àí1 is applied\\nto. For example, if Ki‚àí1 is a 3 √ó 3 kernel, there exists a\\nseparate ai‚àí1 for all 3 √ó 3 regions that the convolution is\\napplied to. An effective convolution Ci‚àí1 ÀÜKi can be written\\nas follows.\\nci‚àí1 ÀÜKi = (Ki ‚äôai‚àí1) ‚àó... ‚àó(K1 ‚äôa0) ‚àóK0\\nci‚àí1 ÀÜKi ‚àóx0 = Ki ‚àóxi\\n(11)\\nNote that in Eq. 11, Ci‚àí1 ÀÜKi contains speciÔ¨Åc effective\\nconvolutions per region, where a region is deÔ¨Åned accord-\\ning to the receptive Ô¨Åeld of layer i. c is deÔ¨Åned as the con-\\ncatenated categorization results of all relevant regions from\\nprevious layers.\\nOne can observe from Eq.\\n11 that effecive convolu-\\ntions are only dependent on categorizations coming from\\nactivations, which enables the tree equivalence -similar to\\nthe analysis for fully connected network. A difference from\\nfully connected layer case is that many decisions are made\\non partial input regions rather than entire x0.\\n2.3. Continuous Activation Functions\\nIn Eq. 2, for piece-wise linear activations, elements of a\\ncan have a number of values limited by the piece-wise lin-\\near regions in the activation function. This number deÔ¨Ånes\\nthe number of child nodes per effective Ô¨Ålter. The exten-\\nsion to continuous activation functions is trivial as they can\\nbe considered as piece-wise linear functions with inÔ¨Ånite\\nregions. Therefore, for continuous activations, the neural\\nnetwork equivalent tree immediately becomes inÔ¨Ånite width\\neven for a single Ô¨Ålter. This might not be a useful result, but\\nwe provide this discussion here for completeness. In order\\nto guarantee Ô¨Ånite trees, one may consider using quantized\\nversions of continuous activations which may result in a few\\npiece-wise linear regions, hence few child nodes per activa-\\ntion.\\n2.4. Recurrent Networks\\nAs recurrent neural networks (RNNs) can be unrolled to\\nfeed-forward representation, RNNs can also be equivalently\\nrepresented as decision trees. We study following recurrent\\nneural network. Note that we simply omit the bias terms as\\nthey can be represented by concatenating a 1 value to input\\nvectors.\\nh(t) = œÉ(WT h(t‚àí1) + UT x(t))\\no(t) = VT h(t)\\n(12)\\nSimilar to previous analysis, one can rewrite h(t) as fol-\\nlows.\\nh(t) = a(t) ‚äô(WT h(t‚àí1) + UT x(t))\\n(13)\\nEq. 13 can be rewritten follows.\\nh(t) = a(t) ‚äô(\\n1\\nY\\nj=(t‚àí1)\\n(WT ‚äôa(j)))WT h(0)\\n+a(t) ‚äô\\nt\\nX\\ni=1\\n(\\niY\\nj=(t‚àí1)\\n(WT ‚äôa(j)))UT x(i)\\n(14)\\nNote that in Eq. 14, the product operator stands for ma-\\ntrix multiplication, its steps are ‚àí1 and we consider the out-\\nput of product operator to be 1 when i = t. One can rewrite\\nEq. 14 by introducing cj ÀÜWj as follows.\\nh(t) = a(t) ‚äôc1 ÀÜW1WT h(0) + a(t) ‚äô\\nt\\nX\\ni=1\\nci ÀÜWiUT x(i)\\nci ÀÜW\\nT\\ni =\\niY\\nj=(t‚àí1)\\n(WT ‚äôa(j))\\n(15)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251127215050', 'source': '../data/pdf/NNDT.pdf', 'file_path': '../data/pdf/NNDT.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251127215050', 'page': 4, 'source_file': 'NNDT.pdf', 'file_type': 'pdf'}, page_content='x < ‚àí1.16\\nx < 0.32\\nx > 1\\nx < ‚àí0.1\\n0 ÀÜW\\nT x0\\n1 ÀÜW\\nT x0\\nx0 < ‚àí0.1\\n2 ÀÜW\\nT x\\n3 ÀÜW\\nT x\\nx > 0.54\\nx < 0.11\\n4 ÀÜW\\nT x\\n5 ÀÜW\\nT x\\nx < 0.11\\n6 ÀÜW\\nT x\\n7 ÀÜW\\nT x\\nx < 0.32\\nx > 0.52\\nx < ‚àí0.7\\n8 ÀÜW\\nT x\\n9 ÀÜW\\nT x\\nx < ‚àí0.7\\n10 ÀÜW\\nT x\\n11 ÀÜW\\nT x\\nx > 0.39\\nx < ‚àí0.38\\n12 ÀÜW\\nT x\\n13 ÀÜW\\nT x\\nx < ‚àí0.38\\n14 ÀÜW\\nT x\\n15 ÀÜW\\nT x\\nFigure 2. Decision Tree for a y = x2 Regression Neural Network\\nx < ‚àí1.16\\nx < 0.32\\nx > 1\\n0.55x + 0.09\\n3.47x ‚àí2.83\\nx < 0.11.\\n2.37x ‚àí0.50\\n‚àí1.00x ‚àí0.12\\n‚àí3.67x ‚àí3.22\\n(a) Cleaned Tree\\n(b) Neural Network Approximation of y = x2\\nFigure 3. Cleaned Decision Tree for a y = x2 Regression Neural Network\\nCombining Eq. 15 and Eq. 12, one can write o(t) as\\nfollows.\\no(t) = a(t) ÀÜV\\nT\\nc1 ÀÜW1WT h(0)+a(t) ÀÜV\\nT\\nt\\nX\\ni=1\\nci ÀÜWiUT x(i) (16)\\nEq. 16 can be further simpliÔ¨Åed to the following.\\no(t) = c1 ÀÜZ\\nT\\n1 WT h(0) +\\nt\\nX\\ni=1\\nci ÀÜZiUT x(i)\\n(17)\\nIn Eq. 17, ci ÀÜZ\\nT\\ni = a(t) ÀÜV\\nT\\nci ÀÜWi .As one can observe from\\nEq. 17, the RNN output only depends on the categoriza-\\ntion vector ci, which enables the tree equivalence -similar\\nto previous analysis.\\nNote that for RNNs, a popular choice for œÉ in Eq. 12\\nis tanh. As mentioned in Section 2.3, in order to provide\\nÔ¨Ånite trees, one might consider using a piece-wise linear\\napproximation of tanh.\\n3. Experimental Results\\nFirst, we make a toy experiment where we Ô¨Åt a neural\\nnetwork to: y = x2 equation. The neural network has 3\\ndense layers with 2 Ô¨Ålters each, except for last layer which\\nhas 1 Ô¨Ålter. The network uses leaky-ReLU activations after\\nfully connected layers, except for the last layer which has\\nno post-activation. We have used negative slope of 0.3 for\\nleaky-ReLU which is the default value in TensorÔ¨Çow [1].\\nThe network was trained with 5000 (x, y) pairs where x was\\nregularly sampled from [‚àí2.5, 2.5] interval. Fig. 2 shows\\nthe decision tree corresponding to the neural network. In the\\ntree, every black rectangle box indicates a rule, left child\\nfrom the box means the rule does not hold, and the right\\nchild means the rule holds. For better visualization, the\\nrules are obtained via converting wT x + Œ≤ > 0 to direct\\ninequalities acting on x. This can be done for the partic-\\nular regression y = x2, since x is a scalar. In every leaf,\\nthe network applies a linear function -indicated by a red\\nrectangle- based on the decisions so far. We have avoided\\nwriting these functions explicitly due to limited space. At\\nÔ¨Årst glance, the tree representation of a neural network in\\nthis example seems large due to the 2\\nPn‚àí2\\ni\\nmi = 24 = 16\\ncategorizations. However, we notice that a lot of the rules\\nin the decision tree is redundant, and hence some paths in\\nthe decision tree becomes invalid. An example to redundant\\nrule is checking x < 0.32 after x < ‚àí1.16 rule holds. This'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251127215050', 'source': '../data/pdf/NNDT.pdf', 'file_path': '../data/pdf/NNDT.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251127215050', 'page': 5, 'source_file': 'NNDT.pdf', 'file_type': 'pdf'}, page_content='‚àí0.98x ‚àí0.49y + 0.95\\n‚àí1.6x + 0.2y ‚àí0.07\\n‚àí0.15x + 0.19y + 0.41\\n1\\n0.45x ‚àí0.3y + 0.05\\n0\\n1.6x ‚àí1.35y ‚àí1.44\\n0\\n1\\n0\\n‚àí1.6x + 0.2y ‚àí0.07\\n0.5x + 0.52y ‚àí0.22\\n1\\n‚àí0.46x ‚àí0.76y + 0.94\\n0\\n‚àí2.72x ‚àí3.53y + 2.77\\n0\\n1\\n‚àí0.5x + 0.64y ‚àí0.27\\n1.51x ‚àí1y + 1.03\\n1.59x ‚àí1.35y + 0.78\\n0\\n1\\n4.18x ‚àí3.17y + 2.54\\n0\\n1\\n1.51x ‚àí1y + 1.03\\n0\\n5.31x ‚àí4.51y + 3.14\\n0\\n1\\nFigure 4. ClassiÔ¨Åcation Tree for a Half-Moon ClassiÔ¨Åcation Neural Network\\nFigure 5. Categorizations made by the decision tree for half-moon\\ndataset\\ndirectly creates the invalid left child for this node. Hence,\\nthe tree can be cleaned via removing the left child in this\\ncase, and merging the categorization rule to the stricter one :\\nx < ‚àí1.16 in the particular case. Via cleaning the decision\\ntree in Fig. 2, we obtain the simpler tree in Fig. 3a, which\\nonly consists of 5 categories instead of 16. The 5 categories\\nare directly visible also from the model response in Fig. 3b.\\nThe interpretation of the neural network is thus straightfor-\\nward: for each region whose boundaries are determined via\\nthe decision tree representation, the network approximates\\nthe non-linear y = x2 equation by a linear equation. One\\ncan clearly interpret and moreover make deduction from the\\ndecision tree, some of which are as follows. The neural\\nnetwork is unable to grasp the symmetrical nature of the\\nregression problem which is evident from the fact that the\\ndecision boundaries are asymmetrical. The region in below\\n‚àí1.16 and above 1 is unbounded and thus neural decisions\\nlose accuracy as x goes beyond these boundaries.\\nNext, we investigate another toy problem of classifying\\nhalf-moons and analyse the decision tree produced by a neu-\\nral network. We train a fully connected neural network with\\ny = x2\\nHalf-Moon\\nParam.\\nComp.\\nMult./Add.\\nParam.\\nComp.\\nMult./Add.\\nTree\\n14\\n2.6\\n2\\n39\\n4.1\\n8.2\\nNN\\n13\\n4\\n16\\n15\\n5\\n25\\nTable 1. Computation and memory analysis of toy problems.\\n3 layers with leaky-ReLU activations, except for last layer\\nwhich has sigmoid activation. Each layer has 2 Ô¨Ålters ex-\\ncept for the last layer which has 1. The cleaned decision\\ntree induced by the trained network is shown in Fig. 4. The\\ndecision tree Ô¨Ånds many categories whose boundaries are\\ndetermined by the rules in the tree, where each category\\nis assigned a single class. In order to better visualize the\\ncategories, we illustrate them with different colors in Fig.\\n5. One can make several deductions from the decision tree\\nsuch as some regions are very well-deÔ¨Åned, bounded and\\nthe classiÔ¨Åcations they make are perfectly in line with the\\ntraining data, thus these regions are very reliable. There are\\nunbounded categories which help obtaining accurate classi-\\nÔ¨Åcation boundaries, yet fail to provide a compact represen-\\ntation of the training data, these may correspond to inaccu-\\nrate extrapolations made by neural decisions. There are also\\nsome categories that emerged although none of the training\\ndata falls to them.\\nBesides the interpretability aspect, the decision tree rep-\\nresentation also provides some computational advantages.\\nIn Table 1, we compare the number of parameters, Ô¨Çoat-\\npoint comparisons and multiplication or addition operations\\nof the neural network and the tree induced by it. Note that\\nthe comparisons, multiplications and additions in the tree\\nrepresentation are given as expected values, since per each\\ncategory depth of the tree is different. As the induced tree\\nis an unfolding of the neural network, it covers all possi-\\nble routes and keeps all possible effective Ô¨Ålters in mem-\\nory. Thus, as expected, the number of parameters in the tree\\nrepresentation of a neural network is larger than that of the\\nnetwork. In the induced tree, in every layer i, a maximum'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251127215050', 'source': '../data/pdf/NNDT.pdf', 'file_path': '../data/pdf/NNDT.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251127215050', 'page': 6, 'source_file': 'NNDT.pdf', 'file_type': 'pdf'}, page_content='of mi Ô¨Ålters are applied directly on the input, whereas in the\\nneural network always mi Ô¨Ålters are applied on the previous\\nfeature, which is usually much larger than the input in the\\nfeature dimension. Thus, computation-wise, the tree repre-\\nsentation is advantageous compared to the neural network\\none.\\n4. Conclusion\\nIn this manuscript, we have shown that neural networks\\ncan be equivalently represented as decision trees. The tree\\nequivalence holds for fully connected layers, convolutional\\nlayers, residual connections, normalizations, recurrent lay-\\ners and any activation. We believe that this tree equivalence\\nprovides directions to tackle the black-box nature of neural\\nnetworks.\\nReferences\\n[1] Mart¬¥ƒ±n Abadi, Ashish Agarwal, Paul Barham, Eugene\\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy\\nDavis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian\\nGoodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,\\nYangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\\nKudlur, Josh Levenberg, Dandelion Man¬¥e, Rajat Monga,\\nSherry Moore, Derek Murray, Chris Olah, Mike Schuster,\\nJonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-\\nwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-\\nnanda Vi¬¥egas, Oriol Vinyals, Pete Warden, Martin Watten-\\nberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor-\\nFlow: Large-scale machine learning on heterogeneous sys-\\ntems, 2015. Software available from tensorÔ¨Çow.org. 5\\n[2] Karim Ahmed, Mohammad Haris Baig, and Lorenzo Torre-\\nsani. Network of experts for large-scale image categoriza-\\ntion. In European Conference on Computer Vision, pages\\n516‚Äì532. Springer, 2016. 1\\n[3] Randall Balestriero and Richard Baraniuk.\\nMad max:\\nAfÔ¨Åne spline insights into deep learning.\\narXiv preprint\\narXiv:1805.06576, 2018. 2\\n[4] Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader,\\nand Vineeth N Balasubramanian.\\nGrad-cam++: General-\\nized gradient-based visual explanations for deep convolu-\\ntional networks. In 2018 IEEE winter conference on appli-\\ncations of computer vision (WACV), pages 839‚Äì847. IEEE,\\n2018. 1\\n[5] Edo Collins, Radhakrishna Achanta, and Sabine Susstrunk.\\nDeep feature factorization for concept discovery.\\nIn Pro-\\nceedings of the European Conference on Computer Vision\\n(ECCV), pages 336‚Äì352, 2018. 1\\n[6] Rachel Lea Draelos and Lawrence Carin. Use hirescam in-\\nstead of grad-cam for faithful explanations of convolutional\\nneural networks. arXiv e-prints, pages arXiv‚Äì2011, 2020. 1\\n[7] Nicholas Frosst and Geoffrey Hinton.\\nDistilling a neu-\\nral network into a soft decision tree.\\narXiv preprint\\narXiv:1711.09784, 2017. 1\\n[8] Kelli D Humbird, J Luc Peterson, and Ryan G McClar-\\nren. Deep neural network initialization with decision trees.\\nIEEE transactions on neural networks and learning systems,\\n30(5):1286‚Äì1295, 2018. 1\\n[9] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi,\\nand Samuel Rota Bulo. Deep neural decision forests. In Pro-\\nceedings of the IEEE international conference on computer\\nvision, pages 1467‚Äì1475, 2015. 1\\n[10] Mason McGill and Pietro Perona. Deciding how to decide:\\nDynamic routing in artiÔ¨Åcial neural networks. In Interna-\\ntional Conference on Machine Learning, pages 2363‚Äì2372.\\nPMLR, 2017. 1\\n[11] Mohammed Bany Muhammad and Mohammed Yeasin.\\nEigen-cam: Class activation map using principal compo-\\nnents.\\nIn 2020 International Joint Conference on Neural\\nNetworks (IJCNN), pages 1‚Äì7. IEEE, 2020. 1\\n[12] Ravi Teja Mullapudi, William R Mark, Noam Shazeer, and\\nKayvon Fatahalian. Hydranets: Specialized dynamic archi-\\ntectures for efÔ¨Åcient inference. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition,\\npages 8080‚Äì8089, 2018. 1\\n[13] Calvin Murdock, Zhen Li, Howard Zhou, and Tom Duerig.\\nBlockout: Dynamic model selection for hierarchical deep\\nnetworks. In Proceedings of the IEEE conference on com-\\nputer vision and pattern recognition, pages 2583‚Äì2591,\\n2016. 1\\n[14] Venkatesh N Murthy, Vivek Singh, Terrence Chen, R Man-\\nmatha, and Dorin Comaniciu. Deep decision network for\\nmulti-class image classiÔ¨Åcation.\\nIn Proceedings of the\\nIEEE conference on computer vision and pattern recogni-\\ntion, pages 2240‚Äì2248, 2016. 1\\n[15] Duy T Nguyen, Kathryn E Kasmarik, and Hussein A Ab-\\nbass. Towards interpretable anns: An exact transformation\\nto multi-class multivariate decision trees.\\narXiv preprint\\narXiv:2003.04675, 2020. 2\\n[16] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster,\\nstronger. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pages 7263‚Äì7271, 2017. 1\\n[17] Anirban Roy and Sinisa Todorovic. Monocular depth esti-\\nmation using neural regression forest. In Proceedings of the\\nIEEE conference on computer vision and pattern recogni-\\ntion, pages 5506‚Äì5514, 2016. 1\\n[18] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\\nGrad-cam:\\nVisual explanations from deep networks via\\ngradient-based localization. In Proceedings of the IEEE in-\\nternational conference on computer vision, pages 618‚Äì626,\\n2017. 1\\n[19] Ishwar Krishnan Sethi. Entropy nets: from decision trees\\nto neural networks. Proceedings of the IEEE, 78(10):1605‚Äì\\n1613, 1990. 1\\n[20] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.\\nDeep inside convolutional networks:\\nVisualising image\\nclassiÔ¨Åcation models and saliency maps.\\narXiv preprint\\narXiv:1312.6034, 2013. 1\\n[21] Agus Sudjianto, William Knauth, Rahul Singh, Zebin Yang,\\nand Aijun Zhang. Unwrapping the black box of deep relu\\nnetworks: interpretability, diagnostics, and simpliÔ¨Åcation.\\narXiv preprint arXiv:2011.04041, 2020. 2'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251127215050', 'source': '../data/pdf/NNDT.pdf', 'file_path': '../data/pdf/NNDT.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20251127215050', 'page': 7, 'source_file': 'NNDT.pdf', 'file_type': 'pdf'}, page_content='[22] Andreas Veit and Serge Belongie. Convolutional networks\\nwith adaptive inference graphs. In Proceedings of the Euro-\\npean Conference on Computer Vision (ECCV), pages 3‚Äì18,\\n2018. 1\\n[23] Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee,\\nHenry Jin, Suzanne Petryk, Sarah Adel Bargal, and Joseph E\\nGonzalez.\\nNbdt:\\nneural-backed decision trees.\\narXiv\\npreprint arXiv:2004.00221, 2020. 1\\n[24] Mike Wu, Michael Hughes, Sonali Parbhoo, Maurizio Zazzi,\\nVolker Roth, and Finale Doshi-Velez. Beyond sparsity: Tree\\nregularization of deep models for interpretability. In Pro-\\nceedings of the AAAI conference on artiÔ¨Åcial intelligence,\\nvolume 32, 2018. 1\\n[25] Yongxin Yang, Irene Garcia Morillo, and Timothy M\\nHospedales.\\nDeep neural decision trees.\\narXiv preprint\\narXiv:1806.06988, 2018. 1\\n[26] Matthew D Zeiler and Rob Fergus. Visualizing and under-\\nstanding convolutional networks. In European conference on\\ncomputer vision, pages 818‚Äì833. Springer, 2014. 1\\n[27] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan\\nBrandt, Xiaohui Shen, and Stan Sclaroff. Top-down neu-\\nral attention by excitation backprop. International Journal\\nof Computer Vision, 126(10):1084‚Äì1102, 2018. 1\\n[28] Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical\\ngeometry of deep neural networks. In International Confer-\\nence on Machine Learning, pages 5824‚Äì5832. PMLR, 2018.\\n2\\n[29] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,\\nand Antonio Torralba. Learning deep features for discrimina-\\ntive localization. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 2921‚Äì2929,\\n2016. 1'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 0, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar‚àó\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez‚àó‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiÔ¨Åcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been Ô¨Årmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the Ô¨Årst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefÔ¨Åcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 1, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniÔ¨Åcant improvements in computational efÔ¨Åciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiÔ¨Åcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difÔ¨Åcult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the Ô¨Årst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The Ô¨Årst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 2, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 3, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by ‚àödk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n‚àödk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n‚àödk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efÔ¨Åcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n‚àödk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneÔ¨Åcial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the Ô¨Ånal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q ¬∑ k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 4, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n‚ààRdmodel√ódk, W K\\ni\\n‚ààRdmodel√ódk, W V\\ni\\n‚ààRdmodel√ódv\\nand W O ‚ààRhdv√ódmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n‚Ä¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation Ô¨Çow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by ‚àödmodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 5, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 ¬∑ d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n ¬∑ d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k ¬∑ n ¬∑ d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r ¬∑ n ¬∑ d)\\nO(1)\\nO(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and Ô¨Åxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2œÄ to 10000 ¬∑ 2œÄ. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any Ô¨Åxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ‚ààRd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 6, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k ¬∑ n ¬∑ d + n ¬∑ d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneÔ¨Åt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiÔ¨Åcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [17] with Œ≤1 = 0.9, Œ≤2 = 0.98 and œµ = 10‚àí9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d‚àí0.5\\nmodel ¬∑ min(step_num‚àí0.5, step_num ¬∑ warmup_steps‚àí1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the Ô¨Årst warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\nResidual Dropout\\nWe apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 7, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [15]\\n23.75\\nDeep-Att + PosUnk [32]\\n39.2\\n1.0 ¬∑ 1020\\nGNMT + RL [31]\\n24.6\\n39.92\\n2.3 ¬∑ 1019\\n1.4 ¬∑ 1020\\nConvS2S [8]\\n25.16\\n40.46\\n9.6 ¬∑ 1018\\n1.5 ¬∑ 1020\\nMoE [26]\\n26.03\\n40.56\\n2.0 ¬∑ 1019\\n1.2 ¬∑ 1020\\nDeep-Att + PosUnk Ensemble [32]\\n40.4\\n8.0 ¬∑ 1020\\nGNMT + RL Ensemble [31]\\n26.30\\n41.16\\n1.8 ¬∑ 1020\\n1.1 ¬∑ 1021\\nConvS2S Ensemble [8]\\n26.36\\n41.29\\n7.7 ¬∑ 1019\\n1.2 ¬∑ 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 ¬∑ 1018\\nTransformer (big)\\n28.4\\n41.0\\n2.3 ¬∑ 1019\\nLabel Smoothing\\nDuring training, we employed label smoothing of value œµls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conÔ¨Åguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty Œ± = 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of Ô¨Çoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision Ô¨Çoating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 8, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nœµls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n√ó106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneÔ¨Åcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-Ô¨Åtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the Ô¨Årst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiÔ¨Åcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efÔ¨Åciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 9, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770‚Äì778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber. Gradient Ô¨Çow in\\nrecurrent nets: the difÔ¨Åculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735‚Äì1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] ≈Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio ≈Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 10, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] OÔ¨År Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overÔ¨Åtting. Journal of Machine\\nLearning Research, 15(1):1929‚Äì1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440‚Äì2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104‚Äì3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 0, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract‚ÄîDue to object detection‚Äôs close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiÔ¨Åers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modiÔ¨Åcations and useful tricks\\nto improve detection performance further. As distinct speciÔ¨Åc\\ndetection tasks exhibit different characteristics, we also brieÔ¨Çy\\nsurvey several speciÔ¨Åc tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms‚Äîdeep learning, object detection, neural network\\nI. INTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiÔ¨Åcation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these Ô¨Åelds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]‚Äì[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it‚Äôs difÔ¨Å-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this Ô¨Åeld in recent years [15]‚Äì[18].\\nThe problem deÔ¨Ånition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classiÔ¨Åca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classiÔ¨Åcation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan Ô¨Ånd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a Ô¨Åxed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit‚Äôs difÔ¨Åcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassiÔ¨Åcation. Besides, a classiÔ¨Åer is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classiÔ¨Åers, the\\nDPM is a Ô¨Çexible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninefÔ¨Åcient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 1, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n2\\nPedestrian \\ndetection\\nSalient object \\ndetection \\nFace\\ndetection \\nGeneric object \\ndetection\\nObject \\ndetection\\nBounding box \\nregression\\nLocal contrast \\nSegmentation\\nMulti-feature\\nBoosting forest\\nMulti-scale\\nadaption\\nFig. 1. The application domains of object detection.\\nbridged by the combination of manually engineered low-level\\ndescriptors and discriminatively-trained shallow models.\\nThanks to the emergency of Deep Neural Networks (DNNs)\\n[6][S7], a more signiÔ¨Åcant gain is obtained with the introduc-\\ntion of Regions with CNN features (R-CNN) [15]. DNNs, or\\nthe most representative CNNs, act in a quite different way from\\ntraditional approaches. They have deeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [26].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including Fast R-CNN which\\njointly optimizes classiÔ¨Åcation and bounding box regression\\ntasks [16], Faster R-CNN which takes an additional sub-\\nnetwork to generate region proposals [18] and YOLO which\\naccomplishes object detection via a Ô¨Åxed-grid regression [17].\\nAll of them bring different degrees of detection performance\\nimprovements over the primary R-CNN and make real-time\\nand accurate object detection become more achievable.\\nIn this paper, a systematic review is provided to summarise\\nrepresentative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face\\ndetection [29]‚Äì[31] and pedestrian detection [32], [33]. Their\\nrelationships are depicted in Figure 1. Based on basic CNN ar-\\nchitectures, generic object detection is achieved with bounding\\nbox regression, while salient object detection is accomplished\\nwith local contrast enhancement and pixel-level segmentation.\\nFace detection and pedestrian detection are closely related\\nto generic object detection and mainly accomplished with\\nmulti-scale adaption and multi-feature fusion/boosting forest,\\nrespectively. The dotted lines indicate that the corresponding\\ndomains are associated with each other under certain con-\\nditions. It should be noticed that the covered domains are\\ndiversiÔ¨Åed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex\\nvariations in geometric structures and layouts. Therefore,\\ndifferent deep models are required by various images.\\nThere has been a relevant pioneer effort [34] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classiÔ¨Åcation and object detection, but\\npays little attention on detailing speciÔ¨Åc algorithms. Different\\nfrom it, our work not only reviews deep learning based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail, but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section 3. Then reviews\\nof CNN applied in several speciÔ¨Åc tasks, including salient\\nobject detection, face detection and pedestrian detection, are\\nexhibited in Section 4-6, respectively. Several promising future\\ndirections are proposed in Section 7. At last, some concluding\\nremarks are presented in Section 8.\\nII. A BRIEF OVERVIEW OF DEEP LEARNING\\nPrior to overview on deep learning based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. The History: Birth, Decline and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the\\nhuman brain system to solve general learning problems in a\\nprincipled way. It was popular in 1980s and 1990s with the\\nproposal of back-propagation algorithm by Hinton et al. [36].\\nHowever, due to the overÔ¨Åtting of training, lack of large scale\\ntraining data, limited computation power and insigniÔ¨Åcance\\nin performance compared with other machine learning tools,\\nneural networks fell out of fashion in early 2000s.\\nDeep learning has become popular since 2006 [37][S7] with\\na break through in speech recognition [38]. The recovery of\\ndeep learning can be attributed to the following factors.\\n‚Ä¢ The emergence of large scale annotated training data, such\\nas ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n‚Ä¢ Fast development of high performance parallel computing\\nsystems, such as GPU clusters;\\n‚Ä¢ SigniÔ¨Åcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npre-training guided by Auto-Encoder (AE) [40] or Re-\\nstricted Boltzmann Machine (RBM) [41], a good initializa-\\ntion is provided. With dropout and data augmentation, the\\noverÔ¨Åtting problem in training has been relieved [6], [42].\\nWith batch normalization (BN), the training of very deep\\nneural networks becomes quite efÔ¨Åcient [43]. Meanwhile,\\nvarious network structures, such as AlexNet [6], Overfeat\\n[44], GoogLeNet [45], VGG [46] and ResNet [47], have\\nbeen extensively studied to improve the performance.\\nWhat prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton‚Äôs group, whose continuous efforts have demonstrated\\nthat deep learning would bring a revolutionary breakthrough\\non grand challenges rather than just obvious improvements on\\nsmall datasets. Their success results from training a large CNN\\non 1.2 million labeled images together with a few techniques\\n[6] (e.g., ReLU operation [48] and ‚Äòdropout‚Äô regularization).\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [26].\\nA typical CNN architecture, which is referred to as VGG16,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 2, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n3\\ncan be found in Fig. S1. Each layer of CNN is known as a\\nfeature map. The feature map of the input layer is a 3D matrix\\nof pixel intensities for different color channels (e.g. RGB). The\\nfeature map of any internal layer is an induced multi-channel\\nimage, whose ‚Äòpixel‚Äô can be viewed as a speciÔ¨Åc feature. Every\\nneuron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive Ô¨Åeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as Ô¨Åltering and pooling. Filtering (convolution)\\noperation convolutes a Ô¨Ålter matrix (learned weights) with\\nthe values of a receptive Ô¨Åeld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain Ô¨Ånal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],\\nsummaries the responses of a receptive Ô¨Åeld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an\\ninitial feature hierarchy is constructed, which can be Ô¨Åne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the Ô¨Ånal layer with different activation functions [6]\\nis added to get a speciÔ¨Åc conditional probability for each\\noutput neuron. And the whole network can be optimized on\\nan objective function (e.g. mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax\\nclassiÔ¨Åcation layer. The conv feature maps are produced by\\nconvoluting 3*3 Ô¨Ålter windows, and feature map resolutions\\nare reduced with 2 stride max-pooling layers. An arbitrary test\\nimage of the same size as training samples can be processed\\nwith the trained network. Re-scaling or cropping operations\\nmay be needed if different sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarised as follows.\\n‚Ä¢ Hierarchical feature representation, which is the multi-\\nlevel representations from pixel to high-level semantic fea-\\ntures learned by a hierarchical multi-stage structure [15],\\n[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n‚Ä¢ Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expressive\\ncapability.\\n‚Ä¢ The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g. Fast R-\\nCNN combines classiÔ¨Åcation and bounding box regression\\ninto a multi-task leaning manner).\\n‚Ä¢ BeneÔ¨Åtting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can be\\nrecast as high-dimensional data transform problems and\\nsolved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research Ô¨Åelds, such as image super-resolution\\nreconstruction [54], [55], image classiÔ¨Åcation [5], [56], im-\\nage retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]‚Äì[61] and video analysis [62], [63].\\nIII. GENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image, and labeling them with\\nrectangular bounding boxes to show the conÔ¨Ådences of exis-\\ntence. The frameworks of generic object detection methods\\ncan mainly be categorized into two types (see Figure 2).\\nOne follows traditional object detection pipeline, generating\\nregion proposals at Ô¨Årst and then classifying each proposal into\\ndifferent object categories. The other regards object detection\\nas a regression or classiÔ¨Åcation problem, adopting a uniÔ¨Åed\\nframework to achieve Ô¨Ånal results (categories and locations)\\ndirectly. The region proposal based methods mainly include\\nR-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiÔ¨Åes R-\\nCNN with a SPP layer). The regression/classiÔ¨Åcation based\\nmethods mainly includes MultiBox [68], AttentionNet [69],\\nG-CNN [70], YOLO [17], SSD [71], YOLOv2 [72], DSSD\\n[73] and DSOD [74]. The correlations between these two\\npipelines are bridged by the anchors introduced in Faster R-\\nCNN. Details of these methods are as follows.\\nA. Region Proposal Based Framework\\nThe region proposal based framework, a two-step process,\\nmatches the attentional mechanism of human brain to some\\nextent, which gives a coarse scan of the whole scenario Ô¨Årstly\\nand then focuses on regions of interest. Among the pre-related\\nworks [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of\\nthe topmost feature map after obtaining the conÔ¨Ådences of\\nunderlying object categories.\\n1) R-CNN: It is of signiÔ¨Åcance to improve the quality of\\ncandidate bounding boxes and to take a deep architecture to\\nextract high-level features. To solve these problems, R-CNN\\n[15] was proposed by Ross Girshick in 2014 and obtained a\\nmean average precision (mAP) of 53.3% with more than 30%\\nimprovement over the previous best result (DPM HSC [77]) on\\nPASCAL VOC 2012. Figure 3 shows the Ô¨Çowchart of R-CNN,\\nwhich can be divided into three stages as follows.\\nRegion proposal generation. The R-CNN adopts selective\\nsearch [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate\\nboxes of arbitrary sizes quickly and to reduce the searching\\nspace in object detection [24], [39].\\nCNN based deep feature extraction. In this stage, each\\nregion proposal is warped or cropped into a Ô¨Åxed resolution\\nand the CNN module in [6] is utilized to extract a 4096-\\ndimensional feature as the Ô¨Ånal representation. Due to large\\nlearning capacity, dominant expressive power and hierarchical\\nstructure of CNNs, a high-level, semantic and robust feature\\nrepresentation for each region proposal can be obtained.\\nClassiÔ¨Åcation and localization. With pre-trained category-\\nspeciÔ¨Åc linear SVMs for multiple classes, different region pro-\\nposals are scored on a set of positive regions and background\\n(negative) regions. The scored regions are then adjusted with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 3, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n4\\nGeneric object \\ndetection\\nRegion proposal \\nbased\\nRegression/\\nClassification \\nbased \\nR-CNN\\n(2014)\\nSPP-net\\n(2015)\\nFRCN\\n(2015)\\nFaster \\nR-CNN\\n(2015)\\nR-FCN\\n(2016)\\nFPN\\n(2017)\\nMask R-CNN\\n(2017)\\nMultiBox\\n(2014)\\nAttentionNet\\n(2015)\\nG-CNN\\n(2016)\\nYOLO\\n(2016)\\nSSD\\n(2016)\\nYOLOv2\\n(2017)\\nSPP \\nlayer\\nMulti-\\ntask\\nRPN\\nFCN\\nFeature\\npyramid\\nInstance\\nSegmentation\\nRegion\\nproposal\\nUnified\\nloss\\nDirection\\niteration\\nJoint Grid\\nregression\\nRPN\\nBN\\nMulti-scale\\nGrid\\nregression\\nDSSD\\n(2017)\\nDSOD\\n(2017)\\nStem block\\nDense block\\nResNet101 \\nDeconv layers\\nFig. 2. Two types of frameworks: region proposal based and regression/classiÔ¨Åcation based. SPP: Spatial Pyramid Pooling [64], FRCN: Faster R-CNN [16],\\nRPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54]\\n.\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n...\\nCNN\\nR-CNN: Regions with CNN features\\nFig. 3. The Ô¨Çowchart of R-CNN [15], which consists of 3 stages: (1) extracts\\nbottom-up region proposals, (2) computes features for each proposal using a\\nCNN, and then (3) classiÔ¨Åes each region with class-speciÔ¨Åc linear SVMs.\\nbounding box regression and Ô¨Åltered with a greedy non-\\nmaximum suppression (NMS) to produce Ô¨Ånal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufÔ¨Åcient labeled data, pre-\\ntraining is usually conducted. Instead of unsupervised pre-\\ntraining [79], R-CNN Ô¨Årstly conducts supervised pre-training\\non ILSVRC, a very large auxiliary dataset, and then takes a\\ndomain-speciÔ¨Åc Ô¨Åne-tuning. This scheme has been adopted by\\nmost of subsequent approaches [16], [18].\\nIn spite of its improvements over traditional methods and\\nsigniÔ¨Åcance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n‚Ä¢ Due to the existence of FC layers, the CNN requires a\\nÔ¨Åxed-size (e.g., 227√ó227) input image, which directly leads\\nto the re-computation of the whole CNN for each evaluated\\nregion, taking a great deal of time in the testing period.\\n‚Ä¢ Training of R-CNN is a multi-stage pipeline. At Ô¨Årst,\\na convolutional network (ConvNet) on object proposals is\\nÔ¨Åne-tuned. Then the softmax classiÔ¨Åer learned by Ô¨Åne-\\ntuning is replaced by SVMs to Ô¨Åt in with ConvNet features.\\nFinally, bounding-box regressors are trained.\\n‚Ä¢ Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on the\\ndisk. It will take a long time to process a relatively small\\ntraining set with very deep networks, such as VGG16. At the\\nsame time, the storage memory required by these features\\nshould also be a matter of concern.\\n‚Ä¢ Although selective search can generate region proposals\\nwith relatively high recalls, the obtained region proposals\\nare still redundant and this procedure is time-consuming\\n(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-\\ntation to replace traditional graph cuts. MCG [81] searches\\ndifferent scales of the image for multiple hierarchical segmen-\\ntations and combinatorially groups different regions to produce\\nproposals. Instead of extracting visually distinct segments,\\nthe edge boxes method [82] adopts the idea that objects are\\nmore likely to exist in bounding boxes with fewer contours\\nstraggling their boundaries. Also some researches tried to\\nre-rank or reÔ¨Åne pre-extracted region proposals to remove\\nunnecessary ones and obtained a limited number of valuable\\nones, such as DeepBox [83] and SharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and\\ntrained class-speciÔ¨Åc CNN classiÔ¨Åers with a structured loss\\nto penalize the localization inaccuracy explicitly. Saurabh\\nGupta et al. improved object detection for RGB-D images\\nwith semantically rich image and depth features [86], and\\nlearned a new geocentric embedding for depth images to\\nencode each pixel. The combination of object detectors and\\nsuperpixel classiÔ¨Åcation framework gains a promising result\\non semantic scene segmentation task. Ouyang et al. proposed\\na deformable deep CNN (DeepID-Net) [87] which introduces\\na novel deformation constrained pooling (def-pooling) layer\\nto impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role\\nof proposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth bounding boxes with K-means\\nclustering. However, more candidate boxes are required to\\nachieve comparable results to those of R-CNN.\\n2) SPP-net: FC layers must take a Ô¨Åxed-size input. That‚Äôs\\nwhy R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial\\npyramid matching (SPM) [89], [90] into consideration and\\nproposed a novel CNN architecture named SPP-net [64]. SPM\\ntakes several Ô¨Åner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Figure 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the 5-th conv layer (conv5) to project region'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 4, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n5\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n‚Ä¶...\\nfully-connected layers (fc6, fc7)\\nFig. 4. The architecture of SPP-net for object detection [64].\\nDeep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC\\nFC\\nFor each RoI\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to Ô¨Åxed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses, but also have relationships with their spatial\\npositions [64]. The layer after the Ô¨Ånal conv layer is referred\\nto as spatial pyramid pooling layer (SPP layer). If the number\\nof feature maps in conv5 is 256, taking a 3-level pyramid,\\nthe Ô¨Ånal feature vector for each region proposal obtained after\\nSPP layer has a dimension of 256 √ó (12 + 22 + 42) = 5376.\\nSPP-net not only gains better results with correct estimation\\nof different region proposals in their corresponding scales, but\\nalso improves detection efÔ¨Åciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and efÔ¨Åciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost\\nthe same multi-stage pipeline as R-CNN, including feature\\nextraction, network Ô¨Åne-tuning, SVM training and bounding-\\nbox regressor Ô¨Åtting. So an additional expense on storage space\\nis still required. Additionally, the conv layers preceding the\\nSPP layer cannot be updated with the Ô¨Åne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multi-task loss on classiÔ¨Åcation and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a Ô¨Åxed-length feature\\nvector is extracted from each region proposal with a region of\\ninterest (RoI) pooling layer. The RoI pooling layer is a special\\ncase of the SPP layer, which has only one pyramid level. Each\\nfeature vector is then fed into a sequence of FC layers before\\nÔ¨Ånally branching into two sibling output layers. One output\\nlayer is responsible for producing softmax probabilities for\\nall C + 1 categories (C object classes plus one ‚Äòbackground‚Äô\\nclass) and the other output layer encodes reÔ¨Åned bounding-\\nbox positions with four real-valued numbers. All parameters\\nin these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deÔ¨Åned as below to jointly train\\nclassiÔ¨Åcation and bounding-box regression,\\nL(p, u, tu, v) = Lcls(p, u) + Œª[u ‚â•1]Lloc(tu, v)\\n(1)\\nwhere Lcls(p, u) = ‚àílog pu calculates the log loss for ground\\ntruth class u and pu is driven from the discrete probability\\ndistribution p = (p0, ¬∑ ¬∑ ¬∑ , pC) over the C +1 outputs from the\\nlast FC layer. Lloc(tu, v) is deÔ¨Åned over the predicted offsets\\ntu = (tu\\nx, tu\\ny, tu\\nw, tu\\nh) and ground-truth bounding-box regression\\ntargets v = (vx, vy, vw, vh), where x, y, w, h denote the two\\ncoordinates of the box center, width, and height, respectively.\\nEach tu adopts the parameter settings in [15] to specify an\\nobject proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u ‚â•1] is employed to omit all background RoIs. To provide\\nmore robustness against outliers and eliminate the sensitivity\\nin exploding gradients, a smooth L1 loss is adopted to Ô¨Åt\\nbounding-box regressors as below\\nLloc(tu, v) =\\nX\\ni‚ààx,y,w,h\\nsmoothL1(tu\\ni ‚àívi)\\n(2)\\nwhere\\nsmoothL1(x) =\\n(\\n0.5x2\\nif |x| < 1\\n|x| ‚àí0.5\\notherwise\\n(3)\\nTo accelerate the pipeline of Fast R-CNN, another two tricks\\nare of necessity. On one hand, if training samples (i.e. RoIs)\\ncome from different images, back-propagation through the\\nSPP layer becomes highly inefÔ¨Åcient. Fast R-CNN samples\\nmini-batches hierarchically, namely N images sampled ran-\\ndomly at Ô¨Årst and then R/N RoIs sampled in each image,\\nwhere R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time\\nis spent in computing the FC layers during the forward pass\\n[16]. The truncated Singular Value Decomposition (SVD) [91]\\ncan be utilized to compress large FC layers and to accelerate\\nthe testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single-stage with a multi-task loss. It saves the additional\\nexpense on storage space, and improves both accuracy and\\nefÔ¨Åciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also\\na bottleneck in improving efÔ¨Åciency. To solve this problem,\\nRen et al. introduced an additional Region Proposal Network\\n(RPN) [18], [92], which acts in a nearly cost-free way by\\nsharing full-image conv features with detection network.\\nRPN is achieved with a fully-convolutional network, which\\nhas the ability to predict object bounds and scores at each\\nposition simultaneously. Similar to [78], RPN takes an image\\nof arbitrary size to generate a set of rectangular object propos-\\nals. RPN operates on a speciÔ¨Åc conv layer with the preceding\\nlayers shared with object detection network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 5, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n6\\nconv feature map\\nintermediate layer\\n256-d\\n2k scores\\n4k coordinates\\nsliding window\\nreg layer\\ncls layer\\nk anchor boxes\\nFig. 6.\\nThe RPN in Faster R-CNN [18]. K predeÔ¨Åned anchor boxes are\\nconvoluted with each sliding window to produce Ô¨Åxed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nThe architecture of RPN is shown in Figure 6. The network\\nslides over the conv feature map and fully connects to an\\nn √ó n spatial window. A low dimensional vector (512-d for\\nVGG16) is obtained in each sliding window and fed into two\\nsibling FC layers, namely box-classiÔ¨Åcation layer (cls) and\\nbox-regression layer (reg). This architecture is implemented\\nwith an n √ó n conv layer followed by two sibling 1 √ó 1 conv\\nlayers. To increase non-linearity, ReLU is applied to the output\\nof the n √ó n conv layer.\\nThe regressions towards true bounding boxes are achieved\\nby comparing proposals relative to reference boxes (anchors).\\nIn the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi, ti) =\\n1\\nNcls\\nX\\ni\\nLcls(pi, p‚àó\\ni ) + Œª\\n1\\nNreg\\nX\\ni\\np‚àó\\ni Lreg(ti, t‚àó\\ni )\\n(4)\\nwhere pi shows the predicted probability of the i-th anchor\\nbeing an object. The ground truth label p‚àó\\ni is 1 if the anchor is\\npositive, otherwise 0. ti stores 4 parameterized coordinates of\\nthe predicted bounding box while t‚àó\\ni is related to the ground-\\ntruth box overlapping with a positive anchor. Lcls is a binary\\nlog loss and Lreg is a smoothed L1 loss similar to (2). These\\ntwo terms are normalized with the mini-batch size (Ncls)\\nand the number of anchor locations (Nreg), respectively. In\\nthe form of fully-convolutional networks, Faster R-CNN can\\nbe trained end-to-end by back-propagation and SGD in an\\nalternate training manner.\\nWith the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame\\nPer Second) on a GPU is achieved with state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces object-like regions (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a prevalent\\nfamily [16], [18] of deep networks for object detection are\\ncomposed of two subnetworks: a shared fully convolutional\\nsubnetwork (independent of RoIs) and an unshared RoI-wise\\nsubnetwork. This decomposition originates from pioneering\\nclassiÔ¨Åcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciÔ¨Åc spatial pooling layer.\\nRecent state-of-the-art image classiÔ¨Åcation networks, such\\nas Residual Nets (ResNets) [47] and GoogLeNets [45], [93],\\nare fully convolutional. To adapt to these architectures, it‚Äôs\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict\\npredict\\n(b) Single feature map\\npredict\\n(d) Feature Pyramid Network\\npredict\\npredict\\npredict\\n(c) Pyramidal feature hierarchy\\npredict\\npredict\\npredict\\nFig. 7. The main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single scale features is adopted for faster\\ndetection. (c) An alternative to the featurized image pyramid is to reuse the\\npyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both\\n(b) and (c). Blue outlines indicate feature maps and thicker outlines denote\\nsemantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be\\ninferior with such a naive solution [47]. This inconsistence is\\ndue to the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classiÔ¨Åcation. In other words, shifting an object inside\\nan image should be indiscriminative in image classiÔ¨Åcation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregion-wise layers. So Li et al. [65] proposed a region-based\\nfully convolutional networks (R-FCN, Fig. S2).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a Ô¨Åxed grid of k √ó k Ô¨Årstly and a position-\\nsensitive RoI pooling layer is then appended to aggregate the\\nresponses from these score maps. Finally, in each RoI, k2\\nposition-sensitive scores are averaged to produce a C + 1-d\\nvector and softmax responses across categories are computed.\\nAnother 4k2-d conv layer is appended to obtain class-agnostic\\nbounding boxes.\\nWith R-FCN, more powerful classiÔ¨Åcation networks can be\\nadopted to accomplish object detection in a fully-convolutional\\narchitecture by sharing nearly all the layers, and state-of-the-\\nart results are obtained on both PASCAL VOC and Microsoft\\nCOCO [94] datasets at a test speed of 170ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance\\n[24], [64] (Figure 7(a)). However, training time and memory\\nconsumption increase rapidly. To this end, some techniques\\ntake only a single input scale to represent high-level semantics\\nand increase the robustness to scale changes (Figure 7(b)),\\nand image pyramids are built at test time which results in\\nan inconsistency between train/test-time inferences [16], [18].\\nThe in-network feature hierarchy in a deep ConvNet produces\\nfeature maps of different spatial resolutions while introduces\\nlarge semantic gaps caused by different depths (Figure 7(c)).\\nTo avoid using low-level features, pioneer works [71], [95]\\nusually build the pyramid starting from middle layers or\\njust sum transformed feature responses, missing the higher-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 6, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n7\\nFig. 8. The Mask R-CNN framework for instance segmentation [67].\\nresolution maps of the feature hierarchy.\\nDifferent from these approaches, FPN [66] holds an ar-\\nchitecture with a bottom-up pathway, a top-down pathway\\nand several lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and seman-\\ntically weak features (Figure 7(d)). The bottom-up pathway,\\nwhich is the basic forward backbone ConvNet, produces a\\nfeature hierarchy by downsampling the corresponding feature\\nmaps with a stride of 2. The layers owning the same size of\\noutput maps are grouped into the same network stage and the\\noutput of the last layer of each stage is chosen as the reference\\nset of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at Ô¨Årst and then enhanced with\\nthose of the same spatial size from the bottom-up pathway\\nvia lateral connections. A 1 √ó 1 conv layer is appended to\\nthe upsampled map to reduce channel dimensions and the\\nmergence is achieved by element-wise addition. Finally, a 3√ó3\\nconvolution is also appended to each merged map to reduce\\nthe aliasing effect of upsampling and the Ô¨Ånal feature map is\\ngenerated. This process is iterated until the Ô¨Ånest resolution\\nmap is generated.\\nAs feature pyramid can extract rich semantics from all\\nlevels and be trained end-to-end with all scales, state-of-the-\\nart representation can be obtained without sacriÔ¨Åcing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many\\nother computer vision tasks (e.g. instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nAnd the multi-task scheme will create spurious edge and\\nexhibit systematic errors on overlapping instances [98]. To\\nsolve this problem, parallel to the existing branches in Faster\\nR-CNN for classiÔ¨Åcation and bounding box regression, the\\nMask R-CNN [67] adds a branch to predict segmentation\\nmasks in a pixel-to-pixel manner (Figure 8).\\nDifferent from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m √ó m mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nrepresentation requires fewer parameters but is more accurate\\nthan that of [97]. Formally, besides the two losses in (1) for\\nclassiÔ¨Åcation and bounding box regression, an additional loss\\nfor segmentation mask branch is deÔ¨Åned to reach a multi-task\\nloss. An this loss is only associated with ground-truth class\\nand relies on the classiÔ¨Åcation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classiÔ¨Åcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch only\\nadds a small computational burden and its cooperation with\\nother tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection\\nresults. In a word, Mask R-CNN is a Ô¨Çexible and efÔ¨Åcient\\nframework for instance-level recognition, which can be easily\\ngeneralized to other tasks (e.g. human pose estimation [7][S4])\\nwith minimal modiÔ¨Åcation.\\n8) Multi-task Learning, Multi-scale Representation and\\nContextual Modelling:\\nAlthough the Faster R-CNN gets\\npromising results with several hundred proposals, it still strug-\\ngles in small-size object detection and localization, mainly due\\nto the coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO dataset which consists\\nof objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmulti-task learning [100], multi-scale representation [95] and\\ncontext modelling [101] to combine complementary informa-\\ntion from multiple sources.\\nMulti-task Learning learns a useful representation for\\nmultiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. introduced conv features trained for ob-\\nject segmentation and ‚Äòstuff‚Äô (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet) [100]. Dai et al. [97] presented Multitask\\nNetwork Cascades of three networks, namely class-agnostic\\nregion proposal generation, pixel-level instance segmentation\\nand regional instance classiÔ¨Åcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based\\nobject detection into a multi-stage architecture to fully exploit\\nthe learned segmentation features [104].\\nMulti-scale Representation combines activations from\\nmultiple layers with skip-layer connections to provide seman-\\ntic information of different spatial resolutions [66]. Cai et\\nal. proposed the MS-CNN [105] to ease the inconsistency\\nbetween the sizes of objects and receptive Ô¨Åelds with multiple\\nscale-independent output layers. Yang et al. investigated two\\nstrategies, namely scale-dependent pooling (SDP) and layer-\\nwise cascaded rejection classiÔ¨Åers (CRC), to exploit appropri-\\nate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps from different resolutions into a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 7, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n8\\nuniform space [101].\\nContextual Modelling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. proposed the SegDeepM to exploit\\nobject segmentation which reduces the dependency on initial\\ncandidate boxes with Markov Random Field [106]. Moysset\\net al. took advantage of 4 directional 2D-LSTMs [107] to\\nconvey global context between different local regions and re-\\nduced trainable parameters with local parameter-sharing [108].\\nZeng et al. proposed a novel GBD-Net by introducing gated\\nfunctions to control message transmission between different\\nsupport regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)\\nmodel [110] to capture different aspects of an object, the\\ndistinct appearances of various object parts and semantic\\nsegmentation-aware features. To obtain contextual and multi-\\nscale representations, Bell et al. proposed the Inside-Outside\\nNet (ION) by exploiting information both inside and outside\\nthe RoI [95] with spatial recurrent neural networks [111] and\\nskip pooling [101]. Zagoruyko et al. proposed the MultiPath\\narchitecture by introducing three modiÔ¨Åcations to the Fast\\nR-CNN [112], including multi-scale skip connections [95],\\na modiÔ¨Åed foveal structure [110] and a novel loss function\\nsumming different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. proposed an effective online mining algo-\\nrithm (OHEM) [113] for automatic selection of the hard ex-\\namples, which leads to a more effective and efÔ¨Åcient training.\\nInstead of concentrating on feature extraction, Ren et al.\\nmade a detailed analysis on object classiÔ¨Åers [114], and\\nfound that it is of particular importance for object detection\\nto construct a deep and convolutional per-region classiÔ¨Åer\\ncarefully, especially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signiÔ¨Åcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-\\nvolved. To address this problem, Xiang et al. proposed a\\nnovel subcategory-aware region proposal network [60], which\\nguides the generation of region proposals with subcategory\\ninformation related to object poses and jointly optimize object\\ndetection and subcategory classiÔ¨Åcation.\\nOuyang et al. found that the samples from different classes\\nfollow a longtailed distribution [115], which indicates that dif-\\nferent classes with distinct numbers of samples have different\\ndegrees of impacts on feature learning. To this end, objects are\\nÔ¨Årstly clustered into visually similar class groups, and then a\\nhierarchical feature learning scheme is adopted to learn deep\\nrepresentations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‚Äòdeep and thin‚Äô design\\nprinciple and following the pipeline of Fast R-CNN, Hong et\\nal. proposed the architecture of PVANET [116], which adopts\\nsome building blocks including concatenated ReLU [117],\\nInception [45], and HyperNet [101] to reduce the expense on\\nmulti-scale feature extraction and trains the network with batch\\nnormalization [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PVANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 FPS).\\nB. Regression/ClassiÔ¨Åcation Based Framework\\nRegion proposal based frameworks are composed of sev-\\neral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiÔ¨Åcation and bounding box\\nregression, which are usually trained separately. Even in recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different components becomes the bottleneck in real-\\ntime application.\\nOne-step\\nframeworks\\nbased\\non\\nglobal\\nregres-\\nsion/classiÔ¨Åcation, mapping straightly from image pixels\\nto bounding box coordinates and class probabilities, can\\nreduce time expense. We Ô¨Årstly reviews some pioneer CNN\\nmodels, and then focus on two signiÔ¨Åcant frameworks,\\nnamely You only look once (YOLO) [17] and Single Shot\\nMultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiÔ¨Åcation task.\\nSzegedy et al. formulated object detection task as a DNN-\\nbased regression [118], generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has difÔ¨Åculty in handling\\noverlapping objects, and bounding boxes generated by direct\\nupsampling is far from perfect.\\nPinheiro et al. proposed a CNN model with two branches:\\none generates class agnostic segmentation masks and the\\nother predicts the likelihood of a given patch centered on\\nan object [119]. Inference is efÔ¨Åcient since class scores and\\nsegmentation can be obtained in a single model with most of\\nthe CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniÔ¨Åed\\nloss was introduced to bias both localization and conÔ¨Ådences\\nof multiple components to predict the coordinates of class-\\nagnostic bounding boxes. However, a large quantity of addi-\\ntional parameters are introduced to the Ô¨Ånal layer.\\nYoo et al. adopted an iterative classiÔ¨Åcation approach to\\nhandle object detection and proposed an impressive end-to-\\nend CNN architecture named AttentionNet [69]. Starting from\\nthe top-left (TL) and bottom-right (BR) corner of an image,\\nAttentionNet points to a target object by generating quantized\\nweak directions and converges to an accurate object bound-\\nary box with an ensemble of iterative predictions. However,\\nthe model becomes quite inefÔ¨Åcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based\\nobject detector (G-CNN), which models object detection as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 8, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n9\\nFig. 9. Main idea of YOLO [17].\\nÔ¨Ånding a path from a Ô¨Åxed grid to boxes tightly surrounding\\nthe objects [70]. Starting with a Ô¨Åxed multi-scale bounding box\\ngrid, G-CNN trains a regressor to move and scale elements of\\nthe grid towards objects iteratively. However, G-CNN has a\\ndifÔ¨Åculty in dealing with small or highly overlapping objects.\\n2) YOLO: Redmon et al. [17] proposed a novel framework\\ncalled YOLO, which makes use of the whole topmost feature\\nmap to predict both conÔ¨Ådences for multiple categories and\\nbounding boxes. The basic idea of YOLO is exhibited in\\nFigure 9. YOLO divides the input image into an S √ó S grid and\\neach grid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conÔ¨Ådence scores. Formally, conÔ¨Å-\\ndence scores are deÔ¨Åned as Pr(Object) ‚àóIOU truth\\npred , which\\nindicates how likely there exist objects (Pr(Object) ‚â•0) and\\nshows conÔ¨Ådences of its prediction (IOU truth\\npred ). At the same\\ntime, regardless of the number of boxes, C conditional class\\nprobabilities (Pr(Classi|Object)) should also be predicted in\\neach grid cell. It should be noticed that only the contribution\\nfrom the grid cell containing an object is calculated.\\nAt test time, class-speciÔ¨Åc conÔ¨Ådence scores for each box\\nare achieved by multiplying the individual box conÔ¨Ådence\\npredictions with the conditional class probabilities as follows.\\nPr(Object) ‚àóIOU truth\\npred ‚àóPr(Classi|Object)\\n= Pr(Classi) ‚àóIOU truth\\npred\\n(5)\\nwhere the existing probability of class-speciÔ¨Åc objects in the\\nbox and the Ô¨Åtness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized,\\nŒªcoord\\nS2\\nX\\ni=0\\nB\\nX\\nj=0\\n1obj\\nij\\n\\x02\\n(xi ‚àíÀÜxi)2 + (yi ‚àíÀÜyi)2\\x03\\n+Œªcoord\\nS2\\nX\\ni=0\\nB\\nX\\nj=0\\n1obj\\nij\\n\"\\x12‚àöwi ‚àí\\np\\nÀÜwi)2 + (\\np\\nhi ‚àí\\nq\\nÀÜhi\\n\\x132#\\n+\\nS2\\nX\\ni=0\\nB\\nX\\nj=0\\n1obj\\nij\\n\\x10\\nCi ‚àíÀÜCi\\n\\x112\\n+Œªnoobj\\nS2\\nX\\ni=0\\nB\\nX\\nj=0\\n1noobj\\nij\\n\\x10\\nCi ‚àíÀÜCi\\n\\x112\\n+\\nS2\\nX\\ni=0\\n1obj\\ni\\nX\\nc‚ààclasses\\n(pi(c) ‚àíÀÜpi(c))2\\n(6)\\nIn a certain cell i, (xi, yi) denote the center of the box relative\\nto the bounds of the grid cell, (wi, hi) are the normalized width\\nand height relative to the image size, Ci represents conÔ¨Ådence\\nscores, 1obj\\ni\\nindicates the existence of objects and 1obj\\nij\\ndenotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiÔ¨Åcation errors. Similarly,\\nwhen the predictor is ‚Äòresponsible‚Äô for the ground truth box\\n(i.e. the highest IoU of any predictor in that grid cell is\\nachieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 √ó 1 reduction layers followed by 3 √ó 3 conv\\nlayers. The network can process images in real-time at 45\\nFPS and a simpliÔ¨Åed version Fast YOLO can reach 155 FPS\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on background, which\\nmakes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,\\ndimension cluster and multi-scale training.\\n3) SSD: YOLO has a difÔ¨Åculty in dealing with small\\nobjects in groups, which is caused by strong spatial constraints\\nimposed on bounding box predictions [17]. Meanwhile, YOLO\\nstruggles to generalize to objects in new/unusual aspect ratios/\\nconÔ¨Ågurations and produces relatively coarse features due to\\nmultiple downsampling operations.\\nAiming at these problems, Liu et al. proposed a Single Shot\\nMultiBox Detector (SSD) [71], which was inspired by the\\nanchors adopted in MultiBox [68], RPN [18] and multi-scale\\nrepresentation [95]. Given a speciÔ¨Åc feature map, instead of\\nÔ¨Åxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle\\nobjects with various sizes, the network fuses predictions from\\nmultiple feature maps with different resolutions .\\nThe architecture of SSD is demonstrated in Figure 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales and\\naspect ratios and their associated conÔ¨Ådences. The network is\\ntrained with a weighted sum of localization loss (e.g. Smooth\\nL1) and conÔ¨Ådence loss (e.g. Softmax), which is similar to\\n(1). Final detection results are obtained by conducting NMS\\non multi-scale reÔ¨Åned bounding boxes.\\nIntegrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiÔ¨Åcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO, while being three\\ntimes faster. The SSD300 (input image size is 300√ó300) runs\\nat 59 FPS, which is more accurate and efÔ¨Åcient than YOLO.\\nHowever, SSD is not skilled at dealing with small objects,\\nwhich can be relieved by adopting better feature extractor\\nbackbone (e.g. ResNet101), adding deconvolution layers with\\nskip connections to introduce additional large-scale context\\n[73] and designing better network structure (e.g. Stem Block\\nand Dense Block) [74].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 9, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n10\\nFig. 10. The architecture of SSD 300 [71]. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor\\nboxes and their associated conÔ¨Ådences. Final detection results are obtained by conducting NMS on multi-scale reÔ¨Åned bounding boxes.\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark datasets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121] and Microsoft COCO [94]. The\\nevaluated approaches include R-CNN [15], SPP-net [64], Fast\\nR-CNN [16], NOC [114], Bayes [85], MR-CNN&S-CNN\\n[105], Faster R-CNN [18], HyperNet [101], ION [95], MS-\\nGR [104], StuffNet [100], SSD300 [71], SSD512 [71], OHEM\\n[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PVANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD\\n[74]. If no speciÔ¨Åc instructions for the adopted framework\\nare provided, the utilized model is a VGG16 [46] pretrained\\non 1000-way ImageNet classiÔ¨Åcation task [39]. Due to the\\nlimitation of paper length, we only provide an overview, in-\\ncluding proposal, learning method, loss function, programming\\nlanguage and platform, of the prominent architectures in Table\\nI. Detailed experimental settings, which can be found in the\\noriginal papers, are missed. In addition to the comparisons of\\ndetection accuracy, another comparison is provided to evaluate\\ntheir test consumption on PASCAL VOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-\\nparative results are exhibited in Table II and III, from which\\nthe following remarks can be obtained.\\n‚Ä¢ If incorporated with a proper way, more powerful back-\\nbone CNN models can deÔ¨Ånitely improve object detection\\nperformance (the comparison among R-CNN with AlexNet,\\nR-CNN with VGG16 and SPP-net with ZF-Net [122]).\\n‚Ä¢ With the introduction of SPP layer (SPP-net), end-to-\\nend multi-task architecture (FRCN) and RPN (Faster R-\\nCNN), object detection performance is improved gradually\\nand apparently.\\n‚Ä¢ Due to large quantities of trainable parameters, in order to\\nobtain multi-level robust features, data augmentation is very\\nimportant for deep learning based models (Faster R-CNN\\nwith ‚Äò07‚Äô ,‚Äò07+12‚Äô and ‚Äò07+12+coco‚Äô).\\n‚Ä¢ Apart from basic models, there are still many other factors\\naffecting object detection performance, such as multi-scale\\nand multi-region feature extraction (e.g. MR-CNN), modi-\\nÔ¨Åed classiÔ¨Åcation networks (e.g. NOC), additional informa-\\ntion from other correlated tasks (e.g. StuffNet, HyperNet),\\nmulti-scale representation (e.g. ION) and mining of hard\\nnegative samples (e.g. OHEM).\\n‚Ä¢ As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN and Ô¨Åne grained features, the\\nlocalization errors are corrected (YOLOv2).\\n‚Ä¢ By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.\\n2) Microsoft COCO: Microsoft COCO is composed of\\n300,000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requirement on object localization,\\nthis dataset is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are shown in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n‚Ä¢ Multi-scale training and test are beneÔ¨Åcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and\\nDSSD provide some better ways to build feature pyramids\\nto achieve multi-scale representation. The complementary\\ninformation from other related tasks is also helpful for\\naccurate object localization (Mask R-CNN with instance\\nsegmentation task).\\n‚Ä¢\\nOverall,\\nregion\\nproposal\\nbased\\nmethods,\\nsuch\\nas\\nFaster R-CNN and R-FCN, perform better than regres-\\nsion/classÔ¨Åcation based approaches, namely YOLO and\\nSSD, due to the fact that quite a lot of localization errors\\nare produced by regression/classÔ¨Åcation based approaches.\\n‚Ä¢ Context modelling is helpful to locate small objects,\\nwhich provides additional information by consulting nearby\\nobjects and surroundings (GBD-Net and multi-path).\\n‚Ä¢ Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g. ResNeXt [123]) and useful\\nstrategies (e.g. multi-task learning [67], [124]), the perfor-\\nmance can be improved.\\n‚Ä¢ The success of DSOD in training from scratch stresses the\\nimportance of network design to release the requirements\\nfor perfect pre-trained classiÔ¨Åers on relevant tasks and large\\nnumbers of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA Titan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 10, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n11\\nTABLE I\\nAN OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES.\\nFramework\\nProposal\\nMulti-scale Input\\nLearning Method\\nLoss Function\\nSoftmax Layer\\nEnd-to-end Train\\nPlatform\\nLanguage\\nR-CNN [15]\\nSelective Search\\n-\\nSGD,BP\\nHinge loss (classiÔ¨Åcation),Bounding box regression\\n+\\n-\\nCaffe\\nMatlab\\nSPP-net [64]\\nEdgeBoxes\\n+\\nSGD\\nHinge loss (classiÔ¨Åcation),Bounding box regression\\n+\\n-\\nCaffe\\nMatlab\\nFast RCNN [16]\\nSelective Search\\n+\\nSGD\\nClass Log loss+bounding box regression\\n+\\n-\\nCaffe\\nPython\\nFaster R-CNN [18]\\nRPN\\n+\\nSGD\\nClass Log loss+bounding box regression\\n+\\n+\\nCaffe\\nPython/Matlab\\nR-FCN [65]\\nRPN\\n+\\nSGD\\nClass Log loss+bounding box regression\\n-\\n+\\nCaffe\\nMatlab\\nMask R-CNN [67]\\nRPN\\n+\\nSGD\\nClass Log loss+bounding box regression\\n+\\n+\\nTensorFlow/Keras\\nPython\\n+Semantic sigmoid loss\\nFPN [66]\\nRPN\\n+\\nSynchronized SGD\\nClass Log loss+bounding box regression\\n+\\n+\\nTensorFlow\\nPython\\nYOLO [17]\\n-\\n-\\nSGD\\nClass sum-squared error loss+bounding box regression\\n+\\n+\\nDarknet\\nC\\n+object conÔ¨Ådence+background conÔ¨Ådence\\nSSD [71]\\n-\\n-\\nSGD\\nClass softmax loss+bounding box regression\\n-\\n+\\nCaffe\\nC++\\nYOLOv2 [72]\\n-\\n-\\nSGD\\nClass sum-squared error loss+bounding box regression\\n+\\n+\\nDarknet\\nC\\n+object conÔ¨Ådence+background conÔ¨Ådence\\n* ‚Äò+‚Äô denotes that corresponding techniques are employed while ‚Äò-‚Äô denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the\\nother architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions\\nby the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).\\nMethods\\nTrained on\\nareo\\nbike\\nbird\\nboat\\nbottle\\nbus\\ncar\\ncat\\nchair\\ncow\\ntable\\ndog\\nhorse\\nmbike\\nperson\\nplant\\nsheep\\nsofa\\ntrain\\ntv\\nmAP\\nR-CNN (Alex) [15]\\n07\\n68.1\\n72.8\\n56.8\\n43.0\\n36.8\\n66.3\\n74.2\\n67.6\\n34.4\\n63.5\\n54.5\\n61.2\\n69.1\\n68.6\\n58.7\\n33.4\\n62.9\\n51.1\\n62.5\\n68.6\\n58.5\\nR-CNN(VGG16) [15]\\n07\\n73.4\\n77.0\\n63.4\\n45.4\\n44.6\\n75.1\\n78.1\\n79.8\\n40.5\\n73.7\\n62.2\\n79.4\\n78.1\\n73.1\\n64.2\\n35.6\\n66.8\\n67.2\\n70.4\\n71.1\\n66.0\\nSPP-net(ZF) [64]\\n07\\n68.5\\n71.7\\n58.7\\n41.9\\n42.5\\n67.7\\n72.1\\n73.8\\n34.7\\n67.0\\n63.4\\n66.0\\n72.5\\n71.3\\n58.9\\n32.8\\n60.9\\n56.1\\n67.9\\n68.8\\n60.9\\nGCNN [70]\\n07\\n68.3\\n77.3\\n68.5\\n52.4\\n38.6\\n78.5\\n79.5\\n81.0\\n47.1\\n73.6\\n64.5\\n77.2\\n80.5\\n75.8\\n66.6\\n34.3\\n65.2\\n64.4\\n75.6\\n66.4\\n66.8\\nBayes [85]\\n07\\n74.1\\n83.2\\n67.0\\n50.8\\n51.6\\n76.2\\n81.4\\n77.2\\n48.1\\n78.9\\n65.6\\n77.3\\n78.4\\n75.1\\n70.1\\n41.4\\n69.6\\n60.8\\n70.2\\n73.7\\n68.5\\nFast R-CNN [16]\\n07+12\\n77.0\\n78.1\\n69.3\\n59.4\\n38.3\\n81.6\\n78.6\\n86.7\\n42.8\\n78.8\\n68.9\\n84.7\\n82.0\\n76.6\\n69.9\\n31.8\\n70.1\\n74.8\\n80.4\\n70.4\\n70.0\\nSDP+CRC [33]\\n07\\n76.1\\n79.4\\n68.2\\n52.6\\n46.0\\n78.4\\n78.4\\n81.0\\n46.7\\n73.5\\n65.3\\n78.6\\n81.0\\n76.7\\n77.3\\n39.0\\n65.1\\n67.2\\n77.5\\n70.3\\n68.9\\nSubCNN [60]\\n07\\n70.2\\n80.5\\n69.5\\n60.3\\n47.9\\n79.0\\n78.7\\n84.2\\n48.5\\n73.9\\n63.0\\n82.7\\n80.6\\n76.0\\n70.2\\n38.2\\n62.4\\n67.7\\n77.7\\n60.5\\n68.5\\nStuffNet30 [100]\\n07\\n72.6\\n81.7\\n70.6\\n60.5\\n53.0\\n81.5\\n83.7\\n83.9\\n52.2\\n78.9\\n70.7\\n85.0\\n85.7\\n77.0\\n78.7\\n42.2\\n73.6\\n69.2\\n79.2\\n73.8\\n72.7\\nNOC [114]\\n07+12\\n76.3\\n81.4\\n74.4\\n61.7\\n60.8\\n84.7\\n78.2\\n82.9\\n53.0\\n79.2\\n69.2\\n83.2\\n83.2\\n78.5\\n68.0\\n45.0\\n71.6\\n76.7\\n82.2\\n75.7\\n73.3\\nMR-CNN&S-CNN [110]\\n07+12\\n80.3\\n84.1\\n78.5\\n70.8\\n68.5\\n88.0\\n85.9\\n87.8\\n60.3\\n85.2\\n73.7\\n87.2\\n86.5\\n85.0\\n76.4\\n48.5\\n76.3\\n75.5\\n85.0\\n81.0\\n78.2\\nHyperNet [101]\\n07+12\\n77.4\\n83.3\\n75.0\\n69.1\\n62.4\\n83.1\\n87.4\\n87.4\\n57.1\\n79.8\\n71.4\\n85.1\\n85.1\\n80.0\\n79.1\\n51.2\\n79.1\\n75.7\\n80.9\\n76.5\\n76.3\\nMS-GR [104]\\n07+12\\n80.0\\n81.0\\n77.4\\n72.1\\n64.3\\n88.2\\n88.1\\n88.4\\n64.4\\n85.4\\n73.1\\n87.3\\n87.4\\n85.1\\n79.6\\n50.1\\n78.4\\n79.5\\n86.9\\n75.5\\n78.6\\nOHEM+Fast R-CNN [113]\\n07+12\\n80.6\\n85.7\\n79.8\\n69.9\\n60.8\\n88.3\\n87.9\\n89.6\\n59.7\\n85.1\\n76.5\\n87.1\\n87.3\\n82.4\\n78.8\\n53.7\\n80.5\\n78.7\\n84.5\\n80.7\\n78.9\\nION [95]\\n07+12+S\\n80.2\\n85.2\\n78.8\\n70.9\\n62.6\\n86.6\\n86.9\\n89.8\\n61.7\\n86.9\\n76.5\\n88.4\\n87.5\\n83.4\\n80.5\\n52.4\\n78.1\\n77.2\\n86.9\\n83.5\\n79.2\\nFaster R-CNN [18]\\n07\\n70.0\\n80.6\\n70.1\\n57.3\\n49.9\\n78.2\\n80.4\\n82.0\\n52.2\\n75.3\\n67.2\\n80.3\\n79.8\\n75.0\\n76.3\\n39.1\\n68.3\\n67.3\\n81.1\\n67.6\\n69.9\\nFaster R-CNN [18]\\n07+12\\n76.5\\n79.0\\n70.9\\n65.5\\n52.1\\n83.1\\n84.7\\n86.4\\n52.0\\n81.9\\n65.7\\n84.8\\n84.6\\n77.5\\n76.7\\n38.8\\n73.6\\n73.9\\n83.0\\n72.6\\n73.2\\nFaster R-CNN [18]\\n07+12+COCO\\n84.3\\n82.0\\n77.7\\n68.9\\n65.7\\n88.1\\n88.4\\n88.9\\n63.6\\n86.3\\n70.8\\n85.9\\n87.6\\n80.1\\n82.3\\n53.6\\n80.4\\n75.8\\n86.6\\n78.9\\n78.8\\nSSD300 [71]\\n07+12+COCO\\n80.9\\n86.3\\n79.0\\n76.2\\n57.6\\n87.3\\n88.2\\n88.6\\n60.5\\n85.4\\n76.7\\n87.5\\n89.2\\n84.5\\n81.4\\n55.0\\n81.9\\n81.5\\n85.9\\n78.9\\n79.6\\nSSD512 [71]\\n07+12+COCO\\n86.6\\n88.3\\n82.4\\n76.0\\n66.3\\n88.6\\n88.9\\n89.1\\n65.1\\n88.4\\n73.6\\n86.5\\n88.9\\n85.3\\n84.6\\n59.1\\n85.0\\n80.4\\n87.4\\n81.2\\n81.6\\n* ‚Äò07‚Äô: VOC2007 trainval, ‚Äò07+12‚Äô: union of VOC2007 and VOC2012 trainval, ‚Äò07+12+COCO‚Äô: trained on COCO trainval35k at Ô¨Årst and then Ô¨Åne-tuned on 07+12. The S in ION ‚Äò07+12+S‚Äô denotes SBD segmentation labels.\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 TEST SET (%).\\nMethods\\nTrained on\\nareo\\nbike\\nbird\\nboat\\nbottle\\nbus\\ncar\\ncat\\nchair\\ncow\\ntable\\ndog\\nhorse\\nmbike\\nperson\\nplant\\nsheep\\nsofa\\ntrain\\ntv\\nmAP\\nR-CNN(Alex) [15]\\n12\\n71.8\\n65.8\\n52.0\\n34.1\\n32.6\\n59.6\\n60.0\\n69.8\\n27.6\\n52.0\\n41.7\\n69.6\\n61.3\\n68.3\\n57.8\\n29.6\\n57.8\\n40.9\\n59.3\\n54.1\\n53.3\\nR-CNN(VGG16) [15]\\n12\\n79.6\\n72.7\\n61.9\\n41.2\\n41.9\\n65.9\\n66.4\\n84.6\\n38.5\\n67.2\\n46.7\\n82.0\\n74.8\\n76.0\\n65.2\\n35.6\\n65.4\\n54.2\\n67.4\\n60.3\\n62.4\\nBayes [85]\\n12\\n82.9\\n76.1\\n64.1\\n44.6\\n49.4\\n70.3\\n71.2\\n84.6\\n42.7\\n68.6\\n55.8\\n82.7\\n77.1\\n79.9\\n68.7\\n41.4\\n69.0\\n60.0\\n72.0\\n66.2\\n66.4\\nFast R-CNN [16]\\n07++12\\n82.3\\n78.4\\n70.8\\n52.3\\n38.7\\n77.8\\n71.6\\n89.3\\n44.2\\n73.0\\n55.0\\n87.5\\n80.5\\n80.8\\n72.0\\n35.1\\n68.3\\n65.7\\n80.4\\n64.2\\n68.4\\nSutffNet30 [100]\\n12\\n83.0\\n76.9\\n71.2\\n51.6\\n50.1\\n76.4\\n75.7\\n87.8\\n48.3\\n74.8\\n55.7\\n85.7\\n81.2\\n80.3\\n79.5\\n44.2\\n71.8\\n61.0\\n78.5\\n65.4\\n70.0\\nNOC [114]\\n07+12\\n82.8\\n79.0\\n71.6\\n52.3\\n53.7\\n74.1\\n69.0\\n84.9\\n46.9\\n74.3\\n53.1\\n85.0\\n81.3\\n79.5\\n72.2\\n38.9\\n72.4\\n59.5\\n76.7\\n68.1\\n68.8\\nMR-CNN&S-CNN [110]\\n07++12\\n85.5\\n82.9\\n76.6\\n57.8\\n62.7\\n79.4\\n77.2\\n86.6\\n55.0\\n79.1\\n62.2\\n87.0\\n83.4\\n84.7\\n78.9\\n45.3\\n73.4\\n65.8\\n80.3\\n74.0\\n73.9\\nHyperNet [101]\\n07++12\\n84.2\\n78.5\\n73.6\\n55.6\\n53.7\\n78.7\\n79.8\\n87.7\\n49.6\\n74.9\\n52.1\\n86.0\\n81.7\\n83.3\\n81.8\\n48.6\\n73.5\\n59.4\\n79.9\\n65.7\\n71.4\\nOHEM+Fast R-CNN [113]\\n07++12+coco\\n90.1\\n87.4\\n79.9\\n65.8\\n66.3\\n86.1\\n85.0\\n92.9\\n62.4\\n83.4\\n69.5\\n90.6\\n88.9\\n88.9\\n83.6\\n59.0\\n82.0\\n74.7\\n88.2\\n77.3\\n80.1\\nION [95]\\n07+12+S\\n87.5\\n84.7\\n76.8\\n63.8\\n58.3\\n82.6\\n79.0\\n90.9\\n57.8\\n82.0\\n64.7\\n88.9\\n86.5\\n84.7\\n82.3\\n51.4\\n78.2\\n69.2\\n85.2\\n73.5\\n76.4\\nFaster R-CNN [18]\\n07++12\\n84.9\\n79.8\\n74.3\\n53.9\\n49.8\\n77.5\\n75.9\\n88.5\\n45.6\\n77.1\\n55.3\\n86.9\\n81.7\\n80.9\\n79.6\\n40.1\\n72.6\\n60.9\\n81.2\\n61.5\\n70.4\\nFaster R-CNN [18]\\n07++12+coco\\n87.4\\n83.6\\n76.8\\n62.9\\n59.6\\n81.9\\n82.0\\n91.3\\n54.9\\n82.6\\n59.0\\n89.0\\n85.5\\n84.7\\n84.1\\n52.2\\n78.9\\n65.5\\n85.4\\n70.2\\n75.9\\nYOLO [17]\\n07++12\\n77.0\\n67.2\\n57.7\\n38.3\\n22.7\\n68.3\\n55.9\\n81.4\\n36.2\\n60.8\\n48.5\\n77.2\\n72.3\\n71.3\\n63.5\\n28.9\\n52.2\\n54.8\\n73.9\\n50.8\\n57.9\\nYOLO+Fast R-CNN [17]\\n07++12\\n83.4\\n78.5\\n73.5\\n55.8\\n43.4\\n79.1\\n73.1\\n89.4\\n49.4\\n75.5\\n57.0\\n87.5\\n80.9\\n81.0\\n74.7\\n41.8\\n71.5\\n68.5\\n82.1\\n67.2\\n70.7\\nYOLOv2 [72]\\n07++12+coco\\n88.8\\n87.0\\n77.8\\n64.9\\n51.8\\n85.2\\n79.3\\n93.1\\n64.4\\n81.4\\n70.2\\n91.3\\n88.1\\n87.2\\n81.0\\n57.7\\n78.1\\n71.0\\n88.5\\n76.8\\n78.2\\nSSD300 [71]\\n07++12+coco\\n91.0\\n86.0\\n78.1\\n65.0\\n55.4\\n84.9\\n84.0\\n93.4\\n62.1\\n83.6\\n67.3\\n91.3\\n88.9\\n88.6\\n85.6\\n54.7\\n83.8\\n77.3\\n88.3\\n76.5\\n79.3\\nSSD512 [71]\\n07++12+coco\\n91.4\\n88.6\\n82.6\\n71.4\\n63.1\\n87.4\\n88.1\\n93.9\\n66.9\\n86.6\\n66.3\\n92.0\\n91.7\\n90.8\\n88.5\\n60.9\\n87.0\\n75.4\\n90.2\\n80.4\\n82.2\\nR-FCN (ResNet101) [16]\\n07++12+coco\\n92.3\\n89.9\\n86.7\\n74.7\\n75.2\\n86.7\\n89.0\\n95.8\\n70.2\\n90.4\\n66.5\\n95.0\\n93.2\\n92.1\\n91.1\\n71.0\\n89.7\\n76.0\\n92.0\\n83.4\\n85.0\\n* ‚Äò07++12‚Äô: union of VOC2007 trainval and test and VOC2012 trainval. ‚Äò07++12+COCO‚Äô: trained on COCO trainval35k at Ô¨Årst then Ô¨Åne-tuned on 07++12.\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods\\nTrained on 0.5:0.95 0.5 0.75\\nS\\nM\\nL\\n1\\n10\\n100\\nS\\nM\\nL\\nFast R-CNN [16]\\ntrain\\n20.5\\n39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0\\nION [95]\\ntrain\\n23.6\\n43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6\\nNOC+FRCN(VGG16) [114]\\ntrain\\n21.2\\n41.5 19.7\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nNOC+FRCN(Google) [114]\\ntrain\\n24.8\\n44.4 25.2\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nNOC+FRCN (ResNet101) [114]\\ntrain\\n27.2\\n48.4 27.6\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nGBD-Net [109]\\ntrain\\n27.0\\n45.8\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nOHEM+FRCN [113]\\ntrain\\n22.6\\n42.5 22.2 5.0 23.7 34.6\\n-\\n-\\n-\\n-\\n-\\n-\\nOHEM+FRCN* [113]\\ntrain\\n24.4\\n44.4 24.8 7.1 26.4 37.9\\n-\\n-\\n-\\n-\\n-\\n-\\nOHEM+FRCN* [113]\\ntrainval\\n25.5\\n45.9 26.1 7.4 27.7 38.5\\n-\\n-\\n-\\n-\\n-\\n-\\nFaster R-CNN [18]\\ntrainval\\n24.2\\n45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4\\nYOLOv2 [72]\\ntrainval35k\\n21.6\\n44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71]\\ntrainval35k\\n23.2\\n41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5\\nSSD512 [71]\\ntrainval35k\\n26.8\\n46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0\\nR-FCN (ResNet101) [65]\\ntrainval\\n29.2\\n51.5\\n-\\n10.8 32.8 45.0\\n-\\n-\\n-\\n-\\n-\\n-\\nR-FCN*(ResNet101) [65]\\ntrainval\\n29.9\\n51.9\\n-\\n10.4 32.4 43.3\\n-\\n-\\n-\\n-\\n-\\n-\\nR-FCN**(ResNet101) [65]\\ntrainval\\n31.5\\n53.2\\n-\\n14.3 35.5 44.2\\n-\\n-\\n-\\n-\\n-\\n-\\nMulti-path [112]\\ntrainval\\n33.2\\n51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4\\nFPN (ResNet101) [66]\\ntrainval35k\\n36.2\\n59.1 39.0 18.2 39.0 48.2\\n-\\n-\\n-\\n-\\n-\\n-\\nMask (ResNet101+FPN) [67]\\ntrainval35k\\n38.2\\n60.3 41.7 20.1 41.1 50.2\\n-\\n-\\n-\\n-\\n-\\n-\\nMask (ResNeXt101+FPN) [67] trainval35k\\n39.8\\n62.3 43.4 22.1 43.2 51.2\\n-\\n-\\n-\\n-\\n-\\n-\\nDSSD513 (ResNet101) [73]\\ntrainval35k\\n33.2\\n53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74]\\ntrainval\\n29.3\\n47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0\\n* FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN\\nwith multi-scale training and testing, Mask: Mask R-CNN.\\nX GPU. Except for ‚ÄòSS‚Äô which is processed with CPU, the\\nother procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n‚Ä¢ By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test time is\\nfurther reduced with the uniÔ¨Åed multi-task learning (FRCN)\\nand removal of additional region proposal generation stage\\n(Faster R-CNN). It‚Äôs also helpful to compress the parameters\\nof FC layers with SVD [91] (PAVNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET.\\nMethods\\nTrained on\\nmAP(%)\\nTest time(sec/img)\\nRate(FPS)\\nSS+R-CNN [15]\\n07\\n66.0\\n32.84\\n0.03\\nSS+SPP-net [64]\\n07\\n63.1\\n2.3\\n0.44\\nSS+FRCN [16]\\n07+12\\n66.9\\n1.72\\n0.6\\nSDP+CRC [33]\\n07\\n68.9\\n0.47\\n2.1\\nSS+HyperNet* [101]\\n07+12\\n76.3\\n0.20\\n5\\nMR-CNN&S-CNN [110]\\n07+12\\n78.2\\n30\\n0.03\\nION [95]\\n07+12+S\\n79.2\\n1.92\\n0.5\\nFaster R-CNN(VGG16) [18]\\n07+12\\n73.2\\n0.11\\n9.1\\nFaster R-CNN(ResNet101) [18]\\n07+12\\n83.8\\n2.24\\n0.4\\nYOLO [17]\\n07+12\\n63.4\\n0.02\\n45\\nSSD300 [71]\\n07+12\\n74.3\\n0.02\\n46\\nSSD512 [71]\\n07+12\\n76.8\\n0.05\\n19\\nR-FCN(ResNet101) [65]\\n07+12+coco\\n83.6\\n0.17\\n5.9\\nYOLOv2(544*544) [72]\\n07+12\\n78.6\\n0.03\\n40\\nDSSD321(ResNet101) [73]\\n07+12\\n78.6\\n0.07\\n13.6\\nDSOD300 [74]\\n07+12+coco\\n81.7\\n0.06\\n17.4\\nPVANET+ [116]\\n07+12+coco\\n83.8\\n0.05\\n21.7\\nPVANET+(compress) [116]\\n07+12+coco\\n82.9\\n0.03\\n31.3\\n* SS: Selective Search [15], SS*: ‚Äòfast mode‚Äô Selective Search [16], HyperNet*: the speed up version of\\nHyperNet and PAVNET+ (compresss): PAVNET with additional bounding box voting and compressed fully\\nconvolutional layers.\\n‚Ä¢ It takes additional test time to extract multi-scale fea-\\ntures and contextual information (ION and MR-RCNN&S-\\nRCNN).\\n‚Ä¢ It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time con-\\nsumption can be reduced by adding as many layers into\\nshared fully convolutional layers as possible (FRCN).\\n‚Ä¢ Regression based models can usually be processed in real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 11, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n12\\ntime at the cost of a drop in accuracy compared with region\\nproposal based models. Also, region proposal based models\\ncan be modiÔ¨Åed into real-time systems with the introduction\\nof other tricks [116] (PVANET), such as BN [43], residual\\nconnections [123].\\nIV. SALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous ap-\\nplications incorporate the visual saliency to improve their\\nperformance, such as image cropping [125] and segmentation\\n[126], image retrieval [57] and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of\\nthe scene. To learn local feature contrast, various local and\\nglobal features are extracted from pixels, e.g. edges [129],\\nspatial information [130]. However, high-level and multi-scale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is task-\\noriented and takes prior knowledge about object categories\\nto guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep learning in Salient Object Detection\\nDue to the signiÔ¨Åcance for providing high-level and multi-\\nscale feature representation and the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133] and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Eleonora Vig et al. [28] follows a\\ncompletely automatic data-driven approach to perform a large-\\nscale search for optimal features, namely an ensemble of deep\\nnetworks with different layers and parameters. To address the\\nproblem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A\\nsimilar architecture was proposed by Huang et al. to integrate\\nsaliency prediction into pre-trained object recognition DNNs\\n[135]. The transfer is accomplished by Ô¨Åne-tuning DNNs‚Äô\\nweights with an objective function based on the saliency\\nevaluation metrics, such as Similarity, KL-Divergence and\\nNormalized Scanpath Saliency.\\nSome works combined local and global visual clues to\\nimprove salient object detection performance. Wang et al.\\ntrained two independent deep CNNs (DNN-L and DNN-G)\\nto capture local information and global contrast and predicted\\nsaliency maps by integrating both local estimation and global\\nsearch [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reÔ¨Åned the results\\nwith a multi-scale superpixel-averaging [137]. Zhao et al.\\nproposed a multi-context deep learning framework, which\\nutilizes a uniÔ¨Åed learning framework to model global and\\nlocal context jointly with the aid of superpixel segmentation\\n[138]. To predict saliency in videos, Bak et al. fused two\\nstatic saliency models, namely spatial stream net and tem-\\nporal stream net, into a two-stream framework with a novel\\nempirically grounded data augmentation technique [139].\\nComplementary information from semantic segmentation\\nand context modeling is beneÔ¨Åcial. To learn internal represen-\\ntations of saliency efÔ¨Åciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully convolutional neural network, Li\\net al. proposed a multi-task deep saliency model, in which\\nintrinsic correlations between saliency detection and semantic\\nsegmentation are set up [141]. However, due to the conv layers\\nwith large receptive Ô¨Åelds and pooling layers, blurry object\\nboundaries and coarse saliency maps are produced. Tang et\\nal. proposed a novel saliency detection framework (CRPSD)\\n[142], which combines region-level saliency estimation and\\npixel-level saliency prediction together with three closely\\nrelated CNNs. Li et al. proposed a deep contrast network\\nto combine segment-wise spatial pooling and pixel-level fully\\nconvolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiÔ¨Åcance for improving detection performance. Based\\non Fast R-CNN, Wang et al. proposed the RegionNet by\\nperforming salient object detection with end-to-end edge pre-\\nserving and multi-scale contextual modelling [144]. Liu et al.\\n[27] proposed a multi-resolution convolutional neural network\\n(Mr-CNN) to predict eye Ô¨Åxations, which is achieved by\\nlearning both bottom-up visual saliency and top-down visual\\nfactors from raw image data simultaneously. Cornia et al.\\nproposed an architecture which combines features extracted at\\ndifferent levels of the CNN [145]. Li et al. proposed a multi-\\nscale deep CNN framework to extract three scales of deep\\ncontrast features [146], namely the mean-subtracted region,\\nthe bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efÔ¨Åcient and accurate to train a direct pixel-wise\\nCNN architecture to predict salient objects with the aids of\\nRNNs and deconvolution networks. Pan et al. formulated\\nsaliency prediction as a minimization optimization on the\\nEuclidean distance between the predicted saliency map and\\nthe ground truth and proposed two kinds of architectures\\n[147]: a shallow one trained from scratch and a deeper one\\nadapted from deconvoluted VGG network. As convolutional-\\ndeconvolution networks are not expert in recognizing objects\\nof multiple scales, Kuen et al. proposed a recurrent attentional\\nconvolutional-deconvolution network (RACDNN) with several\\nspatial transformer and recurrent network units to conquer\\nthis problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)\\nto perform a full image-to-image saliency detection [149].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 12, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n13\\nB. Experimental Evaluation\\nFour representative datasets, including ECSSD [156], HKU-\\nIS [146], PASCALS [157], and SOD [158], are used to\\nevaluate several state-of-the-art methods. ECSSD consists of\\n1000 structurally complex but semantically meaningful natural\\nimages. HKU-IS is a large-scale dataset containing over 4000\\nchallenging images. Most of these images have more than\\none salient object and own low contrast. PASCALS is a\\nsubset chosen from the validation set of PASCAL VOC 2010\\nsegmentation dataset and is composed of 850 natural images.\\nThe SOD dataset possesses 300 images containing multiple\\nsalient objects. The training and validation sets for different\\ndatasets are kept the same as those in [152].\\nTwo standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is deÔ¨Åned as below\\nFŒ≤ = (1 + Œ≤2)Presion √ó Recall\\nŒ≤2Presion + Recall\\n(7)\\nwhere Œ≤2 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation\\nMAE =\\n1\\nH √ó W\\nH\\nX\\ni=1\\nW\\nX\\nj=1\\n\\x0c\\x0c\\x0c ÀÜS(i, j) = ÀÜZ(i, j)\\n\\x0c\\x0c\\x0c\\n(8)\\nwhere ÀÜZ and ÀÜS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected non-salient pixels [159].\\nThe following approaches are evaluated: CHM [150], RC\\n[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,\\nRC and DRFI are classical ones with the best performance\\n[159], while the other methods are all associated with CNN.\\nF-measure and MAE scores are shown in Table VI.\\nFrom Table VI, we can Ô¨Ånd that CNN based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a\\nmore accurate saliency. ELD refers to low-level handcrafted\\nfeatures for complementary information. LEGS adopts generic\\nregion proposals to provide initial salient regions, which may\\nbe insufÔ¨Åcient for salient detection. DSR and MT act in\\ndifferent ways by introducing recurrent network and semantic\\nsegmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF\\nand DSSC perform the best on these four datasets. DSSC\\nearns the best performance by modelling scale-to-scale short-\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of CNN based methods need to model\\nvisual saliency along region boundaries with the aid of su-\\nperpixel segmentation. Meanwhile, the extraction of multi-\\nscale deep CNN features is of signiÔ¨Åcance for measuring local\\nconspicuity. Finally, it‚Äôs necessary to strengthen local con-\\nnections between different CNN layers and as well to utilize\\ncomplementary information from local and global context.\\nV. FACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition\\n[160]‚Äì[162], face synthesis [163], [164] and facial expression\\nanalysis [165]. Different from generic object detection, this\\ntask is to recognize and locate face regions covering a very\\nlarge range of scales (30-300 pts vs. 10-1000 pts). At the same\\ntime, faces have their unique object structural conÔ¨Ågurations\\n(e.g. the distribution of different face parts) and characteristics\\n(e.g. skin color). All these differences lead to special attention\\nto this task. However, large visual variations of faces, such as\\nocclusions, pose variations and illumination changes, impose\\ngreat challenges for this task in real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiÔ¨Åers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time\\nefÔ¨Åciency. However, this detector may degrade signiÔ¨Åcantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. proposed a deformable part model (DPM) for face\\ndetection [24]. However, for these traditional face detection\\nmethods, high computational expenses and large quantities\\nof annotations are required to achieve a reasonable result.\\nBesides, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep learning in Face Detection\\nRecently, some CNN based face detection approaches have\\nbeen proposed [167]‚Äì[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting\\nthe four bounds of box jointly. Farfade et al. [168] proposed a\\nDeep Dense Face Detector (DDFD) to conduct multi-view face\\ndetection, which is able to detect faces in a wide range of ori-\\nentations without requirement of pose/landmark annotations.\\nYang et al. proposed a novel deep learning based face detection\\nframework [169], which collects the responses from local fa-\\ncial parts (e.g. eyes, nose and mouths) to address face detection\\nunder severe occlusions and unconstrained pose variations.\\nYang et al. [170] proposed a scale-friendly detection network\\nnamed ScaleFace, which splits a large range of target scales\\ninto smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an\\nefÔ¨Åcient CNN to predict the scale distribution histogram of the\\nfaces and took this histogram to guide the zoom-in and zoom-\\nout of the image [171]. Since the faces are approximately\\nin uniform scale after zoom, compared with other state-of-\\nthe-art baselines, better performance is achieved with less\\ncomputation cost. Besides, some generic detection frameworks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 13, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n14\\nTABLE VI\\nCOMPARISON BETWEEN STATE OF THE ART METHODS.\\nDataset\\nMetrics\\nCHM [150]\\nRC [151]\\nDRFI [152]\\nMC [138]\\nMDF [146]\\nLEGS [136]\\nDSR [149]\\nMTDNN [141]\\nCRPSD [142]\\nDCL [143]\\nELD [153]\\nNLDF [154]\\nDSSC [155]\\nPASCAL-S\\nwFŒ≤\\n0.631\\n0.640\\n0.679\\n0.721\\n0.764\\n0.756\\n0.697\\n0.818\\n0.776\\n0.822\\n0.767\\n0.831\\n0.830\\nMAE\\n0.222\\n0.225\\n0.221\\n0.147\\n0.145\\n0.157\\n0.128\\n0.170\\n0.063\\n0.108\\n0.121\\n0.099\\n0.080\\nECSSD\\nwFŒ≤\\n0.722\\n0.741\\n0.787\\n0.822\\n0.833\\n0.827\\n0.872\\n0.810\\n0.849\\n0.898\\n0.865\\n0.905\\n0.915\\nMAE\\n0.195\\n0.187\\n0.166\\n0.107\\n0.108\\n0.118\\n0.037\\n0.160\\n0.046\\n0.071\\n0.098\\n0.063\\n0.052\\nHKU-IS\\nwFŒ≤\\n0.728\\n0.726\\n0.783\\n0.781\\n0.860\\n0.770\\n0.833\\n-\\n0.821\\n0.907\\n0.844\\n0.902\\n0.913\\nMAE\\n0.158\\n0.165\\n0.143\\n0.098\\n0.129\\n0.118\\n0.040\\n-\\n0.043\\n0.048\\n0.071\\n0.048\\n0.039\\nSOD\\nwFŒ≤\\n0.655\\n0.657\\n0.712\\n0.708\\n0.785\\n0.707\\n-\\n0.781\\n-\\n0.832\\n0.760\\n0.810\\n0.842\\nMAE\\n0.249\\n0.242\\n0.215\\n0.184\\n0.155\\n0.205\\n-\\n0.150\\n-\\n0.126\\n0.154\\n0.143\\n0.118\\n* The bigger wFŒ≤ is or the smaller MAE is, the better the performance is.\\nare extended to face detection with different modiÔ¨Åcations, e.g.\\nFaster R-CNN [29], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3D modelling and face landmarks, in a multi-\\ntask learning manner. Huang et al. proposed a uniÔ¨Åed end-\\nto-end FCN framework called DenseBox to jointly conduct\\nface detection and landmark localization [174]. Li et al.\\n[175] proposed a multi-task discriminative learning framework\\nwhich integrates a ConvNet with a Ô¨Åxed 3D mean face model\\nin an end-to-end manner. In the framework, two issues are\\naddressed to transfer from generic object detection to face\\ndetection, namely eliminating predeÔ¨Åned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with\\na conÔ¨Åguration pooling layer. Zhang et al. [176] proposed a\\ndeep cascaded multi-task framework named MTCNN which\\nexploits the inherent correlations between face detection and\\nalignment in unconstrained environment to boost up detection\\nperformance in a coarse-to-Ô¨Åne manner.\\nReducing computational expenses is of necessity in real ap-\\nplications. To achieve real-time detection on mobile platform,\\nKalinovskii and Spitsyn proposed a new solution of frontal\\nface detection based on compact CNN cascades [177]. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify and reÔ¨Åne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously, and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang et al. proposed a three-stage\\ncascade structure based on FCNs [8], while in each stage, a\\nmulti-scale FCN is utilized to reÔ¨Åne the positions of possible\\nfaces. Qin et al. proposed a uniÔ¨Åed framework which achieves\\nbetter results with the complementary information from dif-\\nferent jointly trained CNNs [178].\\nB. Experimental Evaluation\\nThe FDDB [179] dataset has a total of 2,845 pictures in\\nwhich 5,171 faces are annotated with elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reÔ¨Çect the dependence of\\nthe detected face fractions on the number of false alarms.\\nCompared with annotations, any detection with an IoU ratio\\nexceeding 0.5 is treated as positive. Each annotation is only\\nassociated with one detection. The ROC curve for the contin-\\nuous scores is the reÔ¨Çection of face localization quality.\\nThe evaluated models cover DDFD [168], CascadeCNN\\n[180], ACF-multiscale [181], Pico [182], HeadHunter [183],\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0\\n 500\\n 1000\\n 1500\\n 2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0\\n 500\\n 1000\\n 1500\\n 2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(b) Continuous ROC curves\\nFig. 11. The ROC curves of state-of-the-art methods on FDDB.\\nJoint Cascade [30], SURF-multiview [184], Viola-Jones [166],\\nNPDFace [185], Faceness [169], CCF [186], MTCNN [176],\\nConv3D [175], Hyperface [187], UnitBox [167], LDCF+ [S2],\\nDeepIR [173], HR-ER [188], Face-R-CNN [172] and Scale-\\nFace [170]. ACF-multiscale, Pico, HeadHunter, Joint Cascade,\\nSURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.\\nFrom Figure 11(a), in spite of relatively competitive results\\nproduced by LDCF+, it can be observed that most of classic\\nmethods perform with similar results and are outperformed\\nby CNN based methods by a signiÔ¨Åcant margin. From Figure\\n11(b), it can be observed that most of CNN based methods\\nearn similar true positive rates between 60% and 70% while\\nDeepIR and HR-ER perform much better than them. Among\\nclassic methods, Joint Cascade is still competitive. As earlier\\nworks, DDFD and CCF directly make use of generated feature\\nmaps and obtain relatively poor results. CascadeCNN builds\\ncascaded CNNs to locate face regions, which is efÔ¨Åcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3D and Hyperface proves the effectiveness of multi-task\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales, and make a balance between accuracy and\\nefÔ¨Åciency. DeepIR and Face-R-CNN are two extensions of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 14, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n15\\nFaster R-CNN architecture to face detection, which validate\\nthe signiÔ¨Åcance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cascades, adapting scale-aware\\ndetection and learning multi-task shared CNN features.\\nVI. PEDESTRIAN DETECTION\\nRecently, pedestrian detection has been intensively studied,\\nwhich has a close relationship to pedestrian tracking [189],\\n[190], person re-identiÔ¨Åcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based\\nmethods [195], [196], some researchers combined boosted\\ndecision forests with hand-crafted features to obtain pedestrian\\ndetectors [197]‚Äì[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes\\nin typical scenarios of pedestrian detection (e.g. automatic\\ndriving and intelligent surveillance), the application of RoI\\npooling layer in generic object detection pipeline may result\\nin ‚Äòplain‚Äô features due to collapsing bins. In the meantime, the\\nmain source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different conÔ¨Ågurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some researches have been conducted to analyze the\\nreasons. Zhang et al. attempted to adapt generic Faster R-CNN\\n[18] to pedestrian detection [203]. They modiÔ¨Åed the down-\\nstream classiÔ¨Åer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small\\ninstances and hard negative examples. To deal with complex\\nocclusions existing in pedestrian images, inspired by DPM\\n[24], Tian et al. proposed a deep learning framework called\\nDeepParts [204], which makes decisions based an ensemble of\\nextensive part detectors. DeepParts has advantages in dealing\\nwith weakly labeled data, low IoU positive proposals and\\npartial occlusion.\\nOther researchers also tried to combine complementary in-\\nformation from multiple data sources. CompACT-Deep adopts\\na complexity-aware cascade to combine hand-crafted features\\nand Ô¨Åne-tuned DCNNs [195]. Based on Faster R-CNN, Liu et\\nal. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-\\nassistant CNN (TA-CNN) to jointly learn multiple tasks with\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF\\nSTATE-OF-THE-ART MODELS ON CALTECH PEDESTRIAN DATASET. ALL\\nNUMBERS ARE REPORTED IN L-AMR.\\nMethod\\nReasonable\\nAll\\nFar\\nMedium\\nNear\\nnone\\npartial\\nheavy\\nCheckerboards+ [198]\\n17.1\\n68.4\\n100\\n58.3\\n5.1\\n15.6\\n31.4\\n78.4\\nLDCF++[S2]\\n15.2\\n67.1\\n100\\n58.4\\n5.4\\n13.3\\n33.3\\n76.2\\nSCF+AlexNet [210]\\n23.3\\n70.3\\n100\\n62.3\\n10.2\\n20.0\\n48.5\\n74.7\\nSA-FastRCNN [211]\\n9.7\\n62.6\\n100\\n51.8\\n0\\n7.7\\n24.8\\n64.3\\nMS-CNN [105]\\n10.0\\n61.0\\n97.2\\n49.1\\n2.6\\n8.2\\n19.2\\n60.0\\nDeepParts [204]\\n11.9\\n64.8\\n100\\n56.4\\n4.8\\n10.6\\n19.9\\n60.4\\nCompACT-Deep [195]\\n11.8\\n64.4\\n100\\n53.2\\n4.0\\n9.6\\n25.1\\n65.8\\nRPN+BF [203]\\n9.6\\n64.7\\n100\\n53.9\\n2.3\\n7.7\\n24.2\\n74.2\\nF-DNN+SS [207]\\n8.2\\n50.3\\n77.5\\n33.2\\n2.8\\n6.7\\n15.1\\n53.4\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. proposed\\na deep neural network fusion architecture for fast and robust\\npedestrian detection [207]. Based on the candidate bounding\\nboxes generated with SSD detectors [71], multiple binary\\nclassiÔ¨Åers are processed parallelly to conduct soft-rejection\\nbased network fusion (SNF) by consulting their aggregated\\ndegree of conÔ¨Ådences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel and a large VGG16 model [195]. DeepParts contains\\n45 Ô¨Åne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiÔ¨Åcation and\\nsimpliÔ¨Åcation is of signiÔ¨Åcance to reduce the burden on both\\nsoftware and hardware to satisfy real-time detection demand.\\nTome et al. proposed a novel solution to adapt generic object\\ndetection pipeline to pedestrian detection by optimizing most\\nof its stages [59]. Hu et al. [208] trained an ensemble of\\nboosted decision models by reusing the conv feature maps, and\\na further improvement was gained with simple pixel labelling\\nand additional complementary hand-crafted features. Tome\\net al. [209] proposed a reduced memory region based deep\\nCNN architecture, which fuses regional responses from both\\nACF detectors and SVM classiÔ¨Åers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian dataset [3]. The dataset was collected from the\\nvideos of a vehicle driving through an urban environment\\nand consists of 250,000 frames with about 2300 unique\\npedestrians and 350,000 annotated bounding boxes (BBs).\\nThree kinds of labels, namely ‚ÄòPerson (clear identiÔ¨Åcations)‚Äô,\\n‚ÄòPerson? (unclear identiÔ¨Åcations)‚Äô and ‚ÄòPeople (large group of\\nindividuals)‚Äô, are assigned to different BBs. The performance\\nis measured with the log-average miss rate (L-AMR) which\\nis computed evenly spaced in log-space in the range 10‚àí2 to\\n1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings\\nare adopted to evaluate different properties of these models.\\nDetails of these settings are as [3].\\nEvaluated methods include Checkerboards+ [198], LDCF++\\n[S2], SCF+AlexNet [210], SA-FastRCNN [211], MS-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 15, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n16\\n[105], DeepParts [204], CompACT-Deep [195], RPN+BF\\n[203] and F-DNN+SS [207]. The Ô¨Årst two methods are based\\non hand-crafted features while the rest ones rely on deep CNN\\nfeatures. All results are exhibited in Table VII. From this table,\\nwe observe that different from other tasks, classic handcrafted\\nfeatures can still earn competitive results with boosted decision\\nforests [203], ACF [197] and HOG+LUV channels [S2]. As\\nan early attempt to adapt CNN to pedestrian detection, the\\nfeatures generated by SCF+AlexNet are not so discriminant\\nand produce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to\\ncomplexity, it is too time-consuming to achieve real-time de-\\ntection. The multi-scale representation of MS-CNN improves\\naccuracy of pedestrian locations. SA-FastRCNN extends Fast\\nR-CNN to automatically detecting pedestrians according to\\ntheir different scales, which has trouble when there are partial\\nocclusions. RPN+BF combines the detectors produced by\\nFaster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN+SS, which is composed\\nof multiple parallel classiÔ¨Åers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN and MS-CNN.\\nIn short, CNN based methods can provide more accurate\\ncandidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classiÔ¨Åers, extracting multi-scale and part based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. PROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor future work.\\nThe Ô¨Årst one is small object detection such as occurring\\nin COCO dataset and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n‚Ä¢ Multi-task joint optimization and multi-modal infor-\\nmation fusion. Due to the correlations between different\\ntasks within and outside object detection, multi-task joint\\noptimization has already been studied by many researchers\\n[16] [18]. However, apart from the tasks mentioned in\\nSubs. III-A8, it is desirable to think over the characteristics\\nof different sub-tasks of object detection (e.g. superpixel\\nsemantic segmentation in salient object detection) and ex-\\ntend multi-task optimization to other applications such as\\ninstance segmentation [66], multi-object tracking [202] and\\nmulti-person pose estimation [S4]. Besides, given a speciÔ¨Åc\\napplication, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.\\n‚Ä¢ Scale adaption. Objects usually exist in different scales,\\nwhich is more apparent in face detection and pedestrian\\ndetection. To increase the robustness to scale changes, it\\nis demanded to train scale-invariant, multi-scale or scale-\\nadaptive detectors. For scale-invariant detectors, more pow-\\nerful backbone architectures (e.g. ResNext [123]), negative\\nsample mining [113], reverse connection [213] and sub-\\ncategory modelling [60] are all beneÔ¨Åcial. For multi-scale\\ndetectors, both the FPN [66] which produces multi-scale\\nfeature maps and Generative Adversarial Network [214]\\nwhich narrows representation differences between small ob-\\njects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge\\ngraph [215], attentional mechanism [216], cascade network\\n[180] and scale distribution estimation [171] to detect ob-\\njects adaptively.\\n‚Ä¢ Spatial correlations and contextual modelling. Spatial\\ndistribution plays an important role in object detection. So\\nregion proposal generation and grid regression are taken\\nto obtain probable object locations. However, the corre-\\nlations between multiple proposals and object categories\\nare ignored. Besides, the global structure information is\\nabandoned by the position-sensitive score maps in R-FCN.\\nTo solve these problems, we can refer to diverse subset\\nselection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor and\\naccomplish real-time object detection, with the emergence of\\nlarge-scale image and video data. The following three aspects\\ncan be taken into account.\\n‚Ä¢ Cascade network. In a cascade network, a cascade of\\ndetectors are built in different stages or layers [180], [220].\\nAnd easily distinguishable examples are rejected at shallow\\nlayers so that features and classiÔ¨Åers at latter stages can\\nhandle more difÔ¨Åcult samples with the aid of the decisions\\nfrom previous stages. However, current cascades are built in\\na greedy manner, where previous stages in cascade are Ô¨Åxed\\nwhen training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it\\nis also a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n‚Ä¢ Unsupervised and weakly supervised learning. It‚Äôs\\nvery time consuming to manually draw large quantities\\nof bounding boxes. To release this burden, semantic prior\\n[55], unsupervised object discovery [221], multiple instance\\nlearning [222] and deep neural network prediction [47] can\\nbe integrated to make best use of image-level supervision to\\nassign object category tags to corresponding object regions\\nand reÔ¨Åne object boundaries. Furthermore, weakly annota-\\ntions (e.g. center-click annotations [223]) are also helpful\\nfor achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n‚Ä¢ Network optimization. Given speciÔ¨Åc applications and\\nplatforms, it is signiÔ¨Åcant to make a balance among speed,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 16, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n17\\nmemory and accuracy by selecting an optimal detection\\narchitecture [116], [224]. However, despite that detection\\naccuracy is reduced, it is more meaningful to learn compact\\nmodels with fewer number of parameters [209]. And this\\nsituation can be relieved by introducing better pre-training\\nschemes [225], knowledge distillation [226] and hint learn-\\ning [227]. DSOD also provides a promising guideline to\\ntrain from scratch to bridge the gap between different image\\nsources and tasks [74].\\nThe third one is to extend typical methods for 2D object de-\\ntection to adapt 3D object detection and video object detection,\\nwith the requirements from autonomous driving, intelligent\\ntransportation and intelligent surveillance.\\n‚Ä¢ 3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can\\nbe utilized to better understand the images in 2D and extend\\nthe image-level knowledge to the real world. However,\\nseldom of these 3D-aware techniques aim to place correct\\n3D bounding boxes around detected objects. To achieve\\nbetter bounding results, multi-view representation [181] and\\n3D proposal network [228] may provide some guidelines to\\nencode depth information with the aid of inertial sensors\\n(accelerometer and gyrometer) [229].\\n‚Ä¢ Video object detection. Temporal information across\\ndifferent frames play an important role in understanding\\nthe behaviors of different objects. However, the accuracy\\nsuffers from degenerated object appearances (e.g., motion\\nblur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical Ô¨Çow [199] and LSTM [107] should\\nbe considered to fundamentally model object associations\\nbetween consecutive frames.\\nVIII. CONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation and background\\nswitches, deep learning based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning based object detection frameworks\\nwhich handle different sub-problems, such as occlusion, clutter\\nand low resolution, with different degrees of modiÔ¨Åcations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieÔ¨Çy reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object\\ndetection landscape. This review is also meaningful for the\\ndevelopments in neural networks and related learning systems,\\nwhich provides valuable insights and guidelines for future\\nprogress.\\nACKNOWLEDGMENTS\\nThis research was supported by the National Natural Sci-\\nence Foundation of China (No.61672203 & 61375047 &\\n91746209), the National Key Research and Development Pro-\\ngram of China (2016YFB1000901), and Anhui Natural Sci-\\nence Funds for Distinguished Young Scholar (No.170808J08).\\nREFERENCES\\n[1] P. F. Felzenszwalb, R. B. Girshick, D. Mcallester, and D. Ramanan,\\n‚ÄúObject detection with discriminatively trained part-based models,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, p. 1627, 2010.\\n[2] K. K. Sung and T. Poggio, ‚ÄúExample-based learning for view-based\\nhuman face detection,‚Äù IEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39‚Äì51, 2002.\\n[3] C. Wojek, P. Dollar, B. Schiele, and P. Perona, ‚ÄúPedestrian detection:\\nAn evaluation of the state of the art,‚Äù IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 34, no. 4, p. 743, 2012.\\n[4] H. Kobatake and Y. Yoshinaga, ‚ÄúDetection of spicules on mammogram\\nbased on skeleton analysis.‚Äù IEEE Trans. Med. Imag., vol. 15, no. 3,\\npp. 235‚Äì245, 1996.\\n[5] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell, ‚ÄúCaffe: Convolutional architecture for\\nfast feature embedding,‚Äù in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation\\nwith deep convolutional neural networks,‚Äù in NIPS, 2012.\\n[7] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, ‚ÄúRealtime multi-person\\n2d pose estimation using part afÔ¨Ånity Ô¨Åelds,‚Äù in CVPR, 2017.\\n[8] Z. Yang and R. Nevatia, ‚ÄúA multi-scale cascade fully convolutional\\nnetwork face detector,‚Äù in ICPR, 2016.\\n[9] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, ‚ÄúDeepdriving:\\nLearning affordance for direct perception in autonomous driving,‚Äù in\\nICCV, 2015.\\n[10] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, ‚ÄúMulti-view 3d object\\ndetection network for autonomous driving,‚Äù in CVPR, 2017.\\n[11] A. Dundar, J. Jin, B. Martini, and E. Culurciello, ‚ÄúEmbedded streaming\\ndeep neural networks accelerator with applications,‚Äù IEEE Trans.\\nNeural Netw. & Learning Syst., vol. 28, no. 7, pp. 1572‚Äì1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, ‚ÄúLow-complexity\\napproximate convolutional neural networks,‚Äù IEEE Trans. Neural Netw.\\n& Learning Syst., vol. PP, no. 99, pp. 1‚Äì12, 2018.\\n[13] S. H. Khan, M. Hayat, M. Bennamoun, F. A. Sohel, and R. Togneri,\\n‚ÄúCost-sensitive learning of deep feature representations from imbal-\\nanced data.‚Äù IEEE Trans. Neural Netw. & Learning Syst., vol. PP,\\nno. 99, pp. 1‚Äì15, 2017.\\n[14] A. Stuhlsatz, J. Lippel, and T. Zielke, ‚ÄúFeature extraction with deep\\nneural networks by a generalized discriminant analysis.‚Äù IEEE Trans.\\nNeural Netw. & Learning Syst., vol. 23, no. 4, pp. 596‚Äì608, 2012.\\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik, ‚ÄúRich feature\\nhierarchies for accurate object detection and semantic segmentation,‚Äù\\nin CVPR, 2014.\\n[16] R. Girshick, ‚ÄúFast r-cnn,‚Äù in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ‚ÄúYou only look\\nonce: UniÔ¨Åed, real-time object detection,‚Äù in CVPR, 2016.\\n[18] S. Ren, K. He, R. Girshick, and J. Sun, ‚ÄúFaster r-cnn: Towards real-\\ntime object detection with region proposal networks,‚Äù in NIPS, 2015,\\npp. 91‚Äì99.\\n[19] D. G. Lowe, ‚ÄúDistinctive image features from scale-invariant key-\\npoints,‚Äù Int. J. of Comput. Vision, vol. 60, no. 2, pp. 91‚Äì110, 2004.\\n[20] N. Dalal and B. Triggs, ‚ÄúHistograms of oriented gradients for human\\ndetection,‚Äù in CVPR, 2005.\\n[21] R. Lienhart and J. Maydt, ‚ÄúAn extended set of haar-like features for\\nrapid object detection,‚Äù in ICIP, 2002.\\n[22] C. Cortes and V. Vapnik, ‚ÄúSupport vector machine,‚Äù Machine Learning,\\nvol. 20, no. 3, pp. 273‚Äì297, 1995.\\n[23] Y. Freund and R. E. Schapire, ‚ÄúA desicion-theoretic generalization of\\non-line learning and an application to boosting,‚Äù J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663‚Äì671, 1997.\\n[24] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,\\n‚ÄúObject detection with discriminatively trained part-based models,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1627‚Äì1645, 2010.\\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman, ‚ÄúThe pascal visual object classes challenge 2007 (voc 2007)\\nresults (2007),‚Äù 2008.\\n[26] Y. LeCun, Y. Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù Nature, vol.\\n521, no. 7553, pp. 436‚Äì444, 2015.\\n[27] N. Liu, J. Han, D. Zhang, S. Wen, and T. Liu, ‚ÄúPredicting eye Ô¨Åxations\\nusing convolutional neural networks,‚Äù in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, ‚ÄúLarge-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,‚Äù in CVPR, 2014.\\n[29] H. Jiang and E. Learned-Miller, ‚ÄúFace detection with the faster r-cnn,‚Äù\\nin FG, 2017.\\n[30] D. Chen, S. Ren, Y. Wei, X. Cao, and J. Sun, ‚ÄúJoint cascade face\\ndetection and alignment,‚Äù in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 17, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n18\\n[31] D. Chen, G. Hua, F. Wen, and J. Sun, ‚ÄúSupervised transformer network\\nfor efÔ¨Åcient face detection,‚Äù in ECCV, 2016.\\n[32] D. Ribeiro, A. Mateus, J. C. Nascimento, and P. Miraldo, ‚ÄúA real-time\\npedestrian detector using deep learning for human-aware navigation,‚Äù\\narXiv:1607.04441, 2016.\\n[33] F. Yang, W. Choi, and Y. Lin, ‚ÄúExploit all the layers: Fast and accurate\\ncnn object detector with scale dependent pooling and cascaded rejection\\nclassiÔ¨Åers,‚Äù in CVPR, 2016.\\n[34] P. Druzhkov and V. Kustikova, ‚ÄúA survey of deep learning methods and\\nsoftware tools for image classiÔ¨Åcation and object detection,‚Äù Pattern\\nRecognition and Image Anal., vol. 26, no. 1, p. 9, 2016.\\n[35] W. Pitts and W. S. McCulloch, ‚ÄúHow we know universals the perception\\nof auditory and visual forms,‚Äù The Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127‚Äì147, 1947.\\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ‚ÄúLearning internal\\nrepresentation by back-propagation of errors,‚Äù Nature, vol. 323, no.\\n323, pp. 533‚Äì536, 1986.\\n[37] G. E. Hinton and R. R. Salakhutdinov, ‚ÄúReducing the dimensionality\\nof data with neural networks,‚Äù Sci., vol. 313, pp. 504‚Äì507, 2006.\\n[38] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\\nA. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., ‚ÄúDeep neural\\nnetworks for acoustic modeling in speech recognition: The shared\\nviews of four research groups,‚Äù IEEE Signal Process. Mag., vol. 29,\\nno. 6, pp. 82‚Äì97, 2012.\\n[39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet:\\nA large-scale hierarchical image database,‚Äù in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and\\nG. Hinton, ‚ÄúBinary coding of speech spectrograms using a deep auto-\\nencoder,‚Äù in INTERSPEECH, 2010.\\n[41] G. Dahl, A.-r. Mohamed, G. E. Hinton et al., ‚ÄúPhone recognition with\\nthe mean-covariance restricted boltzmann machine,‚Äù in NIPS, 2010.\\n[42] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov, ‚ÄúImproving neural networks by preventing co-\\nadaptation of feature detectors,‚Äù arXiv:1207.0580, 2012.\\n[43] S. Ioffe and C. Szegedy, ‚ÄúBatch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,‚Äù in ICML, 2015.\\n[44] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun,\\n‚ÄúOverfeat: Integrated recognition, localization and detection using\\nconvolutional networks,‚Äù arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V. Vanhoucke, and A. Rabinovich, ‚ÄúGoing deeper with\\nconvolutions,‚Äù in CVPR, 2015.\\n[46] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks\\nfor large-scale image recognition,‚Äù arXiv:1409.1556, 2014.\\n[47] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\\nrecognition,‚Äù in CVPR, 2016.\\n[48] V. Nair and G. E. Hinton, ‚ÄúRectiÔ¨Åed linear units improve restricted\\nboltzmann machines,‚Äù in ICML, 2010.\\n[49] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al., ‚ÄúWeakly supervised\\nobject recognition with convolutional neural networks,‚Äù in NIPS, 2014.\\n[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, ‚ÄúLearning and transferring\\nmid-level image representations using convolutional neural networks,‚Äù\\nin CVPR, 2014.\\n[51] F. M. Wadley, ‚ÄúProbit analysis: a statistical treatment of the sigmoid\\nresponse curve,‚Äù Annals of the Entomological Soc. of America, vol. 67,\\nno. 4, pp. 549‚Äì553, 1947.\\n[52] K. Kavukcuoglu, R. Fergus, Y. LeCun et al., ‚ÄúLearning invariant\\nfeatures through topographic Ô¨Ålter maps,‚Äù in CVPR, 2009.\\n[53] K. Kavukcuoglu, P. Sermanet, Y.-L. Boureau, K. Gregor, M. Mathieu,\\nand Y. LeCun, ‚ÄúLearning convolutional feature hierarchies for visual\\nrecognition,‚Äù in NIPS, 2010.\\n[54] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, ‚ÄúDeconvolu-\\ntional networks,‚Äù in CVPR, 2010.\\n[55] H. Noh, S. Hong, and B. Han, ‚ÄúLearning deconvolution network for\\nsemantic segmentation,‚Äù in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y.-m. Cheung, and X. Wu, ‚ÄúPlant leaf iden-\\ntiÔ¨Åcation via a growing convolution neural network with progressive\\nsample learning,‚Äù in ACCV, 2014.\\n[57] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky, ‚ÄúNeural codes\\nfor image retrieval,‚Äù in ECCV, 2014.\\n[58] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y. Zhang, and J. Li,\\n‚ÄúDeep learning for content-based image retrieval: A comprehensive\\nstudy,‚Äù in ACM MM, 2014.\\n[59] D. Tom`e, F. Monti, L. BarofÔ¨Åo, L. Bondi, M. Tagliasacchi, and\\nS. Tubaro, ‚ÄúDeep convolutional neural networks for pedestrian detec-\\ntion,‚Äù Signal Process.: Image Commun., vol. 47, pp. 482‚Äì489, 2016.\\n[60] Y. Xiang, W. Choi, Y. Lin, and S. Savarese, ‚ÄúSubcategory-aware\\nconvolutional neural networks for object proposals and detection,‚Äù in\\nWACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, ‚ÄúPedestrian\\ndetection based on fast r-cnn and batch normalization,‚Äù in ICIC, 2017.\\n[62] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,\\n‚ÄúMultimodal deep learning,‚Äù in ICML, 2011.\\n[63] Z. Wu, X. Wang, Y.-G. Jiang, H. Ye, and X. Xue, ‚ÄúModeling spatial-\\ntemporal clues in a hybrid deep learning framework for video classiÔ¨Å-\\ncation,‚Äù in ACM MM, 2015.\\n[64] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúSpatial pyramid pooling in deep\\nconvolutional networks for visual recognition,‚Äù IEEE Trans. Pattern\\nAnal. Mach. Intell., vol. 37, no. 9, pp. 1904‚Äì1916, 2015.\\n[65] Y. Li, K. He, J. Sun et al., ‚ÄúR-fcn: Object detection via region-based\\nfully convolutional networks,‚Äù in NIPS, 2016, pp. 379‚Äì387.\\n[66] T.-Y. Lin, P. Doll¬¥ar, R. B. Girshick, K. He, B. Hariharan, and S. J.\\nBelongie, ‚ÄúFeature pyramid networks for object detection,‚Äù in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll¬¥ar, and R. B. Girshick, ‚ÄúMask r-cnn,‚Äù in\\nICCV, 2017.\\n[68] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, ‚ÄúScalable object\\ndetection using deep neural networks,‚Äù in CVPR, 2014.\\n[69] D. Yoo, S. Park, J.-Y. Lee, A. S. Paek, and I. So Kweon, ‚ÄúAttentionnet:\\nAggregating weak directions for accurate object detection,‚Äù in CVPR,\\n2015.\\n[70] M. Najibi, M. Rastegari, and L. S. Davis, ‚ÄúG-cnn: an iterative grid\\nbased object detector,‚Äù in CVPR, 2016.\\n[71] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and\\nA. C. Berg, ‚ÄúSsd: Single shot multibox detector,‚Äù in ECCV, 2016.\\n[72] J. Redmon and A. Farhadi, ‚ÄúYolo9000: better, faster, stronger,‚Äù\\narXiv:1612.08242, 2016.\\n[73] C. Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, ‚ÄúDssd:\\nDeconvolutional single shot detector,‚Äù arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y. G. Jiang, Y. Chen, and X. Xue, ‚ÄúDsod:\\nLearning deeply supervised object detectors from scratch,‚Äù in ICCV,\\n2017.\\n[75] G. E. Hinton, A. Krizhevsky, and S. D. Wang, ‚ÄúTransforming auto-\\nencoders,‚Äù in ICANN, 2011.\\n[76] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus, ‚ÄúLearning invariance\\nthrough imitation,‚Äù in CVPR, 2011.\\n[77] X. Ren and D. Ramanan, ‚ÄúHistograms of sparse codes for object\\ndetection,‚Äù in CVPR, 2013.\\n[78] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\\n‚ÄúSelective search for object recognition,‚Äù Int. J. of Comput. Vision, vol.\\n104, no. 2, pp. 154‚Äì171, 2013.\\n[79] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun, ‚ÄúPedestrian\\ndetection with unsupervised multi-stage feature learning,‚Äù in CVPR,\\n2013.\\n[80] P. Kr¬®ahenb¬®uhl and V. Koltun, ‚ÄúGeodesic object proposals,‚Äù in ECCV,\\n2014.\\n[81] P. Arbel¬¥aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\\n‚ÄúMultiscale combinatorial grouping,‚Äù in CVPR, 2014.\\n[82] C. L. Zitnick and P. Doll¬¥ar, ‚ÄúEdge boxes: Locating object proposals\\nfrom edges,‚Äù in ECCV, 2014.\\n[83] W. Kuo, B. Hariharan, and J. Malik, ‚ÄúDeepbox: Learning objectness\\nwith convolutional networks,‚Äù in ICCV, 2015.\\n[84] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll¬¥ar, ‚ÄúLearning to\\nreÔ¨Åne object segments,‚Äù in ECCV, 2016.\\n[85] Y. Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, ‚ÄúImproving object\\ndetection with deep convolutional networks via bayesian optimization\\nand structured prediction,‚Äù in CVPR, 2015.\\n[86] S. Gupta, R. Girshick, P. Arbel¬¥aez, and J. Malik, ‚ÄúLearning rich features\\nfrom rgb-d images for object detection and segmentation,‚Äù in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li, S. Yang,\\nZ. Wang, C.-C. Loy et al., ‚ÄúDeepid-net: Deformable deep convolutional\\nneural networks for object detection,‚Äù in CVPR, 2015.\\n[88] K. Lenc and A. Vedaldi, ‚ÄúR-cnn minus r,‚Äù arXiv:1506.06981, 2015.\\n[89] S. Lazebnik, C. Schmid, and J. Ponce, ‚ÄúBeyond bags of features:\\nSpatial pyramid matching for recognizing natural scene categories,‚Äù\\nin CVPR, 2006.\\n[90] F. Perronnin, J. S¬¥anchez, and T. Mensink, ‚ÄúImproving the Ô¨Åsher kernel\\nfor large-scale image classiÔ¨Åcation,‚Äù in ECCV, 2010.\\n[91] J. Xue, J. Li, and Y. Gong, ‚ÄúRestructuring of deep neural network\\nacoustic models with singular value decomposition.‚Äù in Interspeech,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 18, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n19\\n[92] S. Ren, K. He, R. Girshick, and J. Sun, ‚ÄúFaster r-cnn: Towards real-time\\nobject detection with region proposal networks,‚Äù IEEE Trans. Pattern\\nAnal. Mach. Intell., vol. 39, no. 6, pp. 1137‚Äì1149, 2017.\\n[93] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‚ÄúRethink-\\ning the inception architecture for computer vision,‚Äù in CVPR, 2016.\\n[94] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft coco: Common objects in\\ncontext,‚Äù in ECCV, 2014.\\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, ‚ÄúInside-outside\\nnet: Detecting objects in context with skip pooling and recurrent neural\\nnetworks,‚Äù in CVPR, 2016.\\n[96] A. Arnab and P. H. S. Torr, ‚ÄúPixelwise instance segmentation with a\\ndynamically instantiated network,‚Äù in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, ‚ÄúInstance-aware semantic segmentation via\\nmulti-task network cascades,‚Äù in CVPR, 2016.\\n[98] Y. Li, H. Qi, J. Dai, X. Ji, and Y. Wei, ‚ÄúFully convolutional instance-\\naware semantic segmentation,‚Äù in CVPR, 2017.\\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n‚ÄúSpatial transformer networks,‚Äù in CVPR, 2015.\\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, ‚ÄúStuffnet: Using stuffto\\nimprove object detection,‚Äù in WACV, 2017.\\n[101] T. Kong, A. Yao, Y. Chen, and F. Sun, ‚ÄúHypernet: Towards accurate\\nregion proposal generation and joint object detection,‚Äù in CVPR, 2016.\\n[102] A. Pentina, V. Sharmanska, and C. H. Lampert, ‚ÄúCurriculum learning\\nof multiple tasks,‚Äù in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, ‚ÄúRotating your\\nface using multi-task deep neural network,‚Äù in CVPR, 2015.\\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, ‚ÄúMulti-stage object\\ndetection with group recursive learning,‚Äù arXiv:1608.05159, 2016.\\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, ‚ÄúA uniÔ¨Åed multi-scale\\ndeep convolutional neural network for fast object detection,‚Äù in ECCV,\\n2016.\\n[106] Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, ‚Äúsegdeepm:\\nExploiting segmentation and context in deep neural networks for object\\ndetection,‚Äù in CVPR, 2015.\\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, ‚ÄúScene labeling\\nwith lstm recurrent neural networks,‚Äù in CVPR, 2015.\\n[108] B. Moysset, C. Kermorvant, and C. Wolf, ‚ÄúLearning to detect and\\nlocalize many objects from few examples,‚Äù arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, ‚ÄúGated bi-\\ndirectional cnn for object detection,‚Äù in ECCV, 2016.\\n[110] S. Gidaris and N. Komodakis, ‚ÄúObject detection via a multi-region and\\nsemantic segmentation-aware cnn model,‚Äù in CVPR, 2015.\\n[111] M. Schuster and K. K. Paliwal, ‚ÄúBidirectional recurrent neural net-\\nworks,‚Äù IEEE Trans. Signal Process., vol. 45, pp. 2673‚Äì2681, 1997.\\n[112] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chin-\\ntala, and P. Doll¬¥ar, ‚ÄúA multipath network for object detection,‚Äù\\narXiv:1604.02135, 2016.\\n[113] A. Shrivastava, A. Gupta, and R. Girshick, ‚ÄúTraining region-based\\nobject detectors with online hard example mining,‚Äù in CVPR, 2016.\\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, ‚ÄúObject detection\\nnetworks on convolutional feature maps,‚Äù IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476‚Äì1481, 2017.\\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, ‚ÄúFactors in Ô¨Ånetuning\\ndeep model for object detection with long-tail distribution,‚Äù in CVPR,\\n2016.\\n[116] S. Hong, B. Roh, K.-H. Kim, Y. Cheon, and M. Park, ‚ÄúPvanet:\\nLightweight deep neural networks for real-time object detection,‚Äù\\narXiv:1611.08588, 2016.\\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, ‚ÄúUnderstanding and\\nimproving convolutional neural networks via concatenated rectiÔ¨Åed\\nlinear units,‚Äù in ICML, 2016.\\n[118] C. Szegedy, A. Toshev, and D. Erhan, ‚ÄúDeep neural networks for object\\ndetection,‚Äù in NIPS, 2013.\\n[119] P. O. Pinheiro, R. Collobert, and P. Doll¬¥ar, ‚ÄúLearning to segment object\\ncandidates,‚Äù in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, ‚ÄúScalable,\\nhigh-quality object detection,‚Äù arXiv:1412.1441, 2014.\\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\\n‚ÄúThe pascal visual object classes challenge 2012 (voc2012) results\\n(2012),‚Äù in http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html, 2011.\\n[122] M. D. Zeiler and R. Fergus, ‚ÄúVisualizing and understanding convolu-\\ntional networks,‚Äù in ECCV, 2014.\\n[123] S. Xie, R. B. Girshick, P. Doll¬¥ar, Z. Tu, and K. He, ‚ÄúAggregated residual\\ntransformations for deep neural networks,‚Äù in CVPR, 2017.\\n[124] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\\n‚ÄúDeformable convolutional networks,‚Äù arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y. Hamadi, and A. Blake, ‚ÄúAutocollage,‚Äù ACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847‚Äì852, 2006.\\n[126] C. Jung and C. Kim, ‚ÄúA uniÔ¨Åed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,‚Äù IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272‚Äì1283, 2012.\\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y. Chien, ‚ÄúReal-time salient object\\ndetection with a minimum spanning tree,‚Äù in CVPR, 2016.\\n[128] J. Yang and M.-H. Yang, ‚ÄúTop-down visual saliency via joint crf and\\ndictionary learning,‚Äù IEEE Trans. Pattern Anal. Mach. Intell., vol. 39,\\nno. 3, pp. 576‚Äì588, 2017.\\n[129] P. L. Rosin, ‚ÄúA simple method for detecting salient regions,‚Äù Pattern\\nRecognition, vol. 42, no. 11, pp. 2363‚Äì2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y. Shum,\\n‚ÄúLearning to detect a salient object,‚Äù IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 33, no. 2, pp. 353‚Äì367, 2011.\\n[131] J. Long, E. Shelhamer, and T. Darrell, ‚ÄúFully convolutional networks\\nfor semantic segmentation,‚Äù in CVPR, 2015.\\n[132] D. Gao, S. Han, and N. Vasconcelos, ‚ÄúDiscriminant saliency, the detec-\\ntion of suspicious coincidences, and applications to visual recognition,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 31, pp. 989‚Äì1005, 2009.\\n[133] S. Xie and Z. Tu, ‚ÄúHolistically-nested edge detection,‚Äù in ICCV, 2015.\\n[134] M. K¬®ummerer, L. Theis, and M. Bethge, ‚ÄúDeep gaze i: Boost-\\ning saliency prediction with feature maps trained on imagenet,‚Äù\\narXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, ‚ÄúSalicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,‚Äù\\nin ICCV, 2015.\\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, ‚ÄúDeep networks for saliency\\ndetection via local estimation and global search,‚Äù in CVPR, 2015.\\n[137] H. Cholakkal, J. Johnson, and D. Rajan, ‚ÄúWeakly supervised top-down\\nsalient object detection,‚Äù arXiv:1611.05345, 2016.\\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, ‚ÄúSaliency detection by\\nmulti-context deep learning,‚Äù in CVPR, 2015.\\n[139] C¬∏ . Bak, A. Erdem, and E. Erdem, ‚ÄúTwo-stream convolutional networks\\nfor dynamic saliency prediction,‚Äù arXiv:1607.04730, 2016.\\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, ‚ÄúSupercnn: A su-\\nperpixelwise convolutional neural network for salient object detection,‚Äù\\nInt. J. of Comput. Vision, vol. 115, no. 3, pp. 330‚Äì344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y. Zhuang, H. Ling, and\\nJ. Wang, ‚ÄúDeepsaliency: Multi-task deep neural network model for\\nsalient object detection,‚Äù IEEE Trans. Image Process., vol. 25, no. 8,\\npp. 3919‚Äì3930, 2016.\\n[142] Y. Tang and X. Wu, ‚ÄúSaliency detection via combining region-level\\nand pixel-level predictions with cnns,‚Äù in ECCV, 2016.\\n[143] G. Li and Y. Yu, ‚ÄúDeep contrast learning for salient object detection,‚Äù\\nin CVPR, 2016.\\n[144] X. Wang, H. Ma, S. You, and X. Chen, ‚ÄúEdge preserving and\\nmulti-scale contextual neural network for salient object detection,‚Äù\\narXiv:1608.08029, 2016.\\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, ‚ÄúA deep multi-level\\nnetwork for saliency prediction,‚Äù in ICPR, 2016.\\n[146] G. Li and Y. Yu, ‚ÄúVisual saliency detection based on multiscale deep\\ncnn features,‚Äù IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012‚Äì\\n5024, 2016.\\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O‚ÄôConnor,\\n‚ÄúShallow and deep convolutional networks for saliency prediction,‚Äù in\\nCVPR, 2016.\\n[148] J. Kuen, Z. Wang, and G. Wang, ‚ÄúRecurrent attentional networks for\\nsaliency detection,‚Äù in CVPR, 2016.\\n[149] Y. Tang, X. Wu, and W. Bu, ‚ÄúDeeply-supervised recurrent convolutional\\nneural network for saliency detection,‚Äù in ACM MM, 2016.\\n[150] X. Li, Y. Li, C. Shen, A. Dick, and A. Van Den Hengel, ‚ÄúContextual\\nhypergraph modeling for salient object detection,‚Äù in ICCV, 2013.\\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, ‚ÄúGlobal\\ncontrast based salient region detection,‚Äù IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569‚Äì582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li, ‚ÄúSalient object\\ndetection: A discriminative regional feature integration approach,‚Äù in\\nCVPR, 2013.\\n[153] G. Lee, Y.-W. Tai, and J. Kim, ‚ÄúDeep saliency with encoded low level\\ndistance map and high level features,‚Äù in CVPR, 2016.\\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\\n‚ÄúNon-local deep features for salient object detection,‚Äù in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 19, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n20\\n[155] Q. Hou, M.-M. Cheng, X.-W. Hu, A. Borji, Z. Tu, and P. Torr,\\n‚ÄúDeeply supervised salient object detection with short connections,‚Äù\\narXiv:1611.04849, 2016.\\n[156] Q. Yan, L. Xu, J. Shi, and J. Jia, ‚ÄúHierarchical saliency detection,‚Äù in\\nCVPR, 2013.\\n[157] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, ‚ÄúThe secrets of\\nsalient object segmentation,‚Äù in CVPR, 2014.\\n[158] V. Movahedi and J. H. Elder, ‚ÄúDesign and perceptual validation of\\nperformance measures for salient object segmentation,‚Äù in CVPRW,\\n2010.\\n[159] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, ‚ÄúSalient object detection:\\nA benchmark,‚Äù IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706‚Äì\\n5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, ‚ÄúGraphical representation for\\nheterogeneous face recognition,‚Äù IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301‚Äì312, 2015.\\n[161] C. Peng, N. Wang, X. Gao, and J. Li, ‚ÄúFace recognition from multiple\\nstylistic sketches: Scenarios, datasets, and evaluation,‚Äù in ECCV, 2016.\\n[162] X. Gao, N. Wang, D. Tao, and X. Li, ‚ÄúFace sketchcphoto synthesis\\nand retrieval using sparse representation,‚Äù IEEE Trans. Circuits Syst.\\nVideo Technol., vol. 22, no. 8, pp. 1213‚Äì1226, 2012.\\n[163] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, ‚ÄúA comprehensive survey\\nto face hallucination,‚Äù Int. J. of Comput. Vision, vol. 106, no. 1, pp.\\n9‚Äì30, 2014.\\n[164] C. Peng, X. Gao, N. Wang, D. Tao, X. Li, and J. Li, ‚ÄúMultiple\\nrepresentations-based face sketch-photo synthesis.‚Äù IEEE Trans. Neural\\nNetw. & Learning Syst., vol. 27, no. 11, pp. 2201‚Äì2215, 2016.\\n[165] A. Majumder, L. Behera, and V. K. Subramanian, ‚ÄúAutomatic facial\\nexpression recognition system using deep network-based data fusion,‚Äù\\nIEEE Trans. Cybern., vol. 48, pp. 103‚Äì114, 2018.\\n[166] P. Viola and M. Jones, ‚ÄúRobust real-time face detection,‚Äù Int. J. of\\nComput. Vision, vol. 57, no. 2, pp. 137‚Äì154, 2004.\\n[167] J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. Huang, ‚ÄúUnitbox: An advanced\\nobject detection network,‚Äù in ACM MM, 2016.\\n[168] S. S. Farfade, M. J. Saberian, and L.-J. Li, ‚ÄúMulti-view face detection\\nusing deep convolutional neural networks,‚Äù in ICMR, 2015.\\n[169] S. Yang, P. Luo, C.-C. Loy, and X. Tang, ‚ÄúFrom facial parts responses\\nto face detection: A deep learning approach,‚Äù in ICCV, 2015.\\n[170] S. Yang, Y. Xiong, C. C. Loy, and X. Tang, ‚ÄúFace detection through\\nscale-friendly deep convolutional networks,‚Äù in CVPR, 2017.\\n[171] Z. Hao, Y. Liu, H. Qin, J. Yan, X. Li, and X. Hu, ‚ÄúScale-aware face\\ndetection,‚Äù in CVPR, 2017.\\n[172] H. Wang, Z. Li, X. Ji, and Y. Wang, ‚ÄúFace r-cnn,‚Äù arXiv:1706.01061,\\n2017.\\n[173] X. Sun, P. Wu, and S. C. Hoi, ‚ÄúFace detection using deep learning: An\\nimproved faster rcnn approach,‚Äù arXiv:1701.08289, 2017.\\n[174] L. Huang, Y. Yang, Y. Deng, and Y. Yu, ‚ÄúDensebox: Unifying landmark\\nlocalization with end to end object detection,‚Äù arXiv:1509.04874, 2015.\\n[175] Y. Li, B. Sun, T. Wu, and Y. Wang, ‚Äúface detection with end-to-end\\nintegration of a convnet and a 3d model,‚Äù in ECCV, 2016.\\n[176] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, ‚ÄúJoint face detection and\\nalignment using multitask cascaded convolutional networks,‚Äù IEEE\\nSignal Process. Lett., vol. 23, no. 10, pp. 1499‚Äì1503, 2016.\\n[177] I. A. Kalinovsky and V. G. Spitsyn, ‚ÄúCompact convolutional neural\\nnetwork cascadefor face detection,‚Äù in CEUR Workshop, 2016.\\n[178] H. Qin, J. Yan, X. Li, and X. Hu, ‚ÄúJoint training of cascaded cnn for\\nface detection,‚Äù in CVPR, 2016.\\n[179] V. Jain and E. Learned-Miller, ‚ÄúFddb: A benchmark for face detection\\nin unconstrained settings,‚Äù Tech. Rep., 2010.\\n[180] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, ‚ÄúA convolutional neural\\nnetwork cascade for face detection,‚Äù in CVPR, 2015.\\n[181] B. Yang, J. Yan, Z. Lei, and S. Z. Li, ‚ÄúAggregate channel features for\\nmulti-view face detection,‚Äù in IJCB, 2014.\\n[182] N. MarkuÀás, M. Frljak, I. S. PandÀázi¬¥c, J. Ahlberg, and R. Forchheimer,\\n‚ÄúObject detection with pixel intensity comparisons organized in deci-\\nsion trees,‚Äù arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, ‚ÄúFace\\ndetection without bells and whistles,‚Äù in ECCV, 2014.\\n[184] J. Li and Y. Zhang, ‚ÄúLearning surf cascade for fast and accurate object\\ndetection,‚Äù in CVPR, 2013.\\n[185] S. Liao, A. K. Jain, and S. Z. Li, ‚ÄúA fast and accurate unconstrained\\nface detector,‚Äù IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 2,\\npp. 211‚Äì223, 2016.\\n[186] B. Yang, J. Yan, Z. Lei, and S. Z. Li, ‚ÄúConvolutional channel features,‚Äù\\nin ICCV, 2015.\\n[187] R. Ranjan, V. M. Patel, and R. Chellappa, ‚ÄúHyperface: A deep multi-\\ntask learning framework for face detection, landmark localization, pose\\nestimation, and gender recognition,‚Äù arXiv:1603.01249, 2016.\\n[188] P. Hu and D. Ramanan, ‚ÄúFinding tiny faces,‚Äù in CVPR, 2017.\\n[189] Z. Jiang and D. Q. Huynh, ‚ÄúMultiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,‚Äù IEEE Trans.\\nImage Process., vol. 27, pp. 1361‚Äì1375, 2018.\\n[190] D. Gavrila and S. Munder, ‚ÄúMulti-cue pedestrian detection and tracking\\nfrom a moving vehicle,‚Äù Int. J. of Comput. Vision, vol. 73, pp. 41‚Äì59,\\n2006.\\n[191] S. Xu, Y. Cheng, K. Gu, Y. Yang, S. Chang, and P. Zhou, ‚ÄúJointly\\nattentive spatial-temporal pooling networks for video-based person re-\\nidentiÔ¨Åcation,‚Äù in ICCV, 2017.\\n[192] Z. Liu, D. Wang, and H. Lu, ‚ÄúStepwise metric promotion for unsuper-\\nvised video person re-identiÔ¨Åcation,‚Äù in ICCV, 2017.\\n[193] A. Khan, B. Rinner, and A. Cavallaro, ‚ÄúCooperative robots to observe\\nmoving targets: Review,‚Äù IEEE Trans. Cybern., vol. 48, pp. 187‚Äì198,\\n2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, ‚ÄúVision meets robotics:\\nThe kitti dataset,‚Äù Int. J. of Robotics Res., vol. 32, pp. 1231‚Äì1237,\\n2013.\\n[195] Z. Cai, M. Saberian, and N. Vasconcelos, ‚ÄúLearning complexity-aware\\ncascades for deep pedestrian detection,‚Äù in ICCV, 2015.\\n[196] Y. Tian, P. Luo, X. Wang, and X. Tang, ‚ÄúDeep learning strong parts\\nfor pedestrian detection,‚Äù in CVPR, 2015.\\n[197] P. Doll¬¥ar, R. Appel, S. Belongie, and P. Perona, ‚ÄúFast feature pyramids\\nfor object detection,‚Äù IEEE Trans. Pattern Anal. Mach. Intell., vol. 36,\\nno. 8, pp. 1532‚Äì1545, 2014.\\n[198] S. Zhang, R. Benenson, and B. Schiele, ‚ÄúFiltered channel features for\\npedestrian detection,‚Äù in CVPR, 2015.\\n[199] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, ‚ÄúPedestrian detec-\\ntion with spatially pooled features and structured ensemble learning,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243‚Äì1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, ‚ÄúDiscriminatively trained\\nand-or graph models for object shape detection,‚Äù IEEE Trans. Pattern\\nAnal. Mach. Intell., vol. 37, no. 5, pp. 959‚Äì972, 2015.\\n[201] M. Mathias, R. Benenson, R. Timofte, and L. Van Gool, ‚ÄúHandling\\nocclusions with franken-classiÔ¨Åers,‚Äù in ICCV, 2013.\\n[202] S. Tang, M. Andriluka, and B. Schiele, ‚ÄúDetection and tracking of\\noccluded people,‚Äù Int. J. of Comput. Vision, vol. 110, pp. 58‚Äì69, 2014.\\n[203] L. Zhang, L. Lin, X. Liang, and K. He, ‚ÄúIs faster r-cnn doing well for\\npedestrian detection?‚Äù in ECCV, 2016.\\n[204] Y. Tian, P. Luo, X. Wang, and X. Tang, ‚ÄúDeep learning strong parts\\nfor pedestrian detection,‚Äù in ICCV, 2015.\\n[205] J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, ‚ÄúMultispectral deep\\nneural networks for pedestrian detection,‚Äù arXiv:1611.02644, 2016.\\n[206] Y. Tian, P. Luo, X. Wang, and X. Tang, ‚ÄúPedestrian detection aided by\\ndeep learning semantic tasks,‚Äù in CVPR, 2015.\\n[207] X. Du, M. El-Khamy, J. Lee, and L. Davis, ‚ÄúFused dnn: A deep neural\\nnetwork fusion approach to fast and robust pedestrian detection,‚Äù in\\nWACV, 2017.\\n[208] Q. Hu, P. Wang, C. Shen, A. van den Hengel, and F. Porikli, ‚ÄúPushing\\nthe limits of deep cnns for pedestrian detection,‚Äù IEEE Trans. Circuits\\nSyst. Video Technol., 2017.\\n[209] D. Tom¬¥e, L. Bondi, L. BarofÔ¨Åo, S. Tubaro, E. Plebani, and D. Pau,\\n‚ÄúReduced memory region based deep convolutional neural network\\ndetection,‚Äù in ICCE-Berlin, 2016.\\n[210] J. Hosang, M. Omran, R. Benenson, and B. Schiele, ‚ÄúTaking a deeper\\nlook at pedestrians,‚Äù in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, ‚ÄúScale-aware fast\\nr-cnn for pedestrian detection,‚Äù arXiv:1510.08160, 2015.\\n[212] Y. Gao, M. Wang, Z.-J. Zha, J. Shen, X. Li, and X. Wu, ‚ÄúVisual-textual\\njoint relevance learning for tag-based social image search,‚Äù IEEE Trans.\\nImage Process., vol. 22, no. 1, pp. 363‚Äì376, 2013.\\n[213] T. Kong, F. Sun, A. Yao, H. Liu, M. Lv, and Y. Chen, ‚ÄúRon: Reverse\\nconnection with objectness prior networks for object detection,‚Äù in\\nCVPR, 2017.\\n[214] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. C. Courville, and Y. Bengio, ‚ÄúGenerative adversarial nets,‚Äù\\nin NIPS, 2014.\\n[215] Y. Fang, K. Kuan, J. Lin, C. Tan, and V. Chandrasekhar, ‚ÄúObject\\ndetection meets knowledge graphs,‚Äù in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, ‚ÄúSaliency-based sequential\\nimage attention with multiset prediction,‚Äù in NIPS, 2017.\\n[217] S. Azadi, J. Feng, and T. Darrell, ‚ÄúLearning detection with diverse\\nproposals,‚Äù in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 21, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'trapped': '', 'modDate': 'D:20190417004522Z', 'creationDate': 'D:20190417004522Z', 'page': 20, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\\n21\\n[218] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, ‚ÄúEnd-to-end\\nmemory networks,‚Äù in NIPS, 2015.\\n[219] P. Dabkowski and Y. Gal, ‚ÄúReal time image saliency for black box\\nclassiÔ¨Åers,‚Äù in NIPS, 2017.\\n[220] B. Yang, J. Yan, Z. Lei, and S. Z. Li, ‚ÄúCraft objects from images,‚Äù in\\nCVPR, 2016.\\n[221] I. Croitoru, S.-V. Bogolin, and M. Leordeanu, ‚ÄúUnsupervised learning\\nfrom video to detect foreground objects in single images,‚Äù in ICCV,\\n2017.\\n[222] C. Wang, W. Ren, K. Huang, and T. Tan, ‚ÄúWeakly supervised object\\nlocalization with latent category learning,‚Äù in ECCV, 2014.\\n[223] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V. Ferrari,\\n‚ÄúTraining object class detectors with click supervision,‚Äù in CVPR, 2017.\\n[224] J. Huang, V. Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y. S. Song, S. Guadarrama, and K. Murphy, ‚ÄúSpeed/accuracy\\ntrade-offs for modern convolutional object detectors,‚Äù in CVPR, 2017.\\n[225] Q. Li, S. Jin, and J. Yan, ‚ÄúMimicking very efÔ¨Åcient network for object\\ndetection,‚Äù in CVPR, 2017.\\n[226] G. Hinton, O. Vinyals, and J. Dean, ‚ÄúDistilling the knowledge in a\\nneural network,‚Äù Comput. Sci., vol. 14, no. 7, pp. 38‚Äì39, 2015.\\n[227] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY. Bengio, ‚ÄúFitnets: Hints for thin deep nets,‚Äù Comput. Sci., 2014.\\n[228] X. Chen, K. Kundu, Y. Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and\\nR. Urtasun, ‚Äú3d object proposals for accurate object class detection,‚Äù\\nin NIPS, 2015.\\n[229] J. Dong, X. Fei, and S. Soatto, ‚ÄúVisual-inertial-semantic scene repre-\\nsentation for 3d object detection,‚Äù in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,\\n‚ÄúObject detection in videos with tubelet proposal networks,‚Äù in CVPR,\\n2017.\\nZhong-Qiu Zhao is a professor at Hefei Univer-\\nsity of Technology, China. He obtained the Ph.D.\\ndegree in Pattern Recognition & Intelligent System\\nat University of Science and Technology, China, in\\n2007. From April 2008 to November 2009, he held a\\npostdoctoral position in image processing in CNRS\\nUMR6168 Lab Sciences de lInformation et des\\nSyst`emes, France. From January 2013 to December\\n2014, he held a research fellow position in image\\nprocessing at the Department of Computer Science\\nof Hongkong Baptist University, Hongkong, China.\\nHis research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his\\nBachelor‚Äôs degree in 2010 from Hefei University of\\nTechnology. His interests cover pattern recognition,\\nimage processing and computer vision.\\nShou-tao Xu is a Master student at Hefei University\\nof Technology. His research interests cover pattern\\nrecognition, image processing, deep learning and\\ncomputer vision.\\nXindong Wu is an Alfred and Helen Lamson En-\\ndowed Professor in Computer Science, University\\nof Louisiana at Lafayette (USA), and a Fellow of\\nthe IEEE and the AAAS. He received his Ph.D.\\ndegree in ArtiÔ¨Åcial Intelligence from the University\\nof Edinburgh, Britain. His research interests include\\ndata mining, knowledge-based systems, and Web in-\\nformation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge\\nand Information Systems (KAIS, by Springer), and\\na Series Editor of the Springer Book Series on Advanced Information and\\nKnowledge Processing (AI&KP). He was the Editor-in-Chief of the IEEE\\nTransactions on Knowledge and Data Engineering (TKDE, by the IEEE\\nComputer Society) between 2005 and 2008.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitting into chunks\n",
    "\n",
    "def split_documents(doc,chunk_size=1000,chunk_overlap=200):\n",
    "  \"\"\"Split Documents into smaller chunks for better RAG performance\"\"\"\n",
    "  text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    "  )\n",
    "  splitted_docs=text_splitter.split_documents(doc)\n",
    "  print(f\"Splitted {len(doc)} documents into {len(splitted_docs)} chunks.\")\n",
    "  \n",
    "  #show example of a chunk\n",
    "  if splitted_docs:\n",
    "    print(f\"\\n Example chunk:\")\n",
    "    print(f\"Content: {splitted_docs[0].page_content[:200]}\")\n",
    "    print(f\"Metadata: {splitted_docs[0].metadata}\")\n",
    "  return splitted_docs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8992b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted 67 into 379 chunks.\n",
      "\n",
      " Example chunk:\n",
      "Content: Speech and Language Processing.\n",
      "Daniel Jurafsky & James H. Martin.\n",
      "Copyright ¬© 2025.\n",
      "All\n",
      "rights reserved.\n",
      "Draft of August 24, 2025.\n",
      "CHAPTER\n",
      "5\n",
      "Embeddings\n",
      "ËçÉËÄÖÊâÄ‰ª•Âú®È±ºÔºåÂæóÈ±ºËÄåÂøòËçÉNets are for Ô¨Åsh;\n",
      "Once you get the \n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:50-07:00', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:50-07:00', 'trapped': '', 'modDate': \"D:20250824115050-07'00'\", 'creationDate': \"D:20250824115050-07'00'\", 'page': 0, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks=split_documents(all_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c97030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
